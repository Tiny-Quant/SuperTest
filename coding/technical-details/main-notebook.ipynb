{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import pygmtools as pygm\n",
    "pygm.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Meta Data\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Variables. \n",
    "max_num_nodes = 30 # 28 in the full dataset.  \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUTAG Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data. \n",
    "from torch_geometric.datasets import TUDataset\n",
    "data_raw = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "# Shuffle.\n",
    "data_raw = data_raw.shuffle()\n",
    "\n",
    "# Split.\n",
    "train_data = data_raw[:150]\n",
    "test_data = data_raw[150:]\n",
    "\n",
    "def preprocess_MUTAG(data: TUDataset, max_num_nodes) -> pyg.data.Data:\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Pad node features.\n",
    "    padded_x = torch.zeros((max_num_nodes, data.x.size(1)))\n",
    "    padded_x[:num_nodes] = data.x\n",
    "\n",
    "    # Relax edges to weights. \n",
    "    padded_adj = torch.zeros((max_num_nodes, max_num_nodes))\n",
    "    padded_adj[:num_nodes, :num_nodes] = (\n",
    "        pyg.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
    "    )\n",
    "    edge_index, edge_weight = pygm.utils.dense_to_sparse(padded_adj + 1)\n",
    "    edge_index = edge_index.transpose(0, 1)\n",
    "    edge_weight = edge_weight.squeeze(0)\n",
    "\n",
    "    # Wrap in data object.\n",
    "    preprocessed_data = pyg.data.Data(x=padded_x, \n",
    "                                      edge_index=edge_index,\n",
    "                                      edge_attr=edge_weight - 1,\n",
    "                                      y=data.y)\n",
    "\n",
    "    return preprocessed_data \n",
    "\n",
    "# Create data lists.\n",
    "train_data_list = []\n",
    "train_data_list_0 = []\n",
    "train_data_list_1 = []\n",
    "test_data_list = []\n",
    "test_data_list_0 = []\n",
    "test_data_list_1 = []\n",
    "\n",
    "for graph in train_data:\n",
    "    train_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        train_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        train_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "for graph in test_data:\n",
    "    test_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        test_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        test_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = pyg.loader.DataLoader(train_data_list, batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "test_loader = pyg.loader.DataLoader(test_data_list, batch_size=batch_size, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainee GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNWeighted(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNWeighted, self).__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(7, hidden_channels) # 7 node features.\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 2) # 2 classes.\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        )\n",
    "\n",
    "        # 1. Node embeddings.\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Pooling.\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Prediction.\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 2 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 3 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 4 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 5 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 6 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 7 Train Accuracy: 0.68 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 8 Train Accuracy: 0.7066666666666667 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 9 Train Accuracy: 0.7133333333333334 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 10 Train Accuracy: 0.7333333333333333 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 11 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 12 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 13 Train Accuracy: 0.78 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 14 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 15 Train Accuracy: 0.76 Test Accuracy: 0.7368421052631579\n",
      "Epoch: 16 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 17 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 18 Train Accuracy: 0.8 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 19 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 20 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 21 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 22 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 23 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 24 Train Accuracy: 0.84 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 25 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 26 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 27 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 28 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 29 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 30 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 31 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 32 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 33 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 34 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 35 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 36 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 37 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 38 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 39 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 40 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 41 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 42 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 43 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 44 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 45 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 46 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 47 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 48 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 49 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 50 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 51 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 52 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 53 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 54 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 55 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 56 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 57 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 58 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 59 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 60 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 61 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 62 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 63 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 64 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 65 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 66 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 67 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 68 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 69 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 70 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 71 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 72 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 73 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 74 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 75 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 76 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 77 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 78 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 79 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 80 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 81 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 82 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 83 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 84 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 85 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 86 Train Accuracy: 0.82 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 87 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 88 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 89 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 90 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 91 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 92 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 93 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 94 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 95 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 96 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 97 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 98 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 99 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 100 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 101 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 102 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 103 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 104 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 105 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 106 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 107 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 108 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 109 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 110 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 111 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 112 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 113 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 114 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 115 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 116 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 117 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 118 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 119 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 120 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 121 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 122 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 123 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 124 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 125 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 126 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 127 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 128 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 129 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 130 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 131 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 132 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 133 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 134 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 135 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 136 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 137 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 138 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 139 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 140 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 141 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 142 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 143 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 144 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 145 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 146 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 147 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 148 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 149 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 150 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 151 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 152 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 153 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 154 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 155 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 156 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 157 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 158 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 159 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 160 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 161 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 162 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 163 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 164 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 165 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 166 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 167 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 168 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 169 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 170 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 171 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 172 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 173 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 174 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 175 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 176 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 177 Train Accuracy: 0.82 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 178 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 179 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 180 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 181 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 182 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 183 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 184 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 185 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 186 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 187 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 188 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 189 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 190 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 191 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 192 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 193 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 194 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 195 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 196 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 197 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 198 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 199 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 200 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Training Explainee.\n",
    "explainee = GCNWeighted(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(explainee.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data_loader): \n",
    "    explainee.train()\n",
    "\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def explainee_accuracy(data_loader):\n",
    "    explainee.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(1, 201): \n",
    "    train(train_loader)\n",
    "    train_accuracy = explainee_accuracy(train_loader)\n",
    "    test_accuracy = explainee_accuracy(test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch} Train Accuracy: {train_accuracy} \" + \n",
    "          f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,  17],\n",
       "       [ 10, 115]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "full_batch = pyg.data.Batch.from_data_list(test_data_list + train_data_list)\n",
    "explainee.eval()\n",
    "preds = explainee(full_batch).argmax(dim=1).numpy()\n",
    "targets = full_batch.y.numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(targets, preds)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4Explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "T = 50\n",
    "time_embed_dim = 10\n",
    "num_node_feats = 7\n",
    "\n",
    "betas = torch.linspace(start=0.001, end=0.1, steps=T)\n",
    "beta_bars = []\n",
    "cum_prod = 1\n",
    "\n",
    "for beta in betas:\n",
    "    cum_prod *= (1 - 2*beta)\n",
    "    beta_bars.append(0.5 - 0.5 * cum_prod)\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(graphs: pyg.data.Batch, \n",
    "                             t: int) -> pyg.data.Batch:\n",
    "    \"\"\"\n",
    "    Input: Batch of observed graphs.\n",
    "    Output: Batch of noised graphs.\n",
    "    \"\"\"\n",
    "    edge_weight = graphs.edge_attr\n",
    "\n",
    "    transition_probs = torch.full_like(edge_weight, beta_bars[t])\n",
    "    transition_dist = torch.distributions.RelaxedBernoulli(\n",
    "        temperature=0.15, probs=transition_probs\n",
    "    )\n",
    "\n",
    "    noised_edge_weights = torch.abs(\n",
    "        edge_weight + transition_dist.rsample()\n",
    "    )\n",
    "    noised_graph = pyg.data.Batch(x=graphs.x, \n",
    "                                  edge_index=graphs.edge_index,\n",
    "                                  edge_attr=noised_edge_weights,  \n",
    "                                  y=graphs.y, batch=graphs.batch)\n",
    "\n",
    "    return noised_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model(nn.Module):\n",
    "    def __init__(self, T, time_embed_dim, \n",
    "                 num_node_feats, h1=10, h2=50, h3=30, p_dropout=0.5): \n",
    "        super(denoising_model, self).__init__()\n",
    "\n",
    "        self.time_embedder = nn.Embedding(num_embeddings=T, \n",
    "                                          embedding_dim=time_embed_dim) \n",
    "\n",
    "        self.conv1 = pyg.nn.GCNConv(num_node_feats, h1)\n",
    "        self.conv2 = pyg.nn.GCNConv(h1, h2)\n",
    "        self.conv3 = pyg.nn.GCNConv(h2, h3)\n",
    "\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lin = nn.Linear(h3, max_num_nodes**2) # predicting weights.\n",
    "\n",
    "    def forward(self, noised_graphs, t):\n",
    "        \n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            noised_graphs.x, noised_graphs.edge_index, \n",
    "            noised_graphs.batch, noised_graphs.edge_weight\n",
    "        )\n",
    "\n",
    "        time_embedding = self.time_embedder(t)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        \n",
    "        x = x + time_embedding\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.lin(x)\n",
    "        x = F.sigmoid(x) \n",
    "\n",
    "        pred_weights = x.reshape(-1)\n",
    "        pred_graph = pyg.data.Batch(x=noised_graphs.x, \n",
    "                                    edge_index=noised_graphs.edge_index,\n",
    "                                    edge_attr=pred_weights,\n",
    "                                    batch=noised_graphs.batch)\n",
    "    \n",
    "        return [pred_weights, pred_graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss_dist: 0.691668967405955  Loss_CF: 1.8383878469467163\n",
      "Epoch: 2  Loss_dist: 0.6552395820617676  Loss_CF: 1.652824838956197\n",
      "Epoch: 3  Loss_dist: 0.6067019104957581  Loss_CF: 1.6865162054697673\n",
      "Epoch: 4  Loss_dist: 0.5793521602948507  Loss_CF: 1.6632184584935505\n",
      "Epoch: 5  Loss_dist: 0.4610901375611623  Loss_CF: 1.6824330886205037\n",
      "Epoch: 6  Loss_dist: 0.4167424440383911  Loss_CF: 1.4854201873143513\n",
      "Epoch: 7  Loss_dist: 0.35840973258018494  Loss_CF: 1.390825867652893\n",
      "Epoch: 8  Loss_dist: 0.34042950471242267  Loss_CF: 1.126849929491679\n",
      "Epoch: 9  Loss_dist: 0.3319297929604848  Loss_CF: 0.8661627173423767\n",
      "Epoch: 10  Loss_dist: 0.37773693601290387  Loss_CF: 0.6504358450571696\n",
      "Epoch: 11  Loss_dist: 0.5413018266359965  Loss_CF: 0.5196541746457418\n",
      "Epoch: 12  Loss_dist: 0.3893239200115204  Loss_CF: 0.5397603611151377\n",
      "Epoch: 13  Loss_dist: 0.3767375349998474  Loss_CF: 0.5104562143484751\n",
      "Epoch: 14  Loss_dist: 0.2589414318402608  Loss_CF: 0.7227440476417542\n",
      "Epoch: 15  Loss_dist: 0.24907932678858438  Loss_CF: 0.6605048775672913\n",
      "Epoch: 16  Loss_dist: 0.2281227856874466  Loss_CF: 0.5741018851598104\n",
      "Epoch: 17  Loss_dist: 0.27674296995004016  Loss_CF: 0.5679811437924703\n",
      "Epoch: 18  Loss_dist: 0.23957612117131552  Loss_CF: 0.5488430360953013\n",
      "Epoch: 19  Loss_dist: 0.20848442614078522  Loss_CF: 0.8247237602869669\n",
      "Epoch: 20  Loss_dist: 0.2271000196536382  Loss_CF: 0.5361159443855286\n",
      "Epoch: 21  Loss_dist: 0.2537950277328491  Loss_CF: 0.5083170334498087\n",
      "Epoch: 22  Loss_dist: 0.28208047648270923  Loss_CF: 0.530261218547821\n",
      "Epoch: 23  Loss_dist: 0.2404071440299352  Loss_CF: 0.5259271562099457\n",
      "Epoch: 24  Loss_dist: 0.23829797406991324  Loss_CF: 0.5521542032559713\n",
      "Epoch: 25  Loss_dist: 0.21922777593135834  Loss_CF: 0.5528707702954611\n",
      "Epoch: 26  Loss_dist: 0.26251096030076343  Loss_CF: 0.5125043590863546\n",
      "Epoch: 27  Loss_dist: 0.1945357322692871  Loss_CF: 0.5502277612686157\n",
      "Epoch: 28  Loss_dist: 0.20405374467372894  Loss_CF: 0.5361810326576233\n",
      "Epoch: 29  Loss_dist: 0.23976156612237295  Loss_CF: 0.5389369527498881\n",
      "Epoch: 30  Loss_dist: 0.20183061559995016  Loss_CF: 0.5220603346824646\n",
      "Epoch: 31  Loss_dist: 0.188685675462087  Loss_CF: 0.5761382381121317\n",
      "Epoch: 32  Loss_dist: 0.19072120388348898  Loss_CF: 0.48976083596547443\n",
      "Epoch: 33  Loss_dist: 0.19915721317132315  Loss_CF: 0.5849321881930033\n",
      "Epoch: 34  Loss_dist: 0.21791843076546988  Loss_CF: 0.4834199051062266\n",
      "Epoch: 35  Loss_dist: 0.19112378855546316  Loss_CF: 0.5313853820164999\n",
      "Epoch: 36  Loss_dist: 0.20440049469470978  Loss_CF: 0.5457590520381927\n",
      "Epoch: 37  Loss_dist: 0.21029387414455414  Loss_CF: 0.4872518579165141\n",
      "Epoch: 38  Loss_dist: 0.19607355197270712  Loss_CF: 0.48111433784166974\n",
      "Epoch: 39  Loss_dist: 0.20275801420211792  Loss_CF: 0.5295992890993754\n",
      "Epoch: 40  Loss_dist: 0.21378254890441895  Loss_CF: 0.48284606138865155\n",
      "Epoch: 41  Loss_dist: 0.20699368913968405  Loss_CF: 0.5432889461517334\n",
      "Epoch: 42  Loss_dist: 0.19559252758820853  Loss_CF: 0.55870521068573\n",
      "Epoch: 43  Loss_dist: 0.17053441206614176  Loss_CF: 0.5648359457651774\n",
      "Epoch: 44  Loss_dist: 0.18351679046948752  Loss_CF: 0.4909101227919261\n",
      "Epoch: 45  Loss_dist: 0.18242977559566498  Loss_CF: 0.5408855676651001\n",
      "Epoch: 46  Loss_dist: 0.19923000037670135  Loss_CF: 0.48224059740702313\n",
      "Epoch: 47  Loss_dist: 0.18778801957766214  Loss_CF: 0.5058782796065012\n",
      "Epoch: 48  Loss_dist: 0.18237319588661194  Loss_CF: 0.5122256974379221\n",
      "Epoch: 49  Loss_dist: 0.18535360197226206  Loss_CF: 0.5341958900292715\n",
      "Epoch: 50  Loss_dist: 0.1845457802216212  Loss_CF: 0.5319637556870779\n",
      "Epoch: 51  Loss_dist: 0.19749563932418823  Loss_CF: 0.5514031151930491\n",
      "Epoch: 52  Loss_dist: 0.18975931406021118  Loss_CF: 0.4947267174720764\n",
      "Epoch: 53  Loss_dist: 0.20556539793809256  Loss_CF: 0.492871214946111\n",
      "Epoch: 54  Loss_dist: 0.1660663237174352  Loss_CF: 0.5223288436730703\n",
      "Epoch: 55  Loss_dist: 0.16183467209339142  Loss_CF: 0.5037262737751007\n",
      "Epoch: 56  Loss_dist: 0.16682655115922293  Loss_CF: 0.5525611440340678\n",
      "Epoch: 57  Loss_dist: 0.17392479379971823  Loss_CF: 0.5383949875831604\n",
      "Epoch: 58  Loss_dist: 0.19239703317483267  Loss_CF: 0.5070809026559194\n",
      "Epoch: 59  Loss_dist: 0.20453920463720957  Loss_CF: 0.4994174639383952\n",
      "Epoch: 60  Loss_dist: 0.16728233297665915  Loss_CF: 0.5758764147758484\n",
      "Epoch: 61  Loss_dist: 0.20695176223913828  Loss_CF: 0.4822065432866414\n",
      "Epoch: 62  Loss_dist: 0.20356959104537964  Loss_CF: 0.5169638593991598\n",
      "Epoch: 63  Loss_dist: 0.1972333788871765  Loss_CF: 0.49174273014068604\n",
      "Epoch: 64  Loss_dist: 0.18387349446614584  Loss_CF: 0.4580130378405253\n",
      "Epoch: 65  Loss_dist: 0.16719328860441843  Loss_CF: 0.4870905081431071\n",
      "Epoch: 66  Loss_dist: 0.1635491152604421  Loss_CF: 0.5617816547552744\n",
      "Epoch: 67  Loss_dist: 0.17513424654801688  Loss_CF: 0.48024025559425354\n",
      "Epoch: 68  Loss_dist: 0.18826015293598175  Loss_CF: 0.4623684187730153\n",
      "Epoch: 69  Loss_dist: 0.17680756251017252  Loss_CF: 0.532767136891683\n",
      "Epoch: 70  Loss_dist: 0.18574800590674082  Loss_CF: 0.5108743111292521\n",
      "Epoch: 71  Loss_dist: 0.2177391548951467  Loss_CF: 0.5198428730169932\n",
      "Epoch: 72  Loss_dist: 0.19320167104403177  Loss_CF: 0.46837834517161053\n",
      "Epoch: 73  Loss_dist: 0.15876678625742593  Loss_CF: 0.5558742185433706\n",
      "Epoch: 74  Loss_dist: 0.1662298341592153  Loss_CF: 0.5809158285458883\n",
      "Epoch: 75  Loss_dist: 0.19291340311368307  Loss_CF: 0.4975064198176066\n",
      "Epoch: 76  Loss_dist: 0.1810662398735682  Loss_CF: 0.5179875195026398\n",
      "Epoch: 77  Loss_dist: 0.19097655514876047  Loss_CF: 0.4670921266078949\n",
      "Epoch: 78  Loss_dist: 0.16251355906327566  Loss_CF: 0.5408084293206533\n",
      "Epoch: 79  Loss_dist: 0.17681339383125305  Loss_CF: 0.5606258710225424\n",
      "Epoch: 80  Loss_dist: 0.17837484180927277  Loss_CF: 0.47279537717501324\n",
      "Epoch: 81  Loss_dist: 0.16309026877085367  Loss_CF: 0.6118083993593851\n",
      "Epoch: 82  Loss_dist: 0.20669686794281006  Loss_CF: 0.4495243728160858\n",
      "Epoch: 83  Loss_dist: 0.22177431484063467  Loss_CF: 0.49073758721351624\n",
      "Epoch: 84  Loss_dist: 0.24232267340024313  Loss_CF: 0.5077143708864847\n",
      "Epoch: 85  Loss_dist: 0.18959274888038635  Loss_CF: 0.5202361543973287\n",
      "Epoch: 86  Loss_dist: 0.16537829240163168  Loss_CF: 0.5068913996219635\n",
      "Epoch: 87  Loss_dist: 0.16553199787934622  Loss_CF: 0.5089820424715678\n",
      "Epoch: 88  Loss_dist: 0.1707404504219691  Loss_CF: 0.4651554922262828\n",
      "Epoch: 89  Loss_dist: 0.19722466667493185  Loss_CF: 0.4679301083087921\n",
      "Epoch: 90  Loss_dist: 0.1748723437388738  Loss_CF: 0.49242696166038513\n",
      "Epoch: 91  Loss_dist: 0.16375082731246948  Loss_CF: 0.5094761351744334\n",
      "Epoch: 92  Loss_dist: 0.17148320376873016  Loss_CF: 0.4846594234307607\n",
      "Epoch: 93  Loss_dist: 0.19112150371074677  Loss_CF: 0.5058495998382568\n",
      "Epoch: 94  Loss_dist: 0.17804627120494843  Loss_CF: 0.5478413105010986\n",
      "Epoch: 95  Loss_dist: 0.17093145847320557  Loss_CF: 0.5119277536869049\n",
      "Epoch: 96  Loss_dist: 0.15738239387671152  Loss_CF: 0.5774512390295664\n",
      "Epoch: 97  Loss_dist: 0.20388259490331015  Loss_CF: 0.5082236627737681\n",
      "Epoch: 98  Loss_dist: 0.1848626285791397  Loss_CF: 0.49971774220466614\n",
      "Epoch: 99  Loss_dist: 0.18884630997975668  Loss_CF: 0.48923422892888385\n",
      "Epoch: 100  Loss_dist: 0.17079567909240723  Loss_CF: 0.49302947521209717\n",
      "Epoch: 101  Loss_dist: 0.14517982800801596  Loss_CF: 0.6009345849355062\n",
      "Epoch: 102  Loss_dist: 0.1813316891590754  Loss_CF: 0.4608854353427887\n",
      "Epoch: 103  Loss_dist: 0.15657155215740204  Loss_CF: 0.5435713529586792\n",
      "Epoch: 104  Loss_dist: 0.18224301437536874  Loss_CF: 0.4950515925884247\n",
      "Epoch: 105  Loss_dist: 0.19479880730311075  Loss_CF: 0.5143077174822489\n",
      "Epoch: 106  Loss_dist: 0.16972060998280844  Loss_CF: 0.5029062827428182\n",
      "Epoch: 107  Loss_dist: 0.17157144844532013  Loss_CF: 0.46011920770009357\n",
      "Epoch: 108  Loss_dist: 0.1773367871840795  Loss_CF: 0.457789013783137\n",
      "Epoch: 109  Loss_dist: 0.17284666995207468  Loss_CF: 0.4977025290330251\n",
      "Epoch: 110  Loss_dist: 0.16559713085492453  Loss_CF: 0.47177862127621967\n",
      "Epoch: 111  Loss_dist: 0.15633728603521982  Loss_CF: 0.5185263256231943\n",
      "Epoch: 112  Loss_dist: 0.1765233278274536  Loss_CF: 0.4734791119893392\n",
      "Epoch: 113  Loss_dist: 0.177950049440066  Loss_CF: 0.47492531935373944\n",
      "Epoch: 114  Loss_dist: 0.18566411236921945  Loss_CF: 0.47245822350184125\n",
      "Epoch: 115  Loss_dist: 0.18941541016101837  Loss_CF: 0.5068812966346741\n",
      "Epoch: 116  Loss_dist: 0.16210016111532846  Loss_CF: 0.46592506766319275\n",
      "Epoch: 117  Loss_dist: 0.14824782808621725  Loss_CF: 0.5199577411015829\n",
      "Epoch: 118  Loss_dist: 0.15722817182540894  Loss_CF: 0.5726919968922933\n",
      "Epoch: 119  Loss_dist: 0.16173411905765533  Loss_CF: 0.4993038972218831\n",
      "Epoch: 120  Loss_dist: 0.21080716451009116  Loss_CF: 0.47314215699831647\n",
      "Epoch: 121  Loss_dist: 0.20252389212449393  Loss_CF: 0.454390952984492\n",
      "Epoch: 122  Loss_dist: 0.17116431395212808  Loss_CF: 0.5188740293184916\n",
      "Epoch: 123  Loss_dist: 0.16961040596167246  Loss_CF: 0.47576770186424255\n",
      "Epoch: 124  Loss_dist: 0.1550267438093821  Loss_CF: 0.5379098455111185\n",
      "Epoch: 125  Loss_dist: 0.15363969405492148  Loss_CF: 0.48832552631696063\n",
      "Epoch: 126  Loss_dist: 0.1785116841395696  Loss_CF: 0.47508835792541504\n",
      "Epoch: 127  Loss_dist: 0.1796099990606308  Loss_CF: 0.4881122608979543\n",
      "Epoch: 128  Loss_dist: 0.16445521513621011  Loss_CF: 0.4659756124019623\n",
      "Epoch: 129  Loss_dist: 0.16976996262868246  Loss_CF: 0.43694626291592914\n",
      "Epoch: 130  Loss_dist: 0.1712078352769216  Loss_CF: 0.483768771092097\n",
      "Epoch: 131  Loss_dist: 0.1823538194100062  Loss_CF: 0.4340803821881612\n",
      "Epoch: 132  Loss_dist: 0.1691710203886032  Loss_CF: 0.5033639868100485\n",
      "Epoch: 133  Loss_dist: 0.16152123113473257  Loss_CF: 0.48004209995269775\n",
      "Epoch: 134  Loss_dist: 0.17105431854724884  Loss_CF: 0.47614601254463196\n",
      "Epoch: 135  Loss_dist: 0.16194042066733041  Loss_CF: 0.5419968366622925\n",
      "Epoch: 136  Loss_dist: 0.17902463674545288  Loss_CF: 0.48243587215741474\n",
      "Epoch: 137  Loss_dist: 0.17265327274799347  Loss_CF: 0.48140382766723633\n",
      "Epoch: 138  Loss_dist: 0.17537038028240204  Loss_CF: 0.43021801114082336\n",
      "Epoch: 139  Loss_dist: 0.1937903811534246  Loss_CF: 0.4613930682341258\n",
      "Epoch: 140  Loss_dist: 0.18663878738880157  Loss_CF: 0.5104506512482961\n",
      "Epoch: 141  Loss_dist: 0.16689142088095346  Loss_CF: 0.48970943689346313\n",
      "Epoch: 142  Loss_dist: 0.1573395530382792  Loss_CF: 0.4991021454334259\n",
      "Epoch: 143  Loss_dist: 0.17014463245868683  Loss_CF: 0.44713521003723145\n",
      "Epoch: 144  Loss_dist: 0.16732084254423776  Loss_CF: 0.47041510542233783\n",
      "Epoch: 145  Loss_dist: 0.22276331981023154  Loss_CF: 0.42121774951616925\n",
      "Epoch: 146  Loss_dist: 0.20673265556494394  Loss_CF: 0.4159766534964244\n",
      "Epoch: 147  Loss_dist: 0.1675973435242971  Loss_CF: 0.5035866300264994\n",
      "Epoch: 148  Loss_dist: 0.17057757576306662  Loss_CF: 0.42155492305755615\n",
      "Epoch: 149  Loss_dist: 0.16279281179110208  Loss_CF: 0.420718252658844\n",
      "Epoch: 150  Loss_dist: 0.16487403710683188  Loss_CF: 0.46507637699445087\n",
      "Epoch: 151  Loss_dist: 0.18614334364732107  Loss_CF: 0.38821175694465637\n",
      "Epoch: 152  Loss_dist: 0.22177391250928244  Loss_CF: 0.5071395635604858\n",
      "Epoch: 153  Loss_dist: 0.16924663881460825  Loss_CF: 0.431932270526886\n",
      "Epoch: 154  Loss_dist: 0.16761557261149088  Loss_CF: 0.43037999669710797\n",
      "Epoch: 155  Loss_dist: 0.1588338017463684  Loss_CF: 0.45182440678278607\n",
      "Epoch: 156  Loss_dist: 0.18579269448916116  Loss_CF: 0.481450617313385\n",
      "Epoch: 157  Loss_dist: 0.15283131102720895  Loss_CF: 0.5426024794578552\n",
      "Epoch: 158  Loss_dist: 0.16023444632689157  Loss_CF: 0.45542941490809125\n",
      "Epoch: 159  Loss_dist: 0.16565032800038657  Loss_CF: 0.43803704778353375\n",
      "Epoch: 160  Loss_dist: 0.18935350080331168  Loss_CF: 0.44410820802052814\n",
      "Epoch: 161  Loss_dist: 0.17839549481868744  Loss_CF: 0.47196569045384723\n",
      "Epoch: 162  Loss_dist: 0.17958933115005493  Loss_CF: 0.4610946873823802\n",
      "Epoch: 163  Loss_dist: 0.17222543557484946  Loss_CF: 0.4463119109471639\n",
      "Epoch: 164  Loss_dist: 0.16894581417242685  Loss_CF: 0.49021778504053753\n",
      "Epoch: 165  Loss_dist: 0.17068278789520264  Loss_CF: 0.4631807307402293\n",
      "Epoch: 166  Loss_dist: 0.16726524631182352  Loss_CF: 0.44340280691782635\n",
      "Epoch: 167  Loss_dist: 0.1560709128777186  Loss_CF: 0.4565926094849904\n",
      "Epoch: 168  Loss_dist: 0.1794301321109136  Loss_CF: 0.4525538782278697\n",
      "Epoch: 169  Loss_dist: 0.22122849027315775  Loss_CF: 0.4017840425173442\n",
      "Epoch: 170  Loss_dist: 0.18751556177934012  Loss_CF: 0.4630723297595978\n",
      "Epoch: 171  Loss_dist: 0.1971748173236847  Loss_CF: 0.42298651734987897\n",
      "Epoch: 172  Loss_dist: 0.16359821955362955  Loss_CF: 0.4773489435513814\n",
      "Epoch: 173  Loss_dist: 0.1591121256351471  Loss_CF: 0.4265782634417216\n",
      "Epoch: 174  Loss_dist: 0.1915456602970759  Loss_CF: 0.5064195791880289\n",
      "Epoch: 175  Loss_dist: 0.17545641958713531  Loss_CF: 0.48274006446202594\n",
      "Epoch: 176  Loss_dist: 0.1780791332324346  Loss_CF: 0.539655456940333\n",
      "Epoch: 177  Loss_dist: 0.18268473943074545  Loss_CF: 0.4474596480528514\n",
      "Epoch: 178  Loss_dist: 0.16975846389929453  Loss_CF: 0.4087279935677846\n",
      "Epoch: 179  Loss_dist: 0.16638338069121042  Loss_CF: 0.4409886399904887\n",
      "Epoch: 180  Loss_dist: 0.19692979753017426  Loss_CF: 0.42123304804166156\n",
      "Epoch: 181  Loss_dist: 0.16223326325416565  Loss_CF: 0.4662538170814514\n",
      "Epoch: 182  Loss_dist: 0.20239470899105072  Loss_CF: 0.48038676381111145\n",
      "Epoch: 183  Loss_dist: 0.16457690298557281  Loss_CF: 0.46136802434921265\n",
      "Epoch: 184  Loss_dist: 0.19257624944051108  Loss_CF: 0.436906099319458\n",
      "Epoch: 185  Loss_dist: 0.17182481785615286  Loss_CF: 0.4428682029247284\n",
      "Epoch: 186  Loss_dist: 0.17093719045321146  Loss_CF: 0.4355907738208771\n",
      "Epoch: 187  Loss_dist: 0.17013315856456757  Loss_CF: 0.450177401304245\n",
      "Epoch: 188  Loss_dist: 0.16791803141434988  Loss_CF: 0.4124856193860372\n",
      "Epoch: 189  Loss_dist: 0.17050385475158691  Loss_CF: 0.45531890789667767\n",
      "Epoch: 190  Loss_dist: 0.1842738538980484  Loss_CF: 0.40813808639844257\n",
      "Epoch: 191  Loss_dist: 0.19784733156363168  Loss_CF: 0.40533260504404706\n",
      "Epoch: 192  Loss_dist: 0.1751892069975535  Loss_CF: 0.41536091764767963\n",
      "Epoch: 193  Loss_dist: 0.1603496124347051  Loss_CF: 0.41738365093866986\n",
      "Epoch: 194  Loss_dist: 0.16780944665273032  Loss_CF: 0.42897533377011615\n",
      "Epoch: 195  Loss_dist: 0.17022907733917236  Loss_CF: 0.48422518372535706\n",
      "Epoch: 196  Loss_dist: 0.18087452153364816  Loss_CF: 0.4267410635948181\n",
      "Epoch: 197  Loss_dist: 0.1898116817077001  Loss_CF: 0.4360559682051341\n",
      "Epoch: 198  Loss_dist: 0.183873251080513  Loss_CF: 0.4436265528202057\n",
      "Epoch: 199  Loss_dist: 0.18404618899027506  Loss_CF: 0.42877596616744995\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise_CF = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise_CF.parameters(), lr=0.01)\n",
    "CF_weight = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss_dist = 0.0\n",
    "    running_loss_CF = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise_CF(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss_dist += loss_dist.item()\n",
    "\n",
    "        explainee.eval()\n",
    "        explainee_pred = F.softmax(explainee(pred_graph), dim=-1)\n",
    "        class_prob = explainee_pred[torch.arange(explainee_pred.shape[0]), \n",
    "                                    graphs.y]\n",
    "        loss_CF = (-1 * torch.log(1 - class_prob)).mean()\n",
    "        running_loss_CF += loss_CF.item()\n",
    "\n",
    "        loss = 1.0 * loss_dist + CF_weight * loss_CF\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\", \n",
    "        f\" Loss_dist: {running_loss_dist / len(train_loader)}\", \n",
    "        f\" Loss_CF: {running_loss_CF / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(496.7574)\n",
      "tensor(26.) tensor(42.8113, grad_fn=<SumBackward0>)\n",
      "tensor([[0.7762, 0.2238]], grad_fn=<SoftmaxBackward0>) tensor([[0.3045, 0.6955]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Inference \n",
    "non_mut_example = pyg.data.Batch.from_data_list([test_data_list_0[0]])\n",
    "non_mut_noised = forward_diffusion_sample(non_mut_example, torch.tensor([T-1])) \n",
    "print(non_mut_noised.edge_attr.sum())\n",
    "pred_weight, non_mut_denoised = model_denoise_CF(non_mut_noised, torch.tensor([T-1]))\n",
    "print(test_data_list_0[0].edge_attr.sum(), non_mut_denoised.edge_attr.sum())\n",
    "print(torch.softmax(explainee(test_data_list_0[0]), dim=-1), \n",
    "      torch.softmax(explainee(non_mut_denoised), dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
