{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import pygmtools as pygm\n",
    "pygm.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Meta Data\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Variables. \n",
    "max_num_nodes = 30 # 28 in the full dataset.  \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUTAG Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data. \n",
    "from torch_geometric.datasets import TUDataset\n",
    "data_raw = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "# Shuffle.\n",
    "data_raw = data_raw.shuffle()\n",
    "\n",
    "# Split.\n",
    "train_data = data_raw[:150]\n",
    "test_data = data_raw[150:]\n",
    "\n",
    "def preprocess_MUTAG(data: TUDataset, max_num_nodes) -> pyg.data.Data:\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Pad node features.\n",
    "    padded_x = torch.zeros((max_num_nodes, data.x.size(1)))\n",
    "    padded_x[:num_nodes] = data.x\n",
    "\n",
    "    # Relax edges to weights. \n",
    "    padded_adj = torch.zeros((max_num_nodes, max_num_nodes))\n",
    "    padded_adj[:num_nodes, :num_nodes] = (\n",
    "        pyg.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
    "    )\n",
    "    edge_index, edge_weight = pygm.utils.dense_to_sparse(padded_adj + 1)\n",
    "    edge_index = edge_index.transpose(0, 1)\n",
    "    edge_weight = edge_weight.squeeze(0)\n",
    "\n",
    "    # Wrap in data object.\n",
    "    preprocessed_data = pyg.data.Data(x=padded_x, \n",
    "                                      edge_index=edge_index,\n",
    "                                      edge_attr=edge_weight - 1,\n",
    "                                      y=data.y)\n",
    "\n",
    "    return preprocessed_data \n",
    "\n",
    "# Create data lists.\n",
    "train_data_list = []\n",
    "train_data_list_0 = []\n",
    "train_data_list_1 = []\n",
    "test_data_list = []\n",
    "test_data_list_0 = []\n",
    "test_data_list_1 = []\n",
    "\n",
    "for graph in train_data:\n",
    "    train_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        train_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        train_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "for graph in test_data:\n",
    "    test_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        test_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        test_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = pyg.loader.DataLoader(train_data_list, batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "test_loader = pyg.loader.DataLoader(test_data_list, batch_size=batch_size, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainee GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNWeighted(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNWeighted, self).__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(7, hidden_channels) # 7 node features.\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 2) # 2 classes.\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        )\n",
    "\n",
    "        # 1. Node embeddings.\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Pooling.\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Prediction.\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 2 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 3 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 4 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 5 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 6 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 7 Train Accuracy: 0.68 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 8 Train Accuracy: 0.7066666666666667 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 9 Train Accuracy: 0.7133333333333334 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 10 Train Accuracy: 0.7333333333333333 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 11 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 12 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 13 Train Accuracy: 0.78 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 14 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 15 Train Accuracy: 0.76 Test Accuracy: 0.7368421052631579\n",
      "Epoch: 16 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 17 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 18 Train Accuracy: 0.8 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 19 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 20 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 21 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 22 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 23 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 24 Train Accuracy: 0.84 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 25 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 26 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 27 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 28 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 29 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 30 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 31 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 32 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 33 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 34 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 35 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 36 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 37 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 38 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 39 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 40 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 41 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 42 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 43 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 44 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 45 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 46 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 47 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 48 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 49 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 50 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 51 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 52 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 53 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 54 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 55 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 56 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 57 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 58 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 59 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 60 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 61 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 62 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 63 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 64 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 65 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 66 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 67 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 68 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 69 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 70 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 71 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 72 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 73 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 74 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 75 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 76 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 77 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 78 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 79 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 80 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 81 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 82 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 83 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 84 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 85 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 86 Train Accuracy: 0.82 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 87 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 88 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 89 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 90 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 91 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 92 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 93 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 94 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 95 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 96 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 97 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 98 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 99 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 100 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 101 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 102 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 103 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 104 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 105 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 106 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 107 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 108 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 109 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 110 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 111 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 112 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 113 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 114 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 115 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 116 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 117 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 118 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 119 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 120 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 121 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 122 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 123 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 124 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 125 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 126 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 127 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 128 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 129 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 130 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 131 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 132 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 133 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 134 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 135 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 136 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 137 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 138 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 139 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 140 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 141 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 142 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 143 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 144 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 145 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 146 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 147 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 148 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 149 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 150 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 151 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 152 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 153 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 154 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 155 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 156 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 157 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 158 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 159 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 160 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 161 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 162 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 163 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 164 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 165 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 166 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 167 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 168 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 169 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 170 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 171 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 172 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 173 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 174 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 175 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 176 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 177 Train Accuracy: 0.82 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 178 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 179 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 180 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 181 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 182 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 183 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 184 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 185 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 186 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 187 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 188 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 189 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 190 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 191 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 192 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 193 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 194 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 195 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 196 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 197 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 198 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 199 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 200 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Training Explainee.\n",
    "explainee = GCNWeighted(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(explainee.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data_loader): \n",
    "    explainee.train()\n",
    "\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def explainee_accuracy(data_loader):\n",
    "    explainee.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(1, 201): \n",
    "    train(train_loader)\n",
    "    train_accuracy = explainee_accuracy(train_loader)\n",
    "    test_accuracy = explainee_accuracy(test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch} Train Accuracy: {train_accuracy} \" + \n",
    "          f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,  17],\n",
       "       [ 10, 115]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "full_batch = pyg.data.Batch.from_data_list(test_data_list + train_data_list)\n",
    "explainee.eval()\n",
    "preds = explainee(full_batch).argmax(dim=1).numpy()\n",
    "targets = full_batch.y.numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(targets, preds)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4Explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "T = 50\n",
    "time_embed_dim = 10\n",
    "num_node_feats = 7\n",
    "\n",
    "betas = torch.linspace(start=0.001, end=0.1, steps=T)\n",
    "beta_bars = []\n",
    "cum_prod = 1\n",
    "\n",
    "for beta in betas:\n",
    "    cum_prod *= (1 - 2*beta)\n",
    "    beta_bars.append(0.5 - 0.5 * cum_prod)\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(graphs: pyg.data.Batch, \n",
    "                             t: int) -> pyg.data.Batch:\n",
    "    \"\"\"\n",
    "    Input: Batch of observed graphs.\n",
    "    Output: Batch of noised graphs.\n",
    "    \"\"\"\n",
    "    edge_weight = graphs.edge_attr\n",
    "\n",
    "    transition_probs = torch.full_like(edge_weight, beta_bars[t])\n",
    "    transition_dist = torch.distributions.RelaxedBernoulli(\n",
    "        temperature=0.15, probs=transition_probs\n",
    "    )\n",
    "\n",
    "    noised_edge_weights = torch.abs(\n",
    "        edge_weight + transition_dist.rsample()\n",
    "    )\n",
    "    noised_graph = pyg.data.Batch(x=graphs.x, \n",
    "                                  edge_index=graphs.edge_index,\n",
    "                                  edge_attr=noised_edge_weights,  \n",
    "                                  y=graphs.y, batch=graphs.batch)\n",
    "\n",
    "    return noised_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model(nn.Module):\n",
    "    def __init__(self, T, time_embed_dim, \n",
    "                 num_node_feats, h1=10, h2=50, h3=30, p_dropout=0.5): \n",
    "        super(denoising_model, self).__init__()\n",
    "\n",
    "        self.time_embedder = nn.Embedding(num_embeddings=T, \n",
    "                                          embedding_dim=time_embed_dim) \n",
    "\n",
    "        self.conv1 = pyg.nn.GCNConv(num_node_feats, h1)\n",
    "        self.conv2 = pyg.nn.GCNConv(h1, h2)\n",
    "        self.conv3 = pyg.nn.GCNConv(h2, h3)\n",
    "\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lin = nn.Linear(h3, max_num_nodes**2) # predicting weights.\n",
    "\n",
    "    def forward(self, noised_graphs, t):\n",
    "        \n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            noised_graphs.x, noised_graphs.edge_index, \n",
    "            noised_graphs.batch, noised_graphs.edge_weight\n",
    "        )\n",
    "\n",
    "        time_embedding = self.time_embedder(t)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        \n",
    "        x = x + time_embedding\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.lin(x)\n",
    "        x = F.sigmoid(x) \n",
    "\n",
    "        pred_weights = x.reshape(-1)\n",
    "        pred_graph = pyg.data.Batch(x=noised_graphs.x, \n",
    "                                    edge_index=noised_graphs.edge_index,\n",
    "                                    edge_attr=pred_weights,\n",
    "                                    batch=noised_graphs.batch)\n",
    "    \n",
    "        return [pred_weights, pred_graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6814501682917277\n",
      "Epoch: 2, Loss: 0.5736742814381918\n",
      "Epoch: 3, Loss: 0.4547794759273529\n",
      "Epoch: 4, Loss: 0.3207857708136241\n",
      "Epoch: 5, Loss: 0.1708637277285258\n",
      "Epoch: 6, Loss: 0.13724727680285773\n",
      "Epoch: 7, Loss: 0.11728910605112712\n",
      "Epoch: 8, Loss: 0.09351835151513417\n",
      "Epoch: 9, Loss: 0.09692725042502086\n",
      "Epoch: 10, Loss: 0.14647934089104334\n",
      "Epoch: 11, Loss: 0.10092594722906749\n",
      "Epoch: 12, Loss: 0.09141373882691066\n",
      "Epoch: 13, Loss: 0.11526066809892654\n",
      "Epoch: 14, Loss: 0.09440551449855168\n",
      "Epoch: 15, Loss: 0.08638316889603932\n",
      "Epoch: 16, Loss: 0.08875712255636851\n",
      "Epoch: 17, Loss: 0.07957890878121059\n",
      "Epoch: 18, Loss: 0.07754483819007874\n",
      "Epoch: 19, Loss: 0.07667408386866252\n",
      "Epoch: 20, Loss: 0.07773071030775706\n",
      "Epoch: 21, Loss: 0.09795727580785751\n",
      "Epoch: 22, Loss: 0.07931863764921825\n",
      "Epoch: 23, Loss: 0.07991917183001836\n",
      "Epoch: 24, Loss: 0.08723363031943639\n",
      "Epoch: 25, Loss: 0.0734206885099411\n",
      "Epoch: 26, Loss: 0.07543578495581944\n",
      "Epoch: 27, Loss: 0.07695478200912476\n",
      "Epoch: 28, Loss: 0.0756968582669894\n",
      "Epoch: 29, Loss: 0.07698562989632289\n",
      "Epoch: 30, Loss: 0.0729766661922137\n",
      "Epoch: 31, Loss: 0.07534426202376683\n",
      "Epoch: 32, Loss: 0.07241629560788472\n",
      "Epoch: 33, Loss: 0.07642552256584167\n",
      "Epoch: 34, Loss: 0.07738276322682698\n",
      "Epoch: 35, Loss: 0.07913545270760854\n",
      "Epoch: 36, Loss: 0.07237691432237625\n",
      "Epoch: 37, Loss: 0.07229168216387431\n",
      "Epoch: 38, Loss: 0.07358790934085846\n",
      "Epoch: 39, Loss: 0.07245963315169017\n",
      "Epoch: 40, Loss: 0.07401404281457265\n",
      "Epoch: 41, Loss: 0.07292583336432774\n",
      "Epoch: 42, Loss: 0.07356188694636027\n",
      "Epoch: 43, Loss: 0.07280875494082768\n",
      "Epoch: 44, Loss: 0.07185168812672298\n",
      "Epoch: 45, Loss: 0.07236756136020024\n",
      "Epoch: 46, Loss: 0.07105868558088939\n",
      "Epoch: 47, Loss: 0.07227483143409093\n",
      "Epoch: 48, Loss: 0.07167984296878178\n",
      "Epoch: 49, Loss: 0.07243432352940242\n",
      "Epoch: 50, Loss: 0.07374373823404312\n",
      "Epoch: 51, Loss: 0.0728550876180331\n",
      "Epoch: 52, Loss: 0.07328435033559799\n",
      "Epoch: 53, Loss: 0.0733398050069809\n",
      "Epoch: 54, Loss: 0.07335237910350163\n",
      "Epoch: 55, Loss: 0.07146959503491719\n",
      "Epoch: 56, Loss: 0.07249737282594045\n",
      "Epoch: 57, Loss: 0.07229685038328171\n",
      "Epoch: 58, Loss: 0.0712014411886533\n",
      "Epoch: 59, Loss: 0.0727099950114886\n",
      "Epoch: 60, Loss: 0.07181406517823537\n",
      "Epoch: 61, Loss: 0.07136548807223637\n",
      "Epoch: 62, Loss: 0.07215085625648499\n",
      "Epoch: 63, Loss: 0.07153252015511195\n",
      "Epoch: 64, Loss: 0.06965197622776031\n",
      "Epoch: 65, Loss: 0.07197615752617519\n",
      "Epoch: 66, Loss: 0.07139051457246144\n",
      "Epoch: 67, Loss: 0.07143431653579076\n",
      "Epoch: 68, Loss: 0.0723697120944659\n",
      "Epoch: 69, Loss: 0.07167892654736836\n",
      "Epoch: 70, Loss: 0.07478177547454834\n",
      "Epoch: 71, Loss: 0.07233011225859325\n",
      "Epoch: 72, Loss: 0.07138793170452118\n",
      "Epoch: 73, Loss: 0.0735930601755778\n",
      "Epoch: 74, Loss: 0.0731532871723175\n",
      "Epoch: 75, Loss: 0.07245220988988876\n",
      "Epoch: 76, Loss: 0.07055198401212692\n",
      "Epoch: 77, Loss: 0.07405852278073628\n",
      "Epoch: 78, Loss: 0.07095106194416682\n",
      "Epoch: 79, Loss: 0.07114708423614502\n",
      "Epoch: 80, Loss: 0.07209778825441997\n",
      "Epoch: 81, Loss: 0.07097005347410838\n",
      "Epoch: 82, Loss: 0.07170455654462178\n",
      "Epoch: 83, Loss: 0.07294322053591411\n",
      "Epoch: 84, Loss: 0.07203380763530731\n",
      "Epoch: 85, Loss: 0.07142213980356853\n",
      "Epoch: 86, Loss: 0.07294473797082901\n",
      "Epoch: 87, Loss: 0.07304663707812627\n",
      "Epoch: 88, Loss: 0.07198470085859299\n",
      "Epoch: 89, Loss: 0.07244538515806198\n",
      "Epoch: 90, Loss: 0.07043532282114029\n",
      "Epoch: 91, Loss: 0.07440967112779617\n",
      "Epoch: 92, Loss: 0.07073189069827397\n",
      "Epoch: 93, Loss: 0.07287766287724178\n",
      "Epoch: 94, Loss: 0.07096441090106964\n",
      "Epoch: 95, Loss: 0.07233516623576482\n",
      "Epoch: 96, Loss: 0.07241647442181905\n",
      "Epoch: 97, Loss: 0.07195903609196345\n",
      "Epoch: 98, Loss: 0.07278900096813838\n",
      "Epoch: 99, Loss: 0.07105328887701035\n",
      "Epoch: 100, Loss: 0.07270454863707225\n",
      "Epoch: 101, Loss: 0.07182793070872624\n",
      "Epoch: 102, Loss: 0.07094841698805492\n",
      "Epoch: 103, Loss: 0.07165713359912236\n",
      "Epoch: 104, Loss: 0.0710790827870369\n",
      "Epoch: 105, Loss: 0.0708811953663826\n",
      "Epoch: 106, Loss: 0.07149809102217357\n",
      "Epoch: 107, Loss: 0.07137079785267512\n",
      "Epoch: 108, Loss: 0.07166628539562225\n",
      "Epoch: 109, Loss: 0.07300987094640732\n",
      "Epoch: 110, Loss: 0.07097336153189342\n",
      "Epoch: 111, Loss: 0.07087908933560054\n",
      "Epoch: 112, Loss: 0.0735155592362086\n",
      "Epoch: 113, Loss: 0.07208672414223354\n",
      "Epoch: 114, Loss: 0.07049987465143204\n",
      "Epoch: 115, Loss: 0.07190702607234319\n",
      "Epoch: 116, Loss: 0.07099550714095433\n",
      "Epoch: 117, Loss: 0.07264531900485356\n",
      "Epoch: 118, Loss: 0.07121786723534267\n",
      "Epoch: 119, Loss: 0.07042273630698521\n",
      "Epoch: 120, Loss: 0.0734096144636472\n",
      "Epoch: 121, Loss: 0.07025731851657231\n",
      "Epoch: 122, Loss: 0.06997514516115189\n",
      "Epoch: 123, Loss: 0.06910749276479085\n",
      "Epoch: 124, Loss: 0.070357712606589\n",
      "Epoch: 125, Loss: 0.07077979296445847\n",
      "Epoch: 126, Loss: 0.06913067648808162\n",
      "Epoch: 127, Loss: 0.07009914517402649\n",
      "Epoch: 128, Loss: 0.07117922852436702\n",
      "Epoch: 129, Loss: 0.06994201739629109\n",
      "Epoch: 130, Loss: 0.07115936279296875\n",
      "Epoch: 131, Loss: 0.07027494659026463\n",
      "Epoch: 132, Loss: 0.0711660807331403\n",
      "Epoch: 133, Loss: 0.07101788371801376\n",
      "Epoch: 134, Loss: 0.07282036046187083\n",
      "Epoch: 135, Loss: 0.07208688805500667\n",
      "Epoch: 136, Loss: 0.07241729150215785\n",
      "Epoch: 137, Loss: 0.07120668888092041\n",
      "Epoch: 138, Loss: 0.07104944189389546\n",
      "Epoch: 139, Loss: 0.07231361667315166\n",
      "Epoch: 140, Loss: 0.07108508547147115\n",
      "Epoch: 141, Loss: 0.07050368686517079\n",
      "Epoch: 142, Loss: 0.06902974843978882\n",
      "Epoch: 143, Loss: 0.07103494058052699\n",
      "Epoch: 144, Loss: 0.07155086348454158\n",
      "Epoch: 145, Loss: 0.0704437072078387\n",
      "Epoch: 146, Loss: 0.07187819729248683\n",
      "Epoch: 147, Loss: 0.07045851896206538\n",
      "Epoch: 148, Loss: 0.07270931700865428\n",
      "Epoch: 149, Loss: 0.07160560041666031\n",
      "Epoch: 150, Loss: 0.06880118449529012\n",
      "Epoch: 151, Loss: 0.0702273001273473\n",
      "Epoch: 152, Loss: 0.07195611546436946\n",
      "Epoch: 153, Loss: 0.07285109907388687\n",
      "Epoch: 154, Loss: 0.06976828475793202\n",
      "Epoch: 155, Loss: 0.0719874973098437\n",
      "Epoch: 156, Loss: 0.07268965989351273\n",
      "Epoch: 157, Loss: 0.07113862037658691\n",
      "Epoch: 158, Loss: 0.07227167238791783\n",
      "Epoch: 159, Loss: 0.07062390198310216\n",
      "Epoch: 160, Loss: 0.0710089976588885\n",
      "Epoch: 161, Loss: 0.07417641828457515\n",
      "Epoch: 162, Loss: 0.06985377768675487\n",
      "Epoch: 163, Loss: 0.07107443859179814\n",
      "Epoch: 164, Loss: 0.07328445216019948\n",
      "Epoch: 165, Loss: 0.07016024490197499\n",
      "Epoch: 166, Loss: 0.0705925648411115\n",
      "Epoch: 167, Loss: 0.07029614100853603\n",
      "Epoch: 168, Loss: 0.0704701840877533\n",
      "Epoch: 169, Loss: 0.0699962576230367\n",
      "Epoch: 170, Loss: 0.07005389779806137\n",
      "Epoch: 171, Loss: 0.07102341453234355\n",
      "Epoch: 172, Loss: 0.07267797986666362\n",
      "Epoch: 173, Loss: 0.06991103788216908\n",
      "Epoch: 174, Loss: 0.07100291053454082\n",
      "Epoch: 175, Loss: 0.0713299388686816\n",
      "Epoch: 176, Loss: 0.07136411219835281\n",
      "Epoch: 177, Loss: 0.07066863030195236\n",
      "Epoch: 178, Loss: 0.07053740819295247\n",
      "Epoch: 179, Loss: 0.07048017531633377\n",
      "Epoch: 180, Loss: 0.06992000838120778\n",
      "Epoch: 181, Loss: 0.07146382083495458\n",
      "Epoch: 182, Loss: 0.0715154434243838\n",
      "Epoch: 183, Loss: 0.07221205532550812\n",
      "Epoch: 184, Loss: 0.0726470947265625\n",
      "Epoch: 185, Loss: 0.07109714547793071\n",
      "Epoch: 186, Loss: 0.0726216584444046\n",
      "Epoch: 187, Loss: 0.07074165592590968\n",
      "Epoch: 188, Loss: 0.07115964343150456\n",
      "Epoch: 189, Loss: 0.07011141379674275\n",
      "Epoch: 190, Loss: 0.0732430915037791\n",
      "Epoch: 191, Loss: 0.07226516803105672\n",
      "Epoch: 192, Loss: 0.07166011383136113\n",
      "Epoch: 193, Loss: 0.07015858093897502\n",
      "Epoch: 194, Loss: 0.07077713559071223\n",
      "Epoch: 195, Loss: 0.07094109803438187\n",
      "Epoch: 196, Loss: 0.0698945273955663\n",
      "Epoch: 197, Loss: 0.07075851907332738\n",
      "Epoch: 198, Loss: 0.07056623697280884\n",
      "Epoch: 199, Loss: 0.07167396942774455\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss += loss_dist.item()\n",
    "\n",
    "        loss_dist.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss_dist: 0.691668967405955  Loss_CF: 1.8383878469467163\n",
      "Epoch: 2  Loss_dist: 0.6552395820617676  Loss_CF: 1.652824838956197\n",
      "Epoch: 3  Loss_dist: 0.6067019104957581  Loss_CF: 1.6865162054697673\n",
      "Epoch: 4  Loss_dist: 0.5793521602948507  Loss_CF: 1.6632184584935505\n",
      "Epoch: 5  Loss_dist: 0.4610901375611623  Loss_CF: 1.6824330886205037\n",
      "Epoch: 6  Loss_dist: 0.4167424440383911  Loss_CF: 1.4854201873143513\n",
      "Epoch: 7  Loss_dist: 0.35840973258018494  Loss_CF: 1.390825867652893\n",
      "Epoch: 8  Loss_dist: 0.34042950471242267  Loss_CF: 1.126849929491679\n",
      "Epoch: 9  Loss_dist: 0.3319297929604848  Loss_CF: 0.8661627173423767\n",
      "Epoch: 10  Loss_dist: 0.37773693601290387  Loss_CF: 0.6504358450571696\n",
      "Epoch: 11  Loss_dist: 0.5413018266359965  Loss_CF: 0.5196541746457418\n",
      "Epoch: 12  Loss_dist: 0.3893239200115204  Loss_CF: 0.5397603611151377\n",
      "Epoch: 13  Loss_dist: 0.3767375349998474  Loss_CF: 0.5104562143484751\n",
      "Epoch: 14  Loss_dist: 0.2589414318402608  Loss_CF: 0.7227440476417542\n",
      "Epoch: 15  Loss_dist: 0.24907932678858438  Loss_CF: 0.6605048775672913\n",
      "Epoch: 16  Loss_dist: 0.2281227856874466  Loss_CF: 0.5741018851598104\n",
      "Epoch: 17  Loss_dist: 0.27674296995004016  Loss_CF: 0.5679811437924703\n",
      "Epoch: 18  Loss_dist: 0.23957612117131552  Loss_CF: 0.5488430360953013\n",
      "Epoch: 19  Loss_dist: 0.20848442614078522  Loss_CF: 0.8247237602869669\n",
      "Epoch: 20  Loss_dist: 0.2271000196536382  Loss_CF: 0.5361159443855286\n",
      "Epoch: 21  Loss_dist: 0.2537950277328491  Loss_CF: 0.5083170334498087\n",
      "Epoch: 22  Loss_dist: 0.28208047648270923  Loss_CF: 0.530261218547821\n",
      "Epoch: 23  Loss_dist: 0.2404071440299352  Loss_CF: 0.5259271562099457\n",
      "Epoch: 24  Loss_dist: 0.23829797406991324  Loss_CF: 0.5521542032559713\n",
      "Epoch: 25  Loss_dist: 0.21922777593135834  Loss_CF: 0.5528707702954611\n",
      "Epoch: 26  Loss_dist: 0.26251096030076343  Loss_CF: 0.5125043590863546\n",
      "Epoch: 27  Loss_dist: 0.1945357322692871  Loss_CF: 0.5502277612686157\n",
      "Epoch: 28  Loss_dist: 0.20405374467372894  Loss_CF: 0.5361810326576233\n",
      "Epoch: 29  Loss_dist: 0.23976156612237295  Loss_CF: 0.5389369527498881\n",
      "Epoch: 30  Loss_dist: 0.20183061559995016  Loss_CF: 0.5220603346824646\n",
      "Epoch: 31  Loss_dist: 0.188685675462087  Loss_CF: 0.5761382381121317\n",
      "Epoch: 32  Loss_dist: 0.19072120388348898  Loss_CF: 0.48976083596547443\n",
      "Epoch: 33  Loss_dist: 0.19915721317132315  Loss_CF: 0.5849321881930033\n",
      "Epoch: 34  Loss_dist: 0.21791843076546988  Loss_CF: 0.4834199051062266\n",
      "Epoch: 35  Loss_dist: 0.19112378855546316  Loss_CF: 0.5313853820164999\n",
      "Epoch: 36  Loss_dist: 0.20440049469470978  Loss_CF: 0.5457590520381927\n",
      "Epoch: 37  Loss_dist: 0.21029387414455414  Loss_CF: 0.4872518579165141\n",
      "Epoch: 38  Loss_dist: 0.19607355197270712  Loss_CF: 0.48111433784166974\n",
      "Epoch: 39  Loss_dist: 0.20275801420211792  Loss_CF: 0.5295992890993754\n",
      "Epoch: 40  Loss_dist: 0.21378254890441895  Loss_CF: 0.48284606138865155\n",
      "Epoch: 41  Loss_dist: 0.20699368913968405  Loss_CF: 0.5432889461517334\n",
      "Epoch: 42  Loss_dist: 0.19559252758820853  Loss_CF: 0.55870521068573\n",
      "Epoch: 43  Loss_dist: 0.17053441206614176  Loss_CF: 0.5648359457651774\n",
      "Epoch: 44  Loss_dist: 0.18351679046948752  Loss_CF: 0.4909101227919261\n",
      "Epoch: 45  Loss_dist: 0.18242977559566498  Loss_CF: 0.5408855676651001\n",
      "Epoch: 46  Loss_dist: 0.19923000037670135  Loss_CF: 0.48224059740702313\n",
      "Epoch: 47  Loss_dist: 0.18778801957766214  Loss_CF: 0.5058782796065012\n",
      "Epoch: 48  Loss_dist: 0.18237319588661194  Loss_CF: 0.5122256974379221\n",
      "Epoch: 49  Loss_dist: 0.18535360197226206  Loss_CF: 0.5341958900292715\n",
      "Epoch: 50  Loss_dist: 0.1845457802216212  Loss_CF: 0.5319637556870779\n",
      "Epoch: 51  Loss_dist: 0.19749563932418823  Loss_CF: 0.5514031151930491\n",
      "Epoch: 52  Loss_dist: 0.18975931406021118  Loss_CF: 0.4947267174720764\n",
      "Epoch: 53  Loss_dist: 0.20556539793809256  Loss_CF: 0.492871214946111\n",
      "Epoch: 54  Loss_dist: 0.1660663237174352  Loss_CF: 0.5223288436730703\n",
      "Epoch: 55  Loss_dist: 0.16183467209339142  Loss_CF: 0.5037262737751007\n",
      "Epoch: 56  Loss_dist: 0.16682655115922293  Loss_CF: 0.5525611440340678\n",
      "Epoch: 57  Loss_dist: 0.17392479379971823  Loss_CF: 0.5383949875831604\n",
      "Epoch: 58  Loss_dist: 0.19239703317483267  Loss_CF: 0.5070809026559194\n",
      "Epoch: 59  Loss_dist: 0.20453920463720957  Loss_CF: 0.4994174639383952\n",
      "Epoch: 60  Loss_dist: 0.16728233297665915  Loss_CF: 0.5758764147758484\n",
      "Epoch: 61  Loss_dist: 0.20695176223913828  Loss_CF: 0.4822065432866414\n",
      "Epoch: 62  Loss_dist: 0.20356959104537964  Loss_CF: 0.5169638593991598\n",
      "Epoch: 63  Loss_dist: 0.1972333788871765  Loss_CF: 0.49174273014068604\n",
      "Epoch: 64  Loss_dist: 0.18387349446614584  Loss_CF: 0.4580130378405253\n",
      "Epoch: 65  Loss_dist: 0.16719328860441843  Loss_CF: 0.4870905081431071\n",
      "Epoch: 66  Loss_dist: 0.1635491152604421  Loss_CF: 0.5617816547552744\n",
      "Epoch: 67  Loss_dist: 0.17513424654801688  Loss_CF: 0.48024025559425354\n",
      "Epoch: 68  Loss_dist: 0.18826015293598175  Loss_CF: 0.4623684187730153\n",
      "Epoch: 69  Loss_dist: 0.17680756251017252  Loss_CF: 0.532767136891683\n",
      "Epoch: 70  Loss_dist: 0.18574800590674082  Loss_CF: 0.5108743111292521\n",
      "Epoch: 71  Loss_dist: 0.2177391548951467  Loss_CF: 0.5198428730169932\n",
      "Epoch: 72  Loss_dist: 0.19320167104403177  Loss_CF: 0.46837834517161053\n",
      "Epoch: 73  Loss_dist: 0.15876678625742593  Loss_CF: 0.5558742185433706\n",
      "Epoch: 74  Loss_dist: 0.1662298341592153  Loss_CF: 0.5809158285458883\n",
      "Epoch: 75  Loss_dist: 0.19291340311368307  Loss_CF: 0.4975064198176066\n",
      "Epoch: 76  Loss_dist: 0.1810662398735682  Loss_CF: 0.5179875195026398\n",
      "Epoch: 77  Loss_dist: 0.19097655514876047  Loss_CF: 0.4670921266078949\n",
      "Epoch: 78  Loss_dist: 0.16251355906327566  Loss_CF: 0.5408084293206533\n",
      "Epoch: 79  Loss_dist: 0.17681339383125305  Loss_CF: 0.5606258710225424\n",
      "Epoch: 80  Loss_dist: 0.17837484180927277  Loss_CF: 0.47279537717501324\n",
      "Epoch: 81  Loss_dist: 0.16309026877085367  Loss_CF: 0.6118083993593851\n",
      "Epoch: 82  Loss_dist: 0.20669686794281006  Loss_CF: 0.4495243728160858\n",
      "Epoch: 83  Loss_dist: 0.22177431484063467  Loss_CF: 0.49073758721351624\n",
      "Epoch: 84  Loss_dist: 0.24232267340024313  Loss_CF: 0.5077143708864847\n",
      "Epoch: 85  Loss_dist: 0.18959274888038635  Loss_CF: 0.5202361543973287\n",
      "Epoch: 86  Loss_dist: 0.16537829240163168  Loss_CF: 0.5068913996219635\n",
      "Epoch: 87  Loss_dist: 0.16553199787934622  Loss_CF: 0.5089820424715678\n",
      "Epoch: 88  Loss_dist: 0.1707404504219691  Loss_CF: 0.4651554922262828\n",
      "Epoch: 89  Loss_dist: 0.19722466667493185  Loss_CF: 0.4679301083087921\n",
      "Epoch: 90  Loss_dist: 0.1748723437388738  Loss_CF: 0.49242696166038513\n",
      "Epoch: 91  Loss_dist: 0.16375082731246948  Loss_CF: 0.5094761351744334\n",
      "Epoch: 92  Loss_dist: 0.17148320376873016  Loss_CF: 0.4846594234307607\n",
      "Epoch: 93  Loss_dist: 0.19112150371074677  Loss_CF: 0.5058495998382568\n",
      "Epoch: 94  Loss_dist: 0.17804627120494843  Loss_CF: 0.5478413105010986\n",
      "Epoch: 95  Loss_dist: 0.17093145847320557  Loss_CF: 0.5119277536869049\n",
      "Epoch: 96  Loss_dist: 0.15738239387671152  Loss_CF: 0.5774512390295664\n",
      "Epoch: 97  Loss_dist: 0.20388259490331015  Loss_CF: 0.5082236627737681\n",
      "Epoch: 98  Loss_dist: 0.1848626285791397  Loss_CF: 0.49971774220466614\n",
      "Epoch: 99  Loss_dist: 0.18884630997975668  Loss_CF: 0.48923422892888385\n",
      "Epoch: 100  Loss_dist: 0.17079567909240723  Loss_CF: 0.49302947521209717\n",
      "Epoch: 101  Loss_dist: 0.14517982800801596  Loss_CF: 0.6009345849355062\n",
      "Epoch: 102  Loss_dist: 0.1813316891590754  Loss_CF: 0.4608854353427887\n",
      "Epoch: 103  Loss_dist: 0.15657155215740204  Loss_CF: 0.5435713529586792\n",
      "Epoch: 104  Loss_dist: 0.18224301437536874  Loss_CF: 0.4950515925884247\n",
      "Epoch: 105  Loss_dist: 0.19479880730311075  Loss_CF: 0.5143077174822489\n",
      "Epoch: 106  Loss_dist: 0.16972060998280844  Loss_CF: 0.5029062827428182\n",
      "Epoch: 107  Loss_dist: 0.17157144844532013  Loss_CF: 0.46011920770009357\n",
      "Epoch: 108  Loss_dist: 0.1773367871840795  Loss_CF: 0.457789013783137\n",
      "Epoch: 109  Loss_dist: 0.17284666995207468  Loss_CF: 0.4977025290330251\n",
      "Epoch: 110  Loss_dist: 0.16559713085492453  Loss_CF: 0.47177862127621967\n",
      "Epoch: 111  Loss_dist: 0.15633728603521982  Loss_CF: 0.5185263256231943\n",
      "Epoch: 112  Loss_dist: 0.1765233278274536  Loss_CF: 0.4734791119893392\n",
      "Epoch: 113  Loss_dist: 0.177950049440066  Loss_CF: 0.47492531935373944\n",
      "Epoch: 114  Loss_dist: 0.18566411236921945  Loss_CF: 0.47245822350184125\n",
      "Epoch: 115  Loss_dist: 0.18941541016101837  Loss_CF: 0.5068812966346741\n",
      "Epoch: 116  Loss_dist: 0.16210016111532846  Loss_CF: 0.46592506766319275\n",
      "Epoch: 117  Loss_dist: 0.14824782808621725  Loss_CF: 0.5199577411015829\n",
      "Epoch: 118  Loss_dist: 0.15722817182540894  Loss_CF: 0.5726919968922933\n",
      "Epoch: 119  Loss_dist: 0.16173411905765533  Loss_CF: 0.4993038972218831\n",
      "Epoch: 120  Loss_dist: 0.21080716451009116  Loss_CF: 0.47314215699831647\n",
      "Epoch: 121  Loss_dist: 0.20252389212449393  Loss_CF: 0.454390952984492\n",
      "Epoch: 122  Loss_dist: 0.17116431395212808  Loss_CF: 0.5188740293184916\n",
      "Epoch: 123  Loss_dist: 0.16961040596167246  Loss_CF: 0.47576770186424255\n",
      "Epoch: 124  Loss_dist: 0.1550267438093821  Loss_CF: 0.5379098455111185\n",
      "Epoch: 125  Loss_dist: 0.15363969405492148  Loss_CF: 0.48832552631696063\n",
      "Epoch: 126  Loss_dist: 0.1785116841395696  Loss_CF: 0.47508835792541504\n",
      "Epoch: 127  Loss_dist: 0.1796099990606308  Loss_CF: 0.4881122608979543\n",
      "Epoch: 128  Loss_dist: 0.16445521513621011  Loss_CF: 0.4659756124019623\n",
      "Epoch: 129  Loss_dist: 0.16976996262868246  Loss_CF: 0.43694626291592914\n",
      "Epoch: 130  Loss_dist: 0.1712078352769216  Loss_CF: 0.483768771092097\n",
      "Epoch: 131  Loss_dist: 0.1823538194100062  Loss_CF: 0.4340803821881612\n",
      "Epoch: 132  Loss_dist: 0.1691710203886032  Loss_CF: 0.5033639868100485\n",
      "Epoch: 133  Loss_dist: 0.16152123113473257  Loss_CF: 0.48004209995269775\n",
      "Epoch: 134  Loss_dist: 0.17105431854724884  Loss_CF: 0.47614601254463196\n",
      "Epoch: 135  Loss_dist: 0.16194042066733041  Loss_CF: 0.5419968366622925\n",
      "Epoch: 136  Loss_dist: 0.17902463674545288  Loss_CF: 0.48243587215741474\n",
      "Epoch: 137  Loss_dist: 0.17265327274799347  Loss_CF: 0.48140382766723633\n",
      "Epoch: 138  Loss_dist: 0.17537038028240204  Loss_CF: 0.43021801114082336\n",
      "Epoch: 139  Loss_dist: 0.1937903811534246  Loss_CF: 0.4613930682341258\n",
      "Epoch: 140  Loss_dist: 0.18663878738880157  Loss_CF: 0.5104506512482961\n",
      "Epoch: 141  Loss_dist: 0.16689142088095346  Loss_CF: 0.48970943689346313\n",
      "Epoch: 142  Loss_dist: 0.1573395530382792  Loss_CF: 0.4991021454334259\n",
      "Epoch: 143  Loss_dist: 0.17014463245868683  Loss_CF: 0.44713521003723145\n",
      "Epoch: 144  Loss_dist: 0.16732084254423776  Loss_CF: 0.47041510542233783\n",
      "Epoch: 145  Loss_dist: 0.22276331981023154  Loss_CF: 0.42121774951616925\n",
      "Epoch: 146  Loss_dist: 0.20673265556494394  Loss_CF: 0.4159766534964244\n",
      "Epoch: 147  Loss_dist: 0.1675973435242971  Loss_CF: 0.5035866300264994\n",
      "Epoch: 148  Loss_dist: 0.17057757576306662  Loss_CF: 0.42155492305755615\n",
      "Epoch: 149  Loss_dist: 0.16279281179110208  Loss_CF: 0.420718252658844\n",
      "Epoch: 150  Loss_dist: 0.16487403710683188  Loss_CF: 0.46507637699445087\n",
      "Epoch: 151  Loss_dist: 0.18614334364732107  Loss_CF: 0.38821175694465637\n",
      "Epoch: 152  Loss_dist: 0.22177391250928244  Loss_CF: 0.5071395635604858\n",
      "Epoch: 153  Loss_dist: 0.16924663881460825  Loss_CF: 0.431932270526886\n",
      "Epoch: 154  Loss_dist: 0.16761557261149088  Loss_CF: 0.43037999669710797\n",
      "Epoch: 155  Loss_dist: 0.1588338017463684  Loss_CF: 0.45182440678278607\n",
      "Epoch: 156  Loss_dist: 0.18579269448916116  Loss_CF: 0.481450617313385\n",
      "Epoch: 157  Loss_dist: 0.15283131102720895  Loss_CF: 0.5426024794578552\n",
      "Epoch: 158  Loss_dist: 0.16023444632689157  Loss_CF: 0.45542941490809125\n",
      "Epoch: 159  Loss_dist: 0.16565032800038657  Loss_CF: 0.43803704778353375\n",
      "Epoch: 160  Loss_dist: 0.18935350080331168  Loss_CF: 0.44410820802052814\n",
      "Epoch: 161  Loss_dist: 0.17839549481868744  Loss_CF: 0.47196569045384723\n",
      "Epoch: 162  Loss_dist: 0.17958933115005493  Loss_CF: 0.4610946873823802\n",
      "Epoch: 163  Loss_dist: 0.17222543557484946  Loss_CF: 0.4463119109471639\n",
      "Epoch: 164  Loss_dist: 0.16894581417242685  Loss_CF: 0.49021778504053753\n",
      "Epoch: 165  Loss_dist: 0.17068278789520264  Loss_CF: 0.4631807307402293\n",
      "Epoch: 166  Loss_dist: 0.16726524631182352  Loss_CF: 0.44340280691782635\n",
      "Epoch: 167  Loss_dist: 0.1560709128777186  Loss_CF: 0.4565926094849904\n",
      "Epoch: 168  Loss_dist: 0.1794301321109136  Loss_CF: 0.4525538782278697\n",
      "Epoch: 169  Loss_dist: 0.22122849027315775  Loss_CF: 0.4017840425173442\n",
      "Epoch: 170  Loss_dist: 0.18751556177934012  Loss_CF: 0.4630723297595978\n",
      "Epoch: 171  Loss_dist: 0.1971748173236847  Loss_CF: 0.42298651734987897\n",
      "Epoch: 172  Loss_dist: 0.16359821955362955  Loss_CF: 0.4773489435513814\n",
      "Epoch: 173  Loss_dist: 0.1591121256351471  Loss_CF: 0.4265782634417216\n",
      "Epoch: 174  Loss_dist: 0.1915456602970759  Loss_CF: 0.5064195791880289\n",
      "Epoch: 175  Loss_dist: 0.17545641958713531  Loss_CF: 0.48274006446202594\n",
      "Epoch: 176  Loss_dist: 0.1780791332324346  Loss_CF: 0.539655456940333\n",
      "Epoch: 177  Loss_dist: 0.18268473943074545  Loss_CF: 0.4474596480528514\n",
      "Epoch: 178  Loss_dist: 0.16975846389929453  Loss_CF: 0.4087279935677846\n",
      "Epoch: 179  Loss_dist: 0.16638338069121042  Loss_CF: 0.4409886399904887\n",
      "Epoch: 180  Loss_dist: 0.19692979753017426  Loss_CF: 0.42123304804166156\n",
      "Epoch: 181  Loss_dist: 0.16223326325416565  Loss_CF: 0.4662538170814514\n",
      "Epoch: 182  Loss_dist: 0.20239470899105072  Loss_CF: 0.48038676381111145\n",
      "Epoch: 183  Loss_dist: 0.16457690298557281  Loss_CF: 0.46136802434921265\n",
      "Epoch: 184  Loss_dist: 0.19257624944051108  Loss_CF: 0.436906099319458\n",
      "Epoch: 185  Loss_dist: 0.17182481785615286  Loss_CF: 0.4428682029247284\n",
      "Epoch: 186  Loss_dist: 0.17093719045321146  Loss_CF: 0.4355907738208771\n",
      "Epoch: 187  Loss_dist: 0.17013315856456757  Loss_CF: 0.450177401304245\n",
      "Epoch: 188  Loss_dist: 0.16791803141434988  Loss_CF: 0.4124856193860372\n",
      "Epoch: 189  Loss_dist: 0.17050385475158691  Loss_CF: 0.45531890789667767\n",
      "Epoch: 190  Loss_dist: 0.1842738538980484  Loss_CF: 0.40813808639844257\n",
      "Epoch: 191  Loss_dist: 0.19784733156363168  Loss_CF: 0.40533260504404706\n",
      "Epoch: 192  Loss_dist: 0.1751892069975535  Loss_CF: 0.41536091764767963\n",
      "Epoch: 193  Loss_dist: 0.1603496124347051  Loss_CF: 0.41738365093866986\n",
      "Epoch: 194  Loss_dist: 0.16780944665273032  Loss_CF: 0.42897533377011615\n",
      "Epoch: 195  Loss_dist: 0.17022907733917236  Loss_CF: 0.48422518372535706\n",
      "Epoch: 196  Loss_dist: 0.18087452153364816  Loss_CF: 0.4267410635948181\n",
      "Epoch: 197  Loss_dist: 0.1898116817077001  Loss_CF: 0.4360559682051341\n",
      "Epoch: 198  Loss_dist: 0.183873251080513  Loss_CF: 0.4436265528202057\n",
      "Epoch: 199  Loss_dist: 0.18404618899027506  Loss_CF: 0.42877596616744995\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise_CF = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise_CF.parameters(), lr=0.01)\n",
    "CF_weight = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss_dist = 0.0\n",
    "    running_loss_CF = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise_CF(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss_dist += loss_dist.item()\n",
    "\n",
    "        explainee.eval()\n",
    "        explainee_pred = F.softmax(explainee(pred_graph), dim=-1)\n",
    "        class_prob = explainee_pred[torch.arange(explainee_pred.shape[0]), \n",
    "                                    graphs.y]\n",
    "        loss_CF = (-1 * torch.log(1 - class_prob)).mean()\n",
    "        running_loss_CF += loss_CF.item()\n",
    "\n",
    "        loss = 1.0 * loss_dist + CF_weight * loss_CF\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\", \n",
    "        f\" Loss_dist: {running_loss_dist / len(train_loader)}\", \n",
    "        f\" Loss_CF: {running_loss_CF / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(496.7574)\n",
      "tensor(26.) tensor(42.8113, grad_fn=<SumBackward0>)\n",
      "tensor([[0.7762, 0.2238]], grad_fn=<SoftmaxBackward0>) tensor([[0.3045, 0.6955]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Inference \n",
    "non_mut_example = pyg.data.Batch.from_data_list([test_data_list_0[0]])\n",
    "non_mut_noised = forward_diffusion_sample(non_mut_example, torch.tensor([T-1])) \n",
    "print(non_mut_noised.edge_attr.sum())\n",
    "pred_weight, non_mut_denoised = model_denoise_CF(non_mut_noised, torch.tensor([T-1]))\n",
    "print(test_data_list_0[0].edge_attr.sum(), non_mut_denoised.edge_attr.sum())\n",
    "print(torch.softmax(explainee(test_data_list_0[0]), dim=-1), \n",
    "      torch.softmax(explainee(non_mut_denoised), dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_level_sampling(N: int, T: int,\n",
    "                         obs_graphs: pyg.data.Batch, \n",
    "                         target: int, \n",
    "                         noise_level=torch.tensor([T])) -> pyg.data.Batch:\n",
    "    # Initial random adjacency matrix. \n",
    "    K = obs_graphs.num_graphs # batch size / num candidates. \n",
    "    obs_nodes = obs_graphs.x.shape[0] // K # max num nodes in observed graph.\n",
    "\n",
    "    Gr_T = torch.zeros((K, obs_nodes, obs_nodes))\n",
    "    for batch in range(K):\n",
    "        idx = [(i, j) for i in range(obs_nodes) for j in range(obs_nodes)]\n",
    "        rand_idx = torch.randperm(len(idx))[:N**2]\n",
    "        for index in rand_idx:\n",
    "            i, j = idx[index]\n",
    "            Gr_T[batch, i, j] = 0.5 \n",
    "\n",
    "    assert Gr_T.sum() == 0.5 * K * N**2\n",
    "    Gr_t = Gr_T.reshape(K, obs_nodes**2)\n",
    "\n",
    "    for t in range(T):\n",
    "        noised_graph = pyg.data.Batch(x=obs_graphs.x, \n",
    "                                    edge_index=obs_graphs.edge_index,\n",
    "                                    edge_attr=Gr_t, \n",
    "                                    y=obs_graphs.y, batch=obs_graphs.batch)\n",
    "\n",
    "        candidate_weights, candidates = model_denoise(noised_graph, noise_level) \n",
    "        explainee.eval()\n",
    "        candidate_score = F.softmax(explainee(candidates), dim=-1)[:, target]\n",
    "        candidate_weights = candidate_weights.reshape(K, obs_nodes**2)\n",
    "        best_idx = torch.argmax(candidate_score)\n",
    "        best_weights = candidate_weights[best_idx, :]\n",
    "        best_weights = best_weights.repeat(K, 1)\n",
    "        G_0 = pyg.data.Batch(x=obs_graphs.x, \n",
    "                             edge_index=obs_graphs.edge_index,\n",
    "                             edge_attr=best_weights, \n",
    "                             y=obs_graphs.y, batch=obs_graphs.batch)\n",
    "        print(Gr_t.sum() / K)\n",
    "        Gr_t = forward_diffusion_sample(G_0, noise_level - t).edge_attr\n",
    "        print(noise_level - t)\n",
    "        print(Gr_t.sum() / K)\n",
    "\n",
    "        # print(Gr_t.sum())\n",
    "        # Gr_t = best_weights.repeat(K, 1)\n",
    "        # print(Gr_t.sum())\n",
    "\n",
    "    explanation_graphs = pyg.data.Batch(x=obs_graphs.x, \n",
    "                                edge_index=obs_graphs.edge_index,\n",
    "                                edge_attr=Gr_t, batch=obs_graphs.batch)\n",
    "\n",
    "    _, explanation_graphs = model_denoise(explanation_graphs, noise_level) \n",
    "\n",
    "    return explanation_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m obs_graph \u001b[38;5;241m=\u001b[39m pyg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch\u001b[38;5;241m.\u001b[39mfrom_data_list(train_data_list_0[:\u001b[38;5;241m30\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m explanation_graphs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_level_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m49\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mobs_graphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(explanation_graphs\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m30\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m8\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(F\u001b[38;5;241m.\u001b[39msoftmax(explainee(explanation_graphs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mmodel_level_sampling\u001b[1;34m(N, T, obs_graphs, target, noise_level)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m     21\u001b[0m     noised_graph \u001b[38;5;241m=\u001b[39m pyg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mBatch(x\u001b[38;5;241m=\u001b[39mobs_graphs\u001b[38;5;241m.\u001b[39mx, \n\u001b[0;32m     22\u001b[0m                                 edge_index\u001b[38;5;241m=\u001b[39mobs_graphs\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[0;32m     23\u001b[0m                                 edge_attr\u001b[38;5;241m=\u001b[39mGr_t, \n\u001b[0;32m     24\u001b[0m                                 y\u001b[38;5;241m=\u001b[39mobs_graphs\u001b[38;5;241m.\u001b[39my, batch\u001b[38;5;241m=\u001b[39mobs_graphs\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m---> 26\u001b[0m     candidate_weights, candidates \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     27\u001b[0m     explainee\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     28\u001b[0m     candidate_score \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(explainee(candidates), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, target]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mdenoising_model.forward\u001b[1;34m(self, noised_graphs, t)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noised_graphs, t):\n\u001b[0;32m     18\u001b[0m     x, edge_index, batch, edge_weight \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     19\u001b[0m         noised_graphs\u001b[38;5;241m.\u001b[39mx, noised_graphs\u001b[38;5;241m.\u001b[39medge_index, \n\u001b[0;32m     20\u001b[0m         noised_graphs\u001b[38;5;241m.\u001b[39mbatch, noised_graphs\u001b[38;5;241m.\u001b[39medge_weight\n\u001b[0;32m     21\u001b[0m     )\n\u001b[1;32m---> 23\u001b[0m     time_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index, edge_weight)\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "obs_graph = pyg.data.Batch.from_data_list(train_data_list_0[:30])\n",
    "explanation_graphs = model_level_sampling(N=8, T=49, \n",
    "                                          obs_graphs=obs_graph, \n",
    "                                          target=0)\n",
    "print(explanation_graphs.edge_attr.sum() / (30 * (8**2)))\n",
    "print(F.softmax(explainee(explanation_graphs), dim=-1))\n",
    "F.softmax(explainee(obs_graph), dim=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
