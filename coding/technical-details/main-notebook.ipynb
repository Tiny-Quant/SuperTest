{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import pygmtools as pygm\n",
    "pygm.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Meta Data\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Variables. \n",
    "max_num_nodes = 30 # 28 in the full dataset.  \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUTAG Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data. \n",
    "from torch_geometric.datasets import TUDataset\n",
    "data_raw = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "# Shuffle.\n",
    "data_raw = data_raw.shuffle()\n",
    "\n",
    "# Split.\n",
    "train_data = data_raw[:150]\n",
    "test_data = data_raw[150:]\n",
    "\n",
    "def preprocess_MUTAG(data: TUDataset, max_num_nodes) -> pyg.data.Data:\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Pad node features.\n",
    "    padded_x = torch.zeros((max_num_nodes, data.x.size(1)))\n",
    "    padded_x[:num_nodes] = data.x\n",
    "\n",
    "    # Relax edges to weights. \n",
    "    padded_adj = torch.zeros((max_num_nodes, max_num_nodes))\n",
    "    padded_adj[:num_nodes, :num_nodes] = (\n",
    "        pyg.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
    "    )\n",
    "    edge_index, edge_weight = pygm.utils.dense_to_sparse(padded_adj + 1)\n",
    "    edge_index = edge_index.transpose(0, 1)\n",
    "    edge_weight = edge_weight.squeeze(0)\n",
    "\n",
    "    # Wrap in data object.\n",
    "    preprocessed_data = pyg.data.Data(x=padded_x, \n",
    "                                      edge_index=edge_index,\n",
    "                                      edge_attr=edge_weight - 1,\n",
    "                                      y=data.y)\n",
    "\n",
    "    return preprocessed_data \n",
    "\n",
    "# Create data lists.\n",
    "train_data_list = []\n",
    "train_data_list_0 = []\n",
    "train_data_list_1 = []\n",
    "test_data_list = []\n",
    "test_data_list_0 = []\n",
    "test_data_list_1 = []\n",
    "\n",
    "for graph in train_data:\n",
    "    train_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        train_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        train_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "for graph in test_data:\n",
    "    test_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        test_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        test_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = pyg.loader.DataLoader(train_data_list, batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "test_loader = pyg.loader.DataLoader(test_data_list, batch_size=batch_size, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainee GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNWeighted(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNWeighted, self).__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(7, hidden_channels) # 7 node features.\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 2) # 2 classes.\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        )\n",
    "\n",
    "        # 1. Node embeddings.\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Pooling.\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Prediction.\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 2 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 3 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 4 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 5 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 6 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 7 Train Accuracy: 0.68 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 8 Train Accuracy: 0.7066666666666667 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 9 Train Accuracy: 0.7133333333333334 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 10 Train Accuracy: 0.7333333333333333 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 11 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 12 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 13 Train Accuracy: 0.78 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 14 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 15 Train Accuracy: 0.76 Test Accuracy: 0.7368421052631579\n",
      "Epoch: 16 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 17 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 18 Train Accuracy: 0.8 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 19 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 20 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 21 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 22 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 23 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 24 Train Accuracy: 0.84 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 25 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 26 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 27 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 28 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 29 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 30 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 31 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 32 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 33 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 34 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 35 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 36 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 37 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 38 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 39 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 40 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 41 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 42 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 43 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 44 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 45 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 46 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 47 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 48 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 49 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 50 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 51 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 52 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 53 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 54 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 55 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 56 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 57 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 58 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 59 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 60 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 61 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 62 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 63 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 64 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 65 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 66 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 67 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 68 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 69 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 70 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 71 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 72 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 73 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 74 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 75 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 76 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 77 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 78 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 79 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 80 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 81 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 82 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 83 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 84 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 85 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 86 Train Accuracy: 0.82 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 87 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 88 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 89 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 90 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 91 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 92 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 93 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 94 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 95 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 96 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 97 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 98 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 99 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 100 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 101 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 102 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 103 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 104 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 105 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 106 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 107 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 108 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 109 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 110 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 111 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 112 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 113 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 114 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 115 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 116 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 117 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 118 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 119 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 120 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 121 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 122 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 123 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 124 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 125 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 126 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 127 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 128 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 129 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 130 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 131 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 132 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 133 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 134 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 135 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 136 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 137 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 138 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 139 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 140 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 141 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 142 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 143 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 144 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 145 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 146 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 147 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 148 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 149 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 150 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 151 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 152 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 153 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 154 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 155 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 156 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 157 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 158 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 159 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 160 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 161 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 162 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 163 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 164 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 165 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 166 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 167 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 168 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 169 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 170 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 171 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 172 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 173 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 174 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 175 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 176 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 177 Train Accuracy: 0.82 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 178 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 179 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 180 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 181 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 182 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 183 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 184 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 185 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 186 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 187 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 188 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 189 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 190 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 191 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 192 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 193 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 194 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 195 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 196 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 197 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 198 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 199 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 200 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Training Explainee.\n",
    "explainee = GCNWeighted(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(explainee.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data_loader): \n",
    "    explainee.train()\n",
    "\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def explainee_accuracy(data_loader):\n",
    "    explainee.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(1, 201): \n",
    "    train(train_loader)\n",
    "    train_accuracy = explainee_accuracy(train_loader)\n",
    "    test_accuracy = explainee_accuracy(test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch} Train Accuracy: {train_accuracy} \" + \n",
    "          f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,  17],\n",
       "       [ 10, 115]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "full_batch = pyg.data.Batch.from_data_list(test_data_list + train_data_list)\n",
    "explainee.eval()\n",
    "preds = explainee(full_batch).argmax(dim=1).numpy()\n",
    "targets = full_batch.y.numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(targets, preds)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "betas = []\n",
    "\n",
    "for batch in loader:\n",
    "    adj_batch = func(batch)\n",
    "    t ~ U[1, 50]\n",
    "    noised_adj = func(adj_batch)\n",
    "\n",
    "    pred_adj = model(noised_adj, t)\n",
    "\n",
    "    CrtEnt(adj_batch, pred_adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "T = 50\n",
    "time_embed_dim = 10\n",
    "num_node_feats = 7\n",
    "\n",
    "betas = torch.linspace(start=0.001, end=0.1, steps=T)\n",
    "beta_bars = []\n",
    "cum_prod = 1\n",
    "\n",
    "for beta in betas:\n",
    "    cum_prod *= (1 - 2*beta)\n",
    "    beta_bars.append(0.5 - 0.5 * cum_prod)\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(graphs: pyg.data.Batch, \n",
    "                             t: int) -> pyg.data.Batch:\n",
    "    \"\"\"\n",
    "    Input: Batch of observed graphs.\n",
    "    Output: Batch of noised graphs.\n",
    "    \"\"\"\n",
    "    edge_weight = graphs.edge_attr\n",
    "\n",
    "    transition_probs = torch.full_like(edge_weight, beta_bars[t])\n",
    "    transition_dist = torch.distributions.RelaxedBernoulli(\n",
    "        temperature=0.15, probs=transition_probs\n",
    "    )\n",
    "\n",
    "    noised_edge_weights = torch.abs(\n",
    "        edge_weight + transition_dist.rsample()\n",
    "    )\n",
    "    noised_graph = pyg.data.Batch(x=graphs.x, \n",
    "                                  edge_index=graphs.edge_index,\n",
    "                                  edge_attr=noised_edge_weights,  \n",
    "                                  y=graphs.y, batch=graphs.batch)\n",
    "\n",
    "    return noised_graph\n",
    "\n",
    "    # # [b, n, n]\n",
    "    # adj_batch = pyg.utils.to_dense_adj(graphs.edge_index, batch=graphs.batch, \n",
    "    #                                    max_num_nodes=max_num_nodes)\n",
    "    #                                    #edge_attr = graphs.edge_attr)\n",
    "    # # D4 uses the mask somehow.\n",
    "    # x_batch, node_feat_mask = pyg.utils.to_dense_batch(graphs.x, graphs.batch, \n",
    "    #                                    max_num_nodes=max_num_nodes)    \n",
    "    \n",
    "    # transition_probs = torch.full_like(adj_batch, beta_bars[t])\n",
    "\n",
    "    # # Symmetrically applies noise - treats edges as undirected.\n",
    "    # noise_upper = torch.bernoulli(transition_probs).triu(diagonal=1)\n",
    "    # noise_lower = noise_upper.transpose(-1, -2)\n",
    "    # noised_adj_batch = torch.abs(adj_batch + noise_upper + noise_lower)\n",
    "    # noised_adj_batch_sparse, _ = pyg.utils.dense_to_sparse(noised_adj_batch)\n",
    "    # noised_graph = pyg.data.Batch(x=graphs.x, \n",
    "    #                               edge_index=noised_adj_batch_sparse,\n",
    "    #                               edge_attr=graphs.edge_attr, \n",
    "    #                               y=graphs.y, batch=graphs.batch)\n",
    "\n",
    "    # return noised_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model(nn.Module):\n",
    "    def __init__(self, T, time_embed_dim, \n",
    "                 num_node_feats, h1=10, h2=50, h3=30, p_dropout=0.5): \n",
    "        super(denoising_model, self).__init__()\n",
    "\n",
    "        self.time_embedder = nn.Embedding(num_embeddings=T, \n",
    "                                          embedding_dim=time_embed_dim) \n",
    "\n",
    "        self.conv1 = pyg.nn.GCNConv(num_node_feats, h1)\n",
    "        self.conv2 = pyg.nn.GCNConv(h1, h2)\n",
    "        self.conv3 = pyg.nn.GCNConv(h2, h3)\n",
    "\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lin = nn.Linear(h3, max_num_nodes**2) # predicting weights.\n",
    "\n",
    "    def forward(self, noised_graphs, t):\n",
    "        \n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            noised_graphs.x, noised_graphs.edge_index, \n",
    "            noised_graphs.batch, noised_graphs.edge_weight\n",
    "        )\n",
    "\n",
    "        time_embedding = self.time_embedder(t)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        \n",
    "        x = x + time_embedding\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.lin(x)\n",
    "        x = F.sigmoid(x) \n",
    "\n",
    "        pred_weights = x.reshape(-1)\n",
    "        pred_graph = pyg.data.Batch(x=noised_graphs.x, \n",
    "                                    edge_index=noised_graphs.edge_index,\n",
    "                                    edge_attr=pred_weights,\n",
    "                                    batch=noised_graphs.batch)\n",
    "    \n",
    "        return [pred_weights, pred_graph]\n",
    "\n",
    "        # pred_adj, _ = pyg.utils.to_dense_batch(x, noised_graphs.batch, \n",
    "        #                                 max_num_nodes=max_num_nodes)\n",
    "\n",
    "        # pred_adj_sparse = torch.bernoulli(pred_adj)\n",
    "        # pred_adj_sparse, _ = pyg.utils.dense_to_sparse(pred_adj_sparse)\n",
    "        # pred_graph = pyg.data.Batch(x=noised_graphs.x, \n",
    "        #                             edge_index=pred_adj_sparse,\n",
    "        #                             edge_attr=noised_graphs.edge_attr,\n",
    "        #                             batch=noised_graphs.batch)\n",
    "\n",
    "        # return [pred_adj, pred_graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6814501682917277\n",
      "Epoch: 2, Loss: 0.5736742814381918\n",
      "Epoch: 3, Loss: 0.4547794759273529\n",
      "Epoch: 4, Loss: 0.3207857708136241\n",
      "Epoch: 5, Loss: 0.1708637277285258\n",
      "Epoch: 6, Loss: 0.13724727680285773\n",
      "Epoch: 7, Loss: 0.11728910605112712\n",
      "Epoch: 8, Loss: 0.09351835151513417\n",
      "Epoch: 9, Loss: 0.09692725042502086\n",
      "Epoch: 10, Loss: 0.14647934089104334\n",
      "Epoch: 11, Loss: 0.10092594722906749\n",
      "Epoch: 12, Loss: 0.09141373882691066\n",
      "Epoch: 13, Loss: 0.11526066809892654\n",
      "Epoch: 14, Loss: 0.09440551449855168\n",
      "Epoch: 15, Loss: 0.08638316889603932\n",
      "Epoch: 16, Loss: 0.08875712255636851\n",
      "Epoch: 17, Loss: 0.07957890878121059\n",
      "Epoch: 18, Loss: 0.07754483819007874\n",
      "Epoch: 19, Loss: 0.07667408386866252\n",
      "Epoch: 20, Loss: 0.07773071030775706\n",
      "Epoch: 21, Loss: 0.09795727580785751\n",
      "Epoch: 22, Loss: 0.07931863764921825\n",
      "Epoch: 23, Loss: 0.07991917183001836\n",
      "Epoch: 24, Loss: 0.08723363031943639\n",
      "Epoch: 25, Loss: 0.0734206885099411\n",
      "Epoch: 26, Loss: 0.07543578495581944\n",
      "Epoch: 27, Loss: 0.07695478200912476\n",
      "Epoch: 28, Loss: 0.0756968582669894\n",
      "Epoch: 29, Loss: 0.07698562989632289\n",
      "Epoch: 30, Loss: 0.0729766661922137\n",
      "Epoch: 31, Loss: 0.07534426202376683\n",
      "Epoch: 32, Loss: 0.07241629560788472\n",
      "Epoch: 33, Loss: 0.07642552256584167\n",
      "Epoch: 34, Loss: 0.07738276322682698\n",
      "Epoch: 35, Loss: 0.07913545270760854\n",
      "Epoch: 36, Loss: 0.07237691432237625\n",
      "Epoch: 37, Loss: 0.07229168216387431\n",
      "Epoch: 38, Loss: 0.07358790934085846\n",
      "Epoch: 39, Loss: 0.07245963315169017\n",
      "Epoch: 40, Loss: 0.07401404281457265\n",
      "Epoch: 41, Loss: 0.07292583336432774\n",
      "Epoch: 42, Loss: 0.07356188694636027\n",
      "Epoch: 43, Loss: 0.07280875494082768\n",
      "Epoch: 44, Loss: 0.07185168812672298\n",
      "Epoch: 45, Loss: 0.07236756136020024\n",
      "Epoch: 46, Loss: 0.07105868558088939\n",
      "Epoch: 47, Loss: 0.07227483143409093\n",
      "Epoch: 48, Loss: 0.07167984296878178\n",
      "Epoch: 49, Loss: 0.07243432352940242\n",
      "Epoch: 50, Loss: 0.07374373823404312\n",
      "Epoch: 51, Loss: 0.0728550876180331\n",
      "Epoch: 52, Loss: 0.07328435033559799\n",
      "Epoch: 53, Loss: 0.0733398050069809\n",
      "Epoch: 54, Loss: 0.07335237910350163\n",
      "Epoch: 55, Loss: 0.07146959503491719\n",
      "Epoch: 56, Loss: 0.07249737282594045\n",
      "Epoch: 57, Loss: 0.07229685038328171\n",
      "Epoch: 58, Loss: 0.0712014411886533\n",
      "Epoch: 59, Loss: 0.0727099950114886\n",
      "Epoch: 60, Loss: 0.07181406517823537\n",
      "Epoch: 61, Loss: 0.07136548807223637\n",
      "Epoch: 62, Loss: 0.07215085625648499\n",
      "Epoch: 63, Loss: 0.07153252015511195\n",
      "Epoch: 64, Loss: 0.06965197622776031\n",
      "Epoch: 65, Loss: 0.07197615752617519\n",
      "Epoch: 66, Loss: 0.07139051457246144\n",
      "Epoch: 67, Loss: 0.07143431653579076\n",
      "Epoch: 68, Loss: 0.0723697120944659\n",
      "Epoch: 69, Loss: 0.07167892654736836\n",
      "Epoch: 70, Loss: 0.07478177547454834\n",
      "Epoch: 71, Loss: 0.07233011225859325\n",
      "Epoch: 72, Loss: 0.07138793170452118\n",
      "Epoch: 73, Loss: 0.0735930601755778\n",
      "Epoch: 74, Loss: 0.0731532871723175\n",
      "Epoch: 75, Loss: 0.07245220988988876\n",
      "Epoch: 76, Loss: 0.07055198401212692\n",
      "Epoch: 77, Loss: 0.07405852278073628\n",
      "Epoch: 78, Loss: 0.07095106194416682\n",
      "Epoch: 79, Loss: 0.07114708423614502\n",
      "Epoch: 80, Loss: 0.07209778825441997\n",
      "Epoch: 81, Loss: 0.07097005347410838\n",
      "Epoch: 82, Loss: 0.07170455654462178\n",
      "Epoch: 83, Loss: 0.07294322053591411\n",
      "Epoch: 84, Loss: 0.07203380763530731\n",
      "Epoch: 85, Loss: 0.07142213980356853\n",
      "Epoch: 86, Loss: 0.07294473797082901\n",
      "Epoch: 87, Loss: 0.07304663707812627\n",
      "Epoch: 88, Loss: 0.07198470085859299\n",
      "Epoch: 89, Loss: 0.07244538515806198\n",
      "Epoch: 90, Loss: 0.07043532282114029\n",
      "Epoch: 91, Loss: 0.07440967112779617\n",
      "Epoch: 92, Loss: 0.07073189069827397\n",
      "Epoch: 93, Loss: 0.07287766287724178\n",
      "Epoch: 94, Loss: 0.07096441090106964\n",
      "Epoch: 95, Loss: 0.07233516623576482\n",
      "Epoch: 96, Loss: 0.07241647442181905\n",
      "Epoch: 97, Loss: 0.07195903609196345\n",
      "Epoch: 98, Loss: 0.07278900096813838\n",
      "Epoch: 99, Loss: 0.07105328887701035\n",
      "Epoch: 100, Loss: 0.07270454863707225\n",
      "Epoch: 101, Loss: 0.07182793070872624\n",
      "Epoch: 102, Loss: 0.07094841698805492\n",
      "Epoch: 103, Loss: 0.07165713359912236\n",
      "Epoch: 104, Loss: 0.0710790827870369\n",
      "Epoch: 105, Loss: 0.0708811953663826\n",
      "Epoch: 106, Loss: 0.07149809102217357\n",
      "Epoch: 107, Loss: 0.07137079785267512\n",
      "Epoch: 108, Loss: 0.07166628539562225\n",
      "Epoch: 109, Loss: 0.07300987094640732\n",
      "Epoch: 110, Loss: 0.07097336153189342\n",
      "Epoch: 111, Loss: 0.07087908933560054\n",
      "Epoch: 112, Loss: 0.0735155592362086\n",
      "Epoch: 113, Loss: 0.07208672414223354\n",
      "Epoch: 114, Loss: 0.07049987465143204\n",
      "Epoch: 115, Loss: 0.07190702607234319\n",
      "Epoch: 116, Loss: 0.07099550714095433\n",
      "Epoch: 117, Loss: 0.07264531900485356\n",
      "Epoch: 118, Loss: 0.07121786723534267\n",
      "Epoch: 119, Loss: 0.07042273630698521\n",
      "Epoch: 120, Loss: 0.0734096144636472\n",
      "Epoch: 121, Loss: 0.07025731851657231\n",
      "Epoch: 122, Loss: 0.06997514516115189\n",
      "Epoch: 123, Loss: 0.06910749276479085\n",
      "Epoch: 124, Loss: 0.070357712606589\n",
      "Epoch: 125, Loss: 0.07077979296445847\n",
      "Epoch: 126, Loss: 0.06913067648808162\n",
      "Epoch: 127, Loss: 0.07009914517402649\n",
      "Epoch: 128, Loss: 0.07117922852436702\n",
      "Epoch: 129, Loss: 0.06994201739629109\n",
      "Epoch: 130, Loss: 0.07115936279296875\n",
      "Epoch: 131, Loss: 0.07027494659026463\n",
      "Epoch: 132, Loss: 0.0711660807331403\n",
      "Epoch: 133, Loss: 0.07101788371801376\n",
      "Epoch: 134, Loss: 0.07282036046187083\n",
      "Epoch: 135, Loss: 0.07208688805500667\n",
      "Epoch: 136, Loss: 0.07241729150215785\n",
      "Epoch: 137, Loss: 0.07120668888092041\n",
      "Epoch: 138, Loss: 0.07104944189389546\n",
      "Epoch: 139, Loss: 0.07231361667315166\n",
      "Epoch: 140, Loss: 0.07108508547147115\n",
      "Epoch: 141, Loss: 0.07050368686517079\n",
      "Epoch: 142, Loss: 0.06902974843978882\n",
      "Epoch: 143, Loss: 0.07103494058052699\n",
      "Epoch: 144, Loss: 0.07155086348454158\n",
      "Epoch: 145, Loss: 0.0704437072078387\n",
      "Epoch: 146, Loss: 0.07187819729248683\n",
      "Epoch: 147, Loss: 0.07045851896206538\n",
      "Epoch: 148, Loss: 0.07270931700865428\n",
      "Epoch: 149, Loss: 0.07160560041666031\n",
      "Epoch: 150, Loss: 0.06880118449529012\n",
      "Epoch: 151, Loss: 0.0702273001273473\n",
      "Epoch: 152, Loss: 0.07195611546436946\n",
      "Epoch: 153, Loss: 0.07285109907388687\n",
      "Epoch: 154, Loss: 0.06976828475793202\n",
      "Epoch: 155, Loss: 0.0719874973098437\n",
      "Epoch: 156, Loss: 0.07268965989351273\n",
      "Epoch: 157, Loss: 0.07113862037658691\n",
      "Epoch: 158, Loss: 0.07227167238791783\n",
      "Epoch: 159, Loss: 0.07062390198310216\n",
      "Epoch: 160, Loss: 0.0710089976588885\n",
      "Epoch: 161, Loss: 0.07417641828457515\n",
      "Epoch: 162, Loss: 0.06985377768675487\n",
      "Epoch: 163, Loss: 0.07107443859179814\n",
      "Epoch: 164, Loss: 0.07328445216019948\n",
      "Epoch: 165, Loss: 0.07016024490197499\n",
      "Epoch: 166, Loss: 0.0705925648411115\n",
      "Epoch: 167, Loss: 0.07029614100853603\n",
      "Epoch: 168, Loss: 0.0704701840877533\n",
      "Epoch: 169, Loss: 0.0699962576230367\n",
      "Epoch: 170, Loss: 0.07005389779806137\n",
      "Epoch: 171, Loss: 0.07102341453234355\n",
      "Epoch: 172, Loss: 0.07267797986666362\n",
      "Epoch: 173, Loss: 0.06991103788216908\n",
      "Epoch: 174, Loss: 0.07100291053454082\n",
      "Epoch: 175, Loss: 0.0713299388686816\n",
      "Epoch: 176, Loss: 0.07136411219835281\n",
      "Epoch: 177, Loss: 0.07066863030195236\n",
      "Epoch: 178, Loss: 0.07053740819295247\n",
      "Epoch: 179, Loss: 0.07048017531633377\n",
      "Epoch: 180, Loss: 0.06992000838120778\n",
      "Epoch: 181, Loss: 0.07146382083495458\n",
      "Epoch: 182, Loss: 0.0715154434243838\n",
      "Epoch: 183, Loss: 0.07221205532550812\n",
      "Epoch: 184, Loss: 0.0726470947265625\n",
      "Epoch: 185, Loss: 0.07109714547793071\n",
      "Epoch: 186, Loss: 0.0726216584444046\n",
      "Epoch: 187, Loss: 0.07074165592590968\n",
      "Epoch: 188, Loss: 0.07115964343150456\n",
      "Epoch: 189, Loss: 0.07011141379674275\n",
      "Epoch: 190, Loss: 0.0732430915037791\n",
      "Epoch: 191, Loss: 0.07226516803105672\n",
      "Epoch: 192, Loss: 0.07166011383136113\n",
      "Epoch: 193, Loss: 0.07015858093897502\n",
      "Epoch: 194, Loss: 0.07077713559071223\n",
      "Epoch: 195, Loss: 0.07094109803438187\n",
      "Epoch: 196, Loss: 0.0698945273955663\n",
      "Epoch: 197, Loss: 0.07075851907332738\n",
      "Epoch: 198, Loss: 0.07056623697280884\n",
      "Epoch: 199, Loss: 0.07167396942774455\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "            # adj_batch = pyg.utils.to_dense_adj(\n",
    "            #     graphs.edge_index, batch=graphs.batch, \n",
    "            #     max_num_nodes=max_num_nodes\n",
    "            # )\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise(noised_graphs, t)\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss += loss_dist.item()\n",
    "        loss_dist.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss_dist: 0.691668967405955  Loss_CF: 1.8383880058924358\n",
      "Epoch: 2  Loss_dist: 0.6552395820617676  Loss_CF: 1.6528247992197673\n",
      "Epoch: 3  Loss_dist: 0.606701930363973  Loss_CF: 1.6865163246790569\n",
      "Epoch: 4  Loss_dist: 0.5793521602948507  Loss_CF: 1.6632185379664104\n",
      "Epoch: 5  Loss_dist: 0.4610901474952698  Loss_CF: 1.6824331283569336\n",
      "Epoch: 6  Loss_dist: 0.416742483774821  Loss_CF: 1.4854202667872112\n",
      "Epoch: 7  Loss_dist: 0.3584097425142924  Loss_CF: 1.390825907389323\n",
      "Epoch: 8  Loss_dist: 0.34042950471242267  Loss_CF: 1.1268500089645386\n",
      "Epoch: 9  Loss_dist: 0.3319298028945923  Loss_CF: 0.8661627570788065\n",
      "Epoch: 10  Loss_dist: 0.3777369161446889  Loss_CF: 0.6504358053207397\n",
      "Epoch: 11  Loss_dist: 0.5413017968336741  Loss_CF: 0.5196541945139567\n",
      "Epoch: 12  Loss_dist: 0.3893239398797353  Loss_CF: 0.5397603511810303\n",
      "Epoch: 13  Loss_dist: 0.3767375349998474  Loss_CF: 0.5104561944802603\n",
      "Epoch: 14  Loss_dist: 0.2589414417743683  Loss_CF: 0.7227440277735392\n",
      "Epoch: 15  Loss_dist: 0.24907932678858438  Loss_CF: 0.6605048576990763\n",
      "Epoch: 16  Loss_dist: 0.2281227856874466  Loss_CF: 0.5741019050280253\n",
      "Epoch: 17  Loss_dist: 0.27674293021361035  Loss_CF: 0.5679811239242554\n",
      "Epoch: 18  Loss_dist: 0.23957612613836923  Loss_CF: 0.5488430062929789\n",
      "Epoch: 19  Loss_dist: 0.20848442614078522  Loss_CF: 0.8247238000233968\n",
      "Epoch: 20  Loss_dist: 0.22709999978542328  Loss_CF: 0.5361159245173136\n",
      "Epoch: 21  Loss_dist: 0.2537950277328491  Loss_CF: 0.5083169738451639\n",
      "Epoch: 22  Loss_dist: 0.2820804715156555  Loss_CF: 0.5302612682183584\n",
      "Epoch: 23  Loss_dist: 0.24040713409582773  Loss_CF: 0.5259271363417307\n",
      "Epoch: 24  Loss_dist: 0.23829796413580576  Loss_CF: 0.5521542032559713\n",
      "Epoch: 25  Loss_dist: 0.21922777593135834  Loss_CF: 0.5528707802295685\n",
      "Epoch: 26  Loss_dist: 0.2625109056631724  Loss_CF: 0.5125043590863546\n",
      "Epoch: 27  Loss_dist: 0.19453573723634085  Loss_CF: 0.5502277612686157\n",
      "Epoch: 28  Loss_dist: 0.20405374467372894  Loss_CF: 0.5361810127894083\n",
      "Epoch: 29  Loss_dist: 0.2397615760564804  Loss_CF: 0.5389369527498881\n",
      "Epoch: 30  Loss_dist: 0.20183060566584268  Loss_CF: 0.522060384353002\n",
      "Epoch: 31  Loss_dist: 0.18868567049503326  Loss_CF: 0.5761382381121317\n",
      "Epoch: 32  Loss_dist: 0.19072120388348898  Loss_CF: 0.4897608160972595\n",
      "Epoch: 33  Loss_dist: 0.1991572082042694  Loss_CF: 0.5849321683247884\n",
      "Epoch: 34  Loss_dist: 0.21791844566663107  Loss_CF: 0.48341989517211914\n",
      "Epoch: 35  Loss_dist: 0.19112377862135568  Loss_CF: 0.5313853224118551\n",
      "Epoch: 36  Loss_dist: 0.20440051952997842  Loss_CF: 0.5457590222358704\n",
      "Epoch: 37  Loss_dist: 0.21029389401276907  Loss_CF: 0.4872518579165141\n",
      "Epoch: 38  Loss_dist: 0.19607357184092203  Loss_CF: 0.4811143676439921\n",
      "Epoch: 39  Loss_dist: 0.20275803407033285  Loss_CF: 0.5295992692311605\n",
      "Epoch: 40  Loss_dist: 0.21378255387147269  Loss_CF: 0.48284603158632916\n",
      "Epoch: 41  Loss_dist: 0.20699367920557657  Loss_CF: 0.5432889262835184\n",
      "Epoch: 42  Loss_dist: 0.19559251765410104  Loss_CF: 0.5587052901585897\n",
      "Epoch: 43  Loss_dist: 0.17053441206614176  Loss_CF: 0.5648360351721445\n",
      "Epoch: 44  Loss_dist: 0.18351678550243378  Loss_CF: 0.4909101227919261\n",
      "Epoch: 45  Loss_dist: 0.18242979049682617  Loss_CF: 0.5408856074015299\n",
      "Epoch: 46  Loss_dist: 0.1992299904425939  Loss_CF: 0.48224058747291565\n",
      "Epoch: 47  Loss_dist: 0.18778802951176962  Loss_CF: 0.5058782895406088\n",
      "Epoch: 48  Loss_dist: 0.18237322072188059  Loss_CF: 0.5122257173061371\n",
      "Epoch: 49  Loss_dist: 0.1853536069393158  Loss_CF: 0.5341958502928416\n",
      "Epoch: 50  Loss_dist: 0.18454579015572867  Loss_CF: 0.5319637954235077\n",
      "Epoch: 51  Loss_dist: 0.19749563932418823  Loss_CF: 0.5514031052589417\n",
      "Epoch: 52  Loss_dist: 0.18975931902726492  Loss_CF: 0.49472668766975403\n",
      "Epoch: 53  Loss_dist: 0.20556539297103882  Loss_CF: 0.492871214946111\n",
      "Epoch: 54  Loss_dist: 0.16606630881627402  Loss_CF: 0.5223288337389628\n",
      "Epoch: 55  Loss_dist: 0.16183465719223022  Loss_CF: 0.5037263234456381\n",
      "Epoch: 56  Loss_dist: 0.16682637731234232  Loss_CF: 0.5525609850883484\n",
      "Epoch: 57  Loss_dist: 0.17392348746458688  Loss_CF: 0.5383956730365753\n",
      "Epoch: 58  Loss_dist: 0.19239491720994314  Loss_CF: 0.5070816079775492\n",
      "Epoch: 59  Loss_dist: 0.20453743636608124  Loss_CF: 0.49941591421763104\n",
      "Epoch: 60  Loss_dist: 0.1672811508178711  Loss_CF: 0.5758821964263916\n",
      "Epoch: 61  Loss_dist: 0.20695138474305472  Loss_CF: 0.4822097321351369\n",
      "Epoch: 62  Loss_dist: 0.20356169839700064  Loss_CF: 0.5169976055622101\n",
      "Epoch: 63  Loss_dist: 0.19719634453455606  Loss_CF: 0.4917897582054138\n",
      "Epoch: 64  Loss_dist: 0.18380639453728995  Loss_CF: 0.45808252692222595\n",
      "Epoch: 65  Loss_dist: 0.16712936758995056  Loss_CF: 0.4872230291366577\n",
      "Epoch: 66  Loss_dist: 0.16353952387968698  Loss_CF: 0.5619407494862875\n",
      "Epoch: 67  Loss_dist: 0.17516155540943146  Loss_CF: 0.4801473915576935\n",
      "Epoch: 68  Loss_dist: 0.18831550081570944  Loss_CF: 0.46247198184331256\n",
      "Epoch: 69  Loss_dist: 0.1768660694360733  Loss_CF: 0.5326153139273325\n",
      "Epoch: 70  Loss_dist: 0.18572309613227844  Loss_CF: 0.5108193357785543\n",
      "Epoch: 71  Loss_dist: 0.21785191198190054  Loss_CF: 0.5191251436869303\n",
      "Epoch: 72  Loss_dist: 0.19312617182731628  Loss_CF: 0.468223512172699\n",
      "Epoch: 73  Loss_dist: 0.15881446500619253  Loss_CF: 0.5547540386517843\n",
      "Epoch: 74  Loss_dist: 0.16608770191669464  Loss_CF: 0.5805083513259888\n",
      "Epoch: 75  Loss_dist: 0.1926769216855367  Loss_CF: 0.4961072504520416\n",
      "Epoch: 76  Loss_dist: 0.1805914839108785  Loss_CF: 0.5193970104058584\n",
      "Epoch: 77  Loss_dist: 0.190858523050944  Loss_CF: 0.466902236143748\n",
      "Epoch: 78  Loss_dist: 0.16259071230888367  Loss_CF: 0.5366778671741486\n",
      "Epoch: 79  Loss_dist: 0.17680579920609793  Loss_CF: 0.564451277256012\n",
      "Epoch: 80  Loss_dist: 0.1779132535060247  Loss_CF: 0.47103020548820496\n",
      "Epoch: 81  Loss_dist: 0.16299915810426077  Loss_CF: 0.6084161003430685\n",
      "Epoch: 82  Loss_dist: 0.2066208819548289  Loss_CF: 0.4487428367137909\n",
      "Epoch: 83  Loss_dist: 0.2224153975645701  Loss_CF: 0.49274584650993347\n",
      "Epoch: 84  Loss_dist: 0.24351833760738373  Loss_CF: 0.5085615515708923\n",
      "Epoch: 85  Loss_dist: 0.18843484918276468  Loss_CF: 0.5205299556255341\n",
      "Epoch: 86  Loss_dist: 0.1650976985692978  Loss_CF: 0.5066985487937927\n",
      "Epoch: 87  Loss_dist: 0.16585240761439005  Loss_CF: 0.5118808348973592\n",
      "Epoch: 88  Loss_dist: 0.17114431162675223  Loss_CF: 0.4639468689759572\n",
      "Epoch: 89  Loss_dist: 0.19765183826287588  Loss_CF: 0.4678741494814555\n",
      "Epoch: 90  Loss_dist: 0.17439943552017212  Loss_CF: 0.49143890539805096\n",
      "Epoch: 91  Loss_dist: 0.1635206788778305  Loss_CF: 0.5108393828074137\n",
      "Epoch: 92  Loss_dist: 0.17039563755194345  Loss_CF: 0.48664843042691547\n",
      "Epoch: 93  Loss_dist: 0.19043868283430734  Loss_CF: 0.5112567842006683\n",
      "Epoch: 94  Loss_dist: 0.17957550783952078  Loss_CF: 0.546381950378418\n",
      "Epoch: 95  Loss_dist: 0.1703104774157206  Loss_CF: 0.5141646961371104\n",
      "Epoch: 96  Loss_dist: 0.1571896622578303  Loss_CF: 0.5801229377587637\n",
      "Epoch: 97  Loss_dist: 0.2032226969798406  Loss_CF: 0.5058708985646566\n",
      "Epoch: 98  Loss_dist: 0.18413728972276053  Loss_CF: 0.5036040544509888\n",
      "Epoch: 99  Loss_dist: 0.18786261479059854  Loss_CF: 0.4814412593841553\n",
      "Epoch: 100  Loss_dist: 0.17073148985703787  Loss_CF: 0.494239737590154\n",
      "Epoch: 101  Loss_dist: 0.14487614730993906  Loss_CF: 0.5972156723340353\n",
      "Epoch: 102  Loss_dist: 0.18359514077504477  Loss_CF: 0.4637703796227773\n",
      "Epoch: 103  Loss_dist: 0.15674011905988058  Loss_CF: 0.5444018642107645\n",
      "Epoch: 104  Loss_dist: 0.18148991962273917  Loss_CF: 0.49168291687965393\n",
      "Epoch: 105  Loss_dist: 0.1929563581943512  Loss_CF: 0.5125383834044138\n",
      "Epoch: 106  Loss_dist: 0.16969437897205353  Loss_CF: 0.49999478459358215\n",
      "Epoch: 107  Loss_dist: 0.1711825728416443  Loss_CF: 0.45842476685841876\n",
      "Epoch: 108  Loss_dist: 0.17626088857650757  Loss_CF: 0.454770823319753\n",
      "Epoch: 109  Loss_dist: 0.173552135626475  Loss_CF: 0.49052878220876056\n",
      "Epoch: 110  Loss_dist: 0.16639328499635062  Loss_CF: 0.46912245949109393\n",
      "Epoch: 111  Loss_dist: 0.15643603603045145  Loss_CF: 0.5151213904221853\n",
      "Epoch: 112  Loss_dist: 0.175382932027181  Loss_CF: 0.4786785940329234\n",
      "Epoch: 113  Loss_dist: 0.17570160826047262  Loss_CF: 0.4744059145450592\n",
      "Epoch: 114  Loss_dist: 0.18217228849728903  Loss_CF: 0.46981480717658997\n",
      "Epoch: 115  Loss_dist: 0.18626717726389566  Loss_CF: 0.505326509475708\n",
      "Epoch: 116  Loss_dist: 0.1626736025015513  Loss_CF: 0.4606960912545522\n",
      "Epoch: 117  Loss_dist: 0.14882834752400717  Loss_CF: 0.5134316782156626\n",
      "Epoch: 118  Loss_dist: 0.15692936877409616  Loss_CF: 0.5648905436197916\n",
      "Epoch: 119  Loss_dist: 0.16053390006224313  Loss_CF: 0.4946298698584239\n",
      "Epoch: 120  Loss_dist: 0.21082397301991782  Loss_CF: 0.46959980328877765\n",
      "Epoch: 121  Loss_dist: 0.2006234178940455  Loss_CF: 0.4582219918568929\n",
      "Epoch: 122  Loss_dist: 0.16940829157829285  Loss_CF: 0.5207588275273641\n",
      "Epoch: 123  Loss_dist: 0.16764035324255624  Loss_CF: 0.47380560636520386\n",
      "Epoch: 124  Loss_dist: 0.1541762799024582  Loss_CF: 0.5389207601547241\n",
      "Epoch: 125  Loss_dist: 0.15326634546120962  Loss_CF: 0.48576032121976215\n",
      "Epoch: 126  Loss_dist: 0.17697663605213165  Loss_CF: 0.46922945976257324\n",
      "Epoch: 127  Loss_dist: 0.17931795120239258  Loss_CF: 0.4830724398295085\n",
      "Epoch: 128  Loss_dist: 0.16422892113526663  Loss_CF: 0.4593699077765147\n",
      "Epoch: 129  Loss_dist: 0.16892495254675546  Loss_CF: 0.4359378119309743\n",
      "Epoch: 130  Loss_dist: 0.17270206908384958  Loss_CF: 0.48398099342981976\n",
      "Epoch: 131  Loss_dist: 0.18327000240484873  Loss_CF: 0.4324382146199544\n",
      "Epoch: 132  Loss_dist: 0.16986182828744253  Loss_CF: 0.5112384458382925\n",
      "Epoch: 133  Loss_dist: 0.16181476910909018  Loss_CF: 0.4843019147713979\n",
      "Epoch: 134  Loss_dist: 0.16941459973653158  Loss_CF: 0.4819134672482808\n",
      "Epoch: 135  Loss_dist: 0.16090720891952515  Loss_CF: 0.5428560972213745\n",
      "Epoch: 136  Loss_dist: 0.1795884370803833  Loss_CF: 0.4865906635920207\n",
      "Epoch: 137  Loss_dist: 0.17756442228953043  Loss_CF: 0.4819195469220479\n",
      "Epoch: 138  Loss_dist: 0.17386041084925333  Loss_CF: 0.4334625005722046\n",
      "Epoch: 139  Loss_dist: 0.19834178686141968  Loss_CF: 0.45935800671577454\n",
      "Epoch: 140  Loss_dist: 0.18843892216682434  Loss_CF: 0.507047027349472\n",
      "Epoch: 141  Loss_dist: 0.1623950203259786  Loss_CF: 0.5017953217029572\n",
      "Epoch: 142  Loss_dist: 0.15795698265234628  Loss_CF: 0.49404385685920715\n",
      "Epoch: 143  Loss_dist: 0.1718160609404246  Loss_CF: 0.44475098450978595\n",
      "Epoch: 144  Loss_dist: 0.17292572061220804  Loss_CF: 0.4493504265944163\n",
      "Epoch: 145  Loss_dist: 0.22476224601268768  Loss_CF: 0.42381266752878827\n",
      "Epoch: 146  Loss_dist: 0.20648842056592306  Loss_CF: 0.4105599522590637\n",
      "Epoch: 147  Loss_dist: 0.16756088534990946  Loss_CF: 0.5001628398895264\n",
      "Epoch: 148  Loss_dist: 0.1707198272148768  Loss_CF: 0.4281928936640422\n",
      "Epoch: 149  Loss_dist: 0.16467519104480743  Loss_CF: 0.42550607522328693\n",
      "Epoch: 150  Loss_dist: 0.1678562412659327  Loss_CF: 0.468440184990565\n",
      "Epoch: 151  Loss_dist: 0.18971364696820578  Loss_CF: 0.3914821942647298\n",
      "Epoch: 152  Loss_dist: 0.2252605805794398  Loss_CF: 0.501800129810969\n",
      "Epoch: 153  Loss_dist: 0.1693571756283442  Loss_CF: 0.4294647475083669\n",
      "Epoch: 154  Loss_dist: 0.1683422327041626  Loss_CF: 0.42924662431081134\n",
      "Epoch: 155  Loss_dist: 0.1546673427025477  Loss_CF: 0.458920160929362\n",
      "Epoch: 156  Loss_dist: 0.1796257495880127  Loss_CF: 0.4791676700115204\n",
      "Epoch: 157  Loss_dist: 0.15096747875213623  Loss_CF: 0.5472382108370463\n",
      "Epoch: 158  Loss_dist: 0.16235373417536417  Loss_CF: 0.4596547981103261\n",
      "Epoch: 159  Loss_dist: 0.16948253413041434  Loss_CF: 0.41956321398417157\n",
      "Epoch: 160  Loss_dist: 0.19208043813705444  Loss_CF: 0.4414659837881724\n",
      "Epoch: 161  Loss_dist: 0.17774666845798492  Loss_CF: 0.46577433745066327\n",
      "Epoch: 162  Loss_dist: 0.17869301637013754  Loss_CF: 0.46032718817392987\n",
      "Epoch: 163  Loss_dist: 0.16761580606301626  Loss_CF: 0.45891161759694415\n",
      "Epoch: 164  Loss_dist: 0.16511348386605582  Loss_CF: 0.5056499938170115\n",
      "Epoch: 165  Loss_dist: 0.166940708955129  Loss_CF: 0.4645244777202606\n",
      "Epoch: 166  Loss_dist: 0.166240394115448  Loss_CF: 0.44734834631284076\n",
      "Epoch: 167  Loss_dist: 0.15944967170556387  Loss_CF: 0.43552584449450177\n",
      "Epoch: 168  Loss_dist: 0.18557501832644144  Loss_CF: 0.4424821635087331\n",
      "Epoch: 169  Loss_dist: 0.21494526664415994  Loss_CF: 0.3981618285179138\n",
      "Epoch: 170  Loss_dist: 0.1861565758784612  Loss_CF: 0.46671226620674133\n",
      "Epoch: 171  Loss_dist: 0.19932604332764944  Loss_CF: 0.418871541817983\n",
      "Epoch: 172  Loss_dist: 0.16664054989814758  Loss_CF: 0.4769654969374339\n",
      "Epoch: 173  Loss_dist: 0.16654066741466522  Loss_CF: 0.4212217132250468\n",
      "Epoch: 174  Loss_dist: 0.1938219964504242  Loss_CF: 0.4986613988876343\n",
      "Epoch: 175  Loss_dist: 0.1726869394381841  Loss_CF: 0.49169894059499103\n",
      "Epoch: 176  Loss_dist: 0.1745075782140096  Loss_CF: 0.5081010858217875\n",
      "Epoch: 177  Loss_dist: 0.17397192120552063  Loss_CF: 0.45845263202985126\n",
      "Epoch: 178  Loss_dist: 0.16643229126930237  Loss_CF: 0.40604422489802044\n",
      "Epoch: 179  Loss_dist: 0.16693301995595297  Loss_CF: 0.43191073338190716\n",
      "Epoch: 180  Loss_dist: 0.19425358871618906  Loss_CF: 0.4293443560600281\n",
      "Epoch: 181  Loss_dist: 0.15870106716950735  Loss_CF: 0.46269046266873676\n",
      "Epoch: 182  Loss_dist: 0.19190221528212228  Loss_CF: 0.45412983496983844\n",
      "Epoch: 183  Loss_dist: 0.16451636950174967  Loss_CF: 0.44700350364049274\n",
      "Epoch: 184  Loss_dist: 0.19097213943799338  Loss_CF: 0.43170202771822613\n",
      "Epoch: 185  Loss_dist: 0.17067662874857584  Loss_CF: 0.43059807022412616\n",
      "Epoch: 186  Loss_dist: 0.17519610126813254  Loss_CF: 0.43513331810633343\n",
      "Epoch: 187  Loss_dist: 0.16931529343128204  Loss_CF: 0.4427231053511302\n",
      "Epoch: 188  Loss_dist: 0.16531809171040854  Loss_CF: 0.41329145431518555\n",
      "Epoch: 189  Loss_dist: 0.16620020071665445  Loss_CF: 0.45672011375427246\n",
      "Epoch: 190  Loss_dist: 0.18298386534055075  Loss_CF: 0.3930990795294444\n",
      "Epoch: 191  Loss_dist: 0.2040048986673355  Loss_CF: 0.38860709468523663\n",
      "Epoch: 192  Loss_dist: 0.18294663727283478  Loss_CF: 0.41527100404103595\n",
      "Epoch: 193  Loss_dist: 0.16811662912368774  Loss_CF: 0.3977452019850413\n",
      "Epoch: 194  Loss_dist: 0.17401544253031412  Loss_CF: 0.43069636821746826\n",
      "Epoch: 195  Loss_dist: 0.17951378722985586  Loss_CF: 0.46514861782391864\n",
      "Epoch: 196  Loss_dist: 0.18134909868240356  Loss_CF: 0.4304817318916321\n",
      "Epoch: 197  Loss_dist: 0.18655393024285635  Loss_CF: 0.41692570845286053\n",
      "Epoch: 198  Loss_dist: 0.1839060684045156  Loss_CF: 0.4218742648760478\n",
      "Epoch: 199  Loss_dist: 0.1843077838420868  Loss_CF: 0.43153398235638935\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise_CF = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise_CF.parameters(), lr=0.01)\n",
    "CF_weight = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss_dist = 0.0\n",
    "    running_loss_CF = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "            # adj_batch = pyg.utils.to_dense_adj(\n",
    "            #     graphs.edge_index, batch=graphs.batch, \n",
    "            #     max_num_nodes=max_num_nodes\n",
    "            # )\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise_CF(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss_dist += loss_dist.item()\n",
    "\n",
    "        explainee.eval()\n",
    "        explainee_pred = F.softmax(explainee(pred_graph), dim=-1)\n",
    "        class_prob = explainee_pred[torch.arange(explainee_pred.shape[0]), \n",
    "                                    graphs.y]\n",
    "        loss_CF = (-1 * torch.log(1 - class_prob)).mean()\n",
    "        running_loss_CF += loss_CF.item()\n",
    "\n",
    "        loss = 1.0 * loss_dist + CF_weight * loss_CF\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\", \n",
    "        f\" Loss_dist: {running_loss_dist / len(train_loader)}\", \n",
    "        f\" Loss_CF: {running_loss_CF / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m noise_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39mlow_noise, high\u001b[38;5;241m=\u001b[39mhigh_noise, size\u001b[38;5;241m=\u001b[39mT))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Bernoulli distribution for the probability of an edge existing.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m bernoulli_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull_like(\u001b[43mx\u001b[49m[\u001b[38;5;241m1\u001b[39m], noise_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Symmetrically applies noise - treats edges as undirected.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m noise_upper \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(bernoulli_adj)\u001b[38;5;241m.\u001b[39mtriu(diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "low_noise = 0.0\n",
    "high_noise = 0.5\n",
    "noise_list = list(np.random.uniform(low=low_noise, high=high_noise, size=T))\n",
    "\n",
    "# Bernoulli distribution for the probability of an edge existing.\n",
    "bernoulli_adj = torch.full_like(x[1], noise_list[0])\n",
    "\n",
    "# Symmetrically applies noise - treats edges as undirected.\n",
    "noise_upper = torch.bernoulli(bernoulli_adj).triu(diagonal=1)\n",
    "noise_lower = noise_upper.transpose(-1, -2)\n",
    "train_adj = torch.abs(-x[1] + noise_upper + noise_lower)\n",
    "\n",
    "noisediff = noise_upper + noise_lower # record true noise. \n",
    "\n",
    "print((train_adj - x[1]).abs().sum())\n",
    "print(noisediff.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "max_obs_nodes = 0\n",
    "for graph in train_data_list + test_data_list:\n",
    "    if graph.x.shape[0] > max_obs_nodes:\n",
    "        max_obs_nodes = graph.x.shape[0]\n",
    "\n",
    "print(max_obs_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  1,  1,  1,  2,  2,  3,  3,  3,  4,  5,  6,  6,  6,  6,\n",
       "          7,  7,  8,  9,  9,  9,  9, 11],\n",
       "        [ 5,  6, 11, 12,  5, 10, 11,  1, 13,  0,  2,  9,  4, 10, 11, 17, 18, 21,\n",
       "          8, 12, 11,  7, 12, 18, 19,  3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_adj_sparse = torch.bernoulli(test_adj)\n",
    "test_adj_sparse, _ = pyg.utils.dense_to_sparse(test_adj_sparse)\n",
    "test_adj_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mRelaxedBernoulli(\n\u001b[0;32m      2\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m test \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrsample()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\SuperTest2\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "dist = torch.distributions.RelaxedBernoulli(\n",
    "    temperature=0.15, probs=0.5\n",
    ")\n",
    "test = dist.rsample()\n",
    "test.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57600, 1])\n",
      "torch.Size([57600, 1])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_42380\\3953089207.py:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(loss.grad)\n"
     ]
    }
   ],
   "source": [
    "model_denoise_CF = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "for graphs in train_loader:\n",
    "    with torch.no_grad():\n",
    "        t = torch.randint(low=1, high=T, size=(1,))\n",
    "        noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "    print(graphs.edge_attr.shape)\n",
    "    print(noised_graphs.edge_attr.shape)\n",
    "    pred_weight, pred_graph = model_denoise_CF(noised_graphs, t)\n",
    "\n",
    "    explainee.eval()\n",
    "    explainee_pred = F.softmax(explainee(pred_graph), dim=-1)\n",
    "    class_prob = explainee_pred[torch.arange(explainee_pred.shape[0]), graph.y]\n",
    "    loss_CF = -1 * torch.log(1 - class_prob)\n",
    "    loss = loss_CF.mean()\n",
    "    loss.backward()\n",
    "\n",
    "    print(loss.grad)\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
