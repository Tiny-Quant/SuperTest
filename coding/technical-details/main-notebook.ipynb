{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as pyg\n",
    "\n",
    "import pygmtools as pygm\n",
    "pygm.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Meta Data\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True \n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Variables. \n",
    "max_num_nodes = 30 # 28 in the full dataset.  \n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUTAG Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data. \n",
    "from torch_geometric.datasets import TUDataset\n",
    "data_raw = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "\n",
    "# Shuffle.\n",
    "data_raw = data_raw.shuffle()\n",
    "\n",
    "# Split.\n",
    "train_data = data_raw[:150]\n",
    "test_data = data_raw[150:]\n",
    "\n",
    "def preprocess_MUTAG(data: TUDataset, max_num_nodes) -> pyg.data.Data:\n",
    "    \n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Pad node features.\n",
    "    padded_x = torch.zeros((max_num_nodes, data.x.size(1)))\n",
    "    padded_x[:num_nodes] = data.x\n",
    "\n",
    "    # Relax edges to weights. \n",
    "    padded_adj = torch.zeros((max_num_nodes, max_num_nodes))\n",
    "    padded_adj[:num_nodes, :num_nodes] = (\n",
    "        pyg.utils.to_dense_adj(data.edge_index).squeeze(0)\n",
    "    )\n",
    "    edge_index, edge_weight = pygm.utils.dense_to_sparse(padded_adj + 1)\n",
    "    edge_index = edge_index.transpose(0, 1)\n",
    "    edge_weight = edge_weight.squeeze(0)\n",
    "\n",
    "    # Wrap in data object.\n",
    "    preprocessed_data = pyg.data.Data(x=padded_x, \n",
    "                                      edge_index=edge_index,\n",
    "                                      edge_attr=edge_weight - 1,\n",
    "                                      y=data.y)\n",
    "\n",
    "    return preprocessed_data \n",
    "\n",
    "# Create data lists.\n",
    "train_data_list = []\n",
    "train_data_list_0 = []\n",
    "train_data_list_1 = []\n",
    "test_data_list = []\n",
    "test_data_list_0 = []\n",
    "test_data_list_1 = []\n",
    "\n",
    "for graph in train_data:\n",
    "    train_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        train_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        train_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "for graph in test_data:\n",
    "    test_data_list.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    if graph.y.item() == 0: \n",
    "        test_data_list_0.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "    elif graph.y.item() == 1: \n",
    "        test_data_list_1.append(preprocess_MUTAG(graph, max_num_nodes))\n",
    "\n",
    "# Create data loaders.\n",
    "train_loader = pyg.loader.DataLoader(train_data_list, batch_size=batch_size, \n",
    "                                     shuffle=True)\n",
    "test_loader = pyg.loader.DataLoader(test_data_list, batch_size=batch_size, \n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainee GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNWeighted(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNWeighted, self).__init__()\n",
    "        self.conv1 = pyg.nn.GCNConv(7, hidden_channels) # 7 node features.\n",
    "        self.conv2 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = pyg.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, 2) # 2 classes.\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            data.x, data.edge_index, data.batch, data.edge_attr\n",
    "        )\n",
    "\n",
    "        # 1. Node embeddings.\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "\n",
    "        # 2. Pooling.\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Prediction.\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 2 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 3 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 4 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 5 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 6 Train Accuracy: 0.6533333333333333 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 7 Train Accuracy: 0.68 Test Accuracy: 0.7105263157894737\n",
      "Epoch: 8 Train Accuracy: 0.7066666666666667 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 9 Train Accuracy: 0.7133333333333334 Test Accuracy: 0.7631578947368421\n",
      "Epoch: 10 Train Accuracy: 0.7333333333333333 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 11 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 12 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 13 Train Accuracy: 0.78 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 14 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 15 Train Accuracy: 0.76 Test Accuracy: 0.7368421052631579\n",
      "Epoch: 16 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 17 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 18 Train Accuracy: 0.8 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 19 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 20 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 21 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 22 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 23 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 24 Train Accuracy: 0.84 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 25 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 26 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 27 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 28 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 29 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 30 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 31 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 32 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 33 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 34 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 35 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 36 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 37 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 38 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 39 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 40 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 41 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 42 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 43 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 44 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 45 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 46 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 47 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 48 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 49 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 50 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 51 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 52 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 53 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 54 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 55 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 56 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 57 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 58 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 59 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 60 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 61 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 62 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 63 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 64 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 65 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 66 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 67 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 68 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 69 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 70 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 71 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 72 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 73 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 74 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 75 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 76 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 77 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 78 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 79 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 80 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 81 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 82 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 83 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 84 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 85 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 86 Train Accuracy: 0.82 Test Accuracy: 0.8157894736842105\n",
      "Epoch: 87 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 88 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 89 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 90 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 91 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 92 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 93 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 94 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 95 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 96 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 97 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 98 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 99 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 100 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 101 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 102 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 103 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 104 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 105 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 106 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 107 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 108 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 109 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 110 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 111 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 112 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 113 Train Accuracy: 0.82 Test Accuracy: 0.868421052631579\n",
      "Epoch: 114 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 115 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 116 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 117 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 118 Train Accuracy: 0.8066666666666666 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 119 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 120 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 121 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 122 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 123 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 124 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 125 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 126 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 127 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 128 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 129 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 130 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 131 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 132 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 133 Train Accuracy: 0.84 Test Accuracy: 0.868421052631579\n",
      "Epoch: 134 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 135 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 136 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 137 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 138 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 139 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 140 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 141 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 142 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 143 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 144 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 145 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 146 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 147 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 148 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 149 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 150 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 151 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 152 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 153 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 154 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 155 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 156 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 157 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 158 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 159 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 160 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 161 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 162 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 163 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 164 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 165 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 166 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 167 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 168 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 169 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 170 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 171 Train Accuracy: 0.8266666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 172 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 173 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 174 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 175 Train Accuracy: 0.82 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 176 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 177 Train Accuracy: 0.82 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 178 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 179 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.8421052631578947\n",
      "Epoch: 180 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 181 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 182 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 183 Train Accuracy: 0.8133333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 184 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 185 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 186 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 187 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 188 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 189 Train Accuracy: 0.8466666666666667 Test Accuracy: 0.868421052631579\n",
      "Epoch: 190 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 191 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 192 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 193 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 194 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 195 Train Accuracy: 0.86 Test Accuracy: 0.8947368421052632\n",
      "Epoch: 196 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 197 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 198 Train Accuracy: 0.8333333333333334 Test Accuracy: 0.868421052631579\n",
      "Epoch: 199 Train Accuracy: 0.86 Test Accuracy: 0.868421052631579\n",
      "Epoch: 200 Train Accuracy: 0.8533333333333334 Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Training Explainee.\n",
    "explainee = GCNWeighted(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(explainee.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data_loader): \n",
    "    explainee.train()\n",
    "\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def explainee_accuracy(data_loader):\n",
    "    explainee.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for batch in data_loader: \n",
    "        out = explainee(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == batch.y).sum())\n",
    "\n",
    "    return correct / len(data_loader.dataset)\n",
    "\n",
    "for epoch in range(1, 201): \n",
    "    train(train_loader)\n",
    "    train_accuracy = explainee_accuracy(train_loader)\n",
    "    test_accuracy = explainee_accuracy(test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch} Train Accuracy: {train_accuracy} \" + \n",
    "          f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 46,  17],\n",
       "       [ 10, 115]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "full_batch = pyg.data.Batch.from_data_list(test_data_list + train_data_list)\n",
    "explainee.eval()\n",
    "preds = explainee(full_batch).argmax(dim=1).numpy()\n",
    "targets = full_batch.y.numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(targets, preds)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D4Explainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "T = 50\n",
    "time_embed_dim = 10\n",
    "num_node_feats = 7\n",
    "\n",
    "betas = torch.linspace(start=0.001, end=0.1, steps=T)\n",
    "beta_bars = []\n",
    "cum_prod = 1\n",
    "\n",
    "for beta in betas:\n",
    "    cum_prod *= (1 - 2*beta)\n",
    "    beta_bars.append(0.5 - 0.5 * cum_prod)\n",
    "\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(graphs: pyg.data.Batch, \n",
    "                             t: int) -> pyg.data.Batch:\n",
    "    \"\"\"\n",
    "    Input: Batch of observed graphs.\n",
    "    Output: Batch of noised graphs.\n",
    "    \"\"\"\n",
    "    edge_weight = graphs.edge_attr\n",
    "\n",
    "    transition_probs = torch.full_like(edge_weight, beta_bars[t])\n",
    "    transition_dist = torch.distributions.RelaxedBernoulli(\n",
    "        temperature=0.15, probs=transition_probs\n",
    "    )\n",
    "\n",
    "    noised_edge_weights = torch.abs(\n",
    "        edge_weight + transition_dist.rsample()\n",
    "    )\n",
    "    noised_graph = pyg.data.Batch(x=graphs.x, \n",
    "                                  edge_index=graphs.edge_index,\n",
    "                                  edge_attr=noised_edge_weights,  \n",
    "                                  y=graphs.y, batch=graphs.batch)\n",
    "\n",
    "    return noised_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model(nn.Module):\n",
    "    def __init__(self, T, time_embed_dim, \n",
    "                 num_node_feats, h1=10, h2=50, h3=30, p_dropout=0.5): \n",
    "        super(denoising_model, self).__init__()\n",
    "\n",
    "        self.time_embedder = nn.Embedding(num_embeddings=T, \n",
    "                                          embedding_dim=time_embed_dim) \n",
    "\n",
    "        self.conv1 = pyg.nn.GCNConv(num_node_feats, h1)\n",
    "        self.conv2 = pyg.nn.GCNConv(h1, h2)\n",
    "        self.conv3 = pyg.nn.GCNConv(h2, h3)\n",
    "\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lin = nn.Linear(h3, max_num_nodes**2) # predicting weights.\n",
    "\n",
    "    def forward(self, noised_graphs, t):\n",
    "        \n",
    "        x, edge_index, batch, edge_weight = (\n",
    "            noised_graphs.x, noised_graphs.edge_index, \n",
    "            noised_graphs.batch, noised_graphs.edge_weight\n",
    "        )\n",
    "\n",
    "        time_embedding = self.time_embedder(t)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        \n",
    "        x = x + time_embedding\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "\n",
    "        x = pyg.nn.global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=self.p_dropout)\n",
    "        x = self.lin(x)\n",
    "        x = F.sigmoid(x) \n",
    "\n",
    "        pred_weights = x.reshape(-1)\n",
    "        pred_graph = pyg.data.Batch(x=noised_graphs.x, \n",
    "                                    edge_index=noised_graphs.edge_index,\n",
    "                                    edge_attr=pred_weights,\n",
    "                                    batch=noised_graphs.batch)\n",
    "    \n",
    "        return [pred_weights, pred_graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6814501682917277\n",
      "Epoch: 2, Loss: 0.5736742814381918\n",
      "Epoch: 3, Loss: 0.4547794759273529\n",
      "Epoch: 4, Loss: 0.3207857708136241\n",
      "Epoch: 5, Loss: 0.1708637277285258\n",
      "Epoch: 6, Loss: 0.13724727680285773\n",
      "Epoch: 7, Loss: 0.11728910605112712\n",
      "Epoch: 8, Loss: 0.09351835151513417\n",
      "Epoch: 9, Loss: 0.09692725042502086\n",
      "Epoch: 10, Loss: 0.14647934089104334\n",
      "Epoch: 11, Loss: 0.10092594722906749\n",
      "Epoch: 12, Loss: 0.09141373882691066\n",
      "Epoch: 13, Loss: 0.11526066809892654\n",
      "Epoch: 14, Loss: 0.09440551449855168\n",
      "Epoch: 15, Loss: 0.08638316889603932\n",
      "Epoch: 16, Loss: 0.08875712255636851\n",
      "Epoch: 17, Loss: 0.07957890878121059\n",
      "Epoch: 18, Loss: 0.07754483819007874\n",
      "Epoch: 19, Loss: 0.07667408386866252\n",
      "Epoch: 20, Loss: 0.07773071030775706\n",
      "Epoch: 21, Loss: 0.09795727580785751\n",
      "Epoch: 22, Loss: 0.07931863764921825\n",
      "Epoch: 23, Loss: 0.07991917183001836\n",
      "Epoch: 24, Loss: 0.08723363031943639\n",
      "Epoch: 25, Loss: 0.0734206885099411\n",
      "Epoch: 26, Loss: 0.07543578495581944\n",
      "Epoch: 27, Loss: 0.07695478200912476\n",
      "Epoch: 28, Loss: 0.0756968582669894\n",
      "Epoch: 29, Loss: 0.07698562989632289\n",
      "Epoch: 30, Loss: 0.0729766661922137\n",
      "Epoch: 31, Loss: 0.07534426202376683\n",
      "Epoch: 32, Loss: 0.07241629560788472\n",
      "Epoch: 33, Loss: 0.07642552256584167\n",
      "Epoch: 34, Loss: 0.07738276322682698\n",
      "Epoch: 35, Loss: 0.07913545270760854\n",
      "Epoch: 36, Loss: 0.07237691432237625\n",
      "Epoch: 37, Loss: 0.07229168216387431\n",
      "Epoch: 38, Loss: 0.07358790934085846\n",
      "Epoch: 39, Loss: 0.07245963315169017\n",
      "Epoch: 40, Loss: 0.07401404281457265\n",
      "Epoch: 41, Loss: 0.07292583336432774\n",
      "Epoch: 42, Loss: 0.07356188694636027\n",
      "Epoch: 43, Loss: 0.07280875494082768\n",
      "Epoch: 44, Loss: 0.07185168812672298\n",
      "Epoch: 45, Loss: 0.07236756136020024\n",
      "Epoch: 46, Loss: 0.07105868558088939\n",
      "Epoch: 47, Loss: 0.07227483143409093\n",
      "Epoch: 48, Loss: 0.07167984296878178\n",
      "Epoch: 49, Loss: 0.07243432352940242\n",
      "Epoch: 50, Loss: 0.07374373823404312\n",
      "Epoch: 51, Loss: 0.0728550876180331\n",
      "Epoch: 52, Loss: 0.07328435033559799\n",
      "Epoch: 53, Loss: 0.0733398050069809\n",
      "Epoch: 54, Loss: 0.07335237910350163\n",
      "Epoch: 55, Loss: 0.07146959503491719\n",
      "Epoch: 56, Loss: 0.07249737282594045\n",
      "Epoch: 57, Loss: 0.07229685038328171\n",
      "Epoch: 58, Loss: 0.0712014411886533\n",
      "Epoch: 59, Loss: 0.0727099950114886\n",
      "Epoch: 60, Loss: 0.07181406517823537\n",
      "Epoch: 61, Loss: 0.07136548807223637\n",
      "Epoch: 62, Loss: 0.07215085625648499\n",
      "Epoch: 63, Loss: 0.07153252015511195\n",
      "Epoch: 64, Loss: 0.06965197622776031\n",
      "Epoch: 65, Loss: 0.07197615752617519\n",
      "Epoch: 66, Loss: 0.07139051457246144\n",
      "Epoch: 67, Loss: 0.07143431653579076\n",
      "Epoch: 68, Loss: 0.0723697120944659\n",
      "Epoch: 69, Loss: 0.07167892654736836\n",
      "Epoch: 70, Loss: 0.07478177547454834\n",
      "Epoch: 71, Loss: 0.07233011225859325\n",
      "Epoch: 72, Loss: 0.07138793170452118\n",
      "Epoch: 73, Loss: 0.0735930601755778\n",
      "Epoch: 74, Loss: 0.0731532871723175\n",
      "Epoch: 75, Loss: 0.07245220988988876\n",
      "Epoch: 76, Loss: 0.07055198401212692\n",
      "Epoch: 77, Loss: 0.07405852278073628\n",
      "Epoch: 78, Loss: 0.07095106194416682\n",
      "Epoch: 79, Loss: 0.07114708423614502\n",
      "Epoch: 80, Loss: 0.07209778825441997\n",
      "Epoch: 81, Loss: 0.07097005347410838\n",
      "Epoch: 82, Loss: 0.07170455654462178\n",
      "Epoch: 83, Loss: 0.07294322053591411\n",
      "Epoch: 84, Loss: 0.07203380763530731\n",
      "Epoch: 85, Loss: 0.07142213980356853\n",
      "Epoch: 86, Loss: 0.07294473797082901\n",
      "Epoch: 87, Loss: 0.07304663707812627\n",
      "Epoch: 88, Loss: 0.07198470085859299\n",
      "Epoch: 89, Loss: 0.07244538515806198\n",
      "Epoch: 90, Loss: 0.07043532282114029\n",
      "Epoch: 91, Loss: 0.07440967112779617\n",
      "Epoch: 92, Loss: 0.07073189069827397\n",
      "Epoch: 93, Loss: 0.07287766287724178\n",
      "Epoch: 94, Loss: 0.07096441090106964\n",
      "Epoch: 95, Loss: 0.07233516623576482\n",
      "Epoch: 96, Loss: 0.07241647442181905\n",
      "Epoch: 97, Loss: 0.07195903609196345\n",
      "Epoch: 98, Loss: 0.07278900096813838\n",
      "Epoch: 99, Loss: 0.07105328887701035\n",
      "Epoch: 100, Loss: 0.07270454863707225\n",
      "Epoch: 101, Loss: 0.07182793070872624\n",
      "Epoch: 102, Loss: 0.07094841698805492\n",
      "Epoch: 103, Loss: 0.07165713359912236\n",
      "Epoch: 104, Loss: 0.0710790827870369\n",
      "Epoch: 105, Loss: 0.0708811953663826\n",
      "Epoch: 106, Loss: 0.07149809102217357\n",
      "Epoch: 107, Loss: 0.07137079785267512\n",
      "Epoch: 108, Loss: 0.07166628539562225\n",
      "Epoch: 109, Loss: 0.07300987094640732\n",
      "Epoch: 110, Loss: 0.07097336153189342\n",
      "Epoch: 111, Loss: 0.07087908933560054\n",
      "Epoch: 112, Loss: 0.0735155592362086\n",
      "Epoch: 113, Loss: 0.07208672414223354\n",
      "Epoch: 114, Loss: 0.07049987465143204\n",
      "Epoch: 115, Loss: 0.07190702607234319\n",
      "Epoch: 116, Loss: 0.07099550714095433\n",
      "Epoch: 117, Loss: 0.07264531900485356\n",
      "Epoch: 118, Loss: 0.07121786723534267\n",
      "Epoch: 119, Loss: 0.07042273630698521\n",
      "Epoch: 120, Loss: 0.0734096144636472\n",
      "Epoch: 121, Loss: 0.07025731851657231\n",
      "Epoch: 122, Loss: 0.06997514516115189\n",
      "Epoch: 123, Loss: 0.06910749276479085\n",
      "Epoch: 124, Loss: 0.070357712606589\n",
      "Epoch: 125, Loss: 0.07077979296445847\n",
      "Epoch: 126, Loss: 0.06913067648808162\n",
      "Epoch: 127, Loss: 0.07009914517402649\n",
      "Epoch: 128, Loss: 0.07117922852436702\n",
      "Epoch: 129, Loss: 0.06994201739629109\n",
      "Epoch: 130, Loss: 0.07115936279296875\n",
      "Epoch: 131, Loss: 0.07027494659026463\n",
      "Epoch: 132, Loss: 0.0711660807331403\n",
      "Epoch: 133, Loss: 0.07101788371801376\n",
      "Epoch: 134, Loss: 0.07282036046187083\n",
      "Epoch: 135, Loss: 0.07208688805500667\n",
      "Epoch: 136, Loss: 0.07241729150215785\n",
      "Epoch: 137, Loss: 0.07120668888092041\n",
      "Epoch: 138, Loss: 0.07104944189389546\n",
      "Epoch: 139, Loss: 0.07231361667315166\n",
      "Epoch: 140, Loss: 0.07108508547147115\n",
      "Epoch: 141, Loss: 0.07050368686517079\n",
      "Epoch: 142, Loss: 0.06902974843978882\n",
      "Epoch: 143, Loss: 0.07103494058052699\n",
      "Epoch: 144, Loss: 0.07155086348454158\n",
      "Epoch: 145, Loss: 0.0704437072078387\n",
      "Epoch: 146, Loss: 0.07187819729248683\n",
      "Epoch: 147, Loss: 0.07045851896206538\n",
      "Epoch: 148, Loss: 0.07270931700865428\n",
      "Epoch: 149, Loss: 0.07160560041666031\n",
      "Epoch: 150, Loss: 0.06880118449529012\n",
      "Epoch: 151, Loss: 0.0702273001273473\n",
      "Epoch: 152, Loss: 0.07195611546436946\n",
      "Epoch: 153, Loss: 0.07285109907388687\n",
      "Epoch: 154, Loss: 0.06976828475793202\n",
      "Epoch: 155, Loss: 0.0719874973098437\n",
      "Epoch: 156, Loss: 0.07268965989351273\n",
      "Epoch: 157, Loss: 0.07113862037658691\n",
      "Epoch: 158, Loss: 0.07227167238791783\n",
      "Epoch: 159, Loss: 0.07062390198310216\n",
      "Epoch: 160, Loss: 0.0710089976588885\n",
      "Epoch: 161, Loss: 0.07417641828457515\n",
      "Epoch: 162, Loss: 0.06985377768675487\n",
      "Epoch: 163, Loss: 0.07107443859179814\n",
      "Epoch: 164, Loss: 0.07328445216019948\n",
      "Epoch: 165, Loss: 0.07016024490197499\n",
      "Epoch: 166, Loss: 0.0705925648411115\n",
      "Epoch: 167, Loss: 0.07029614100853603\n",
      "Epoch: 168, Loss: 0.0704701840877533\n",
      "Epoch: 169, Loss: 0.0699962576230367\n",
      "Epoch: 170, Loss: 0.07005389779806137\n",
      "Epoch: 171, Loss: 0.07102341453234355\n",
      "Epoch: 172, Loss: 0.07267797986666362\n",
      "Epoch: 173, Loss: 0.06991103788216908\n",
      "Epoch: 174, Loss: 0.07100291053454082\n",
      "Epoch: 175, Loss: 0.0713299388686816\n",
      "Epoch: 176, Loss: 0.07136411219835281\n",
      "Epoch: 177, Loss: 0.07066863030195236\n",
      "Epoch: 178, Loss: 0.07053740819295247\n",
      "Epoch: 179, Loss: 0.07048017531633377\n",
      "Epoch: 180, Loss: 0.06992000838120778\n",
      "Epoch: 181, Loss: 0.07146382083495458\n",
      "Epoch: 182, Loss: 0.0715154434243838\n",
      "Epoch: 183, Loss: 0.07221205532550812\n",
      "Epoch: 184, Loss: 0.0726470947265625\n",
      "Epoch: 185, Loss: 0.07109714547793071\n",
      "Epoch: 186, Loss: 0.0726216584444046\n",
      "Epoch: 187, Loss: 0.07074165592590968\n",
      "Epoch: 188, Loss: 0.07115964343150456\n",
      "Epoch: 189, Loss: 0.07011141379674275\n",
      "Epoch: 190, Loss: 0.0732430915037791\n",
      "Epoch: 191, Loss: 0.07226516803105672\n",
      "Epoch: 192, Loss: 0.07166011383136113\n",
      "Epoch: 193, Loss: 0.07015858093897502\n",
      "Epoch: 194, Loss: 0.07077713559071223\n",
      "Epoch: 195, Loss: 0.07094109803438187\n",
      "Epoch: 196, Loss: 0.0698945273955663\n",
      "Epoch: 197, Loss: 0.07075851907332738\n",
      "Epoch: 198, Loss: 0.07056623697280884\n",
      "Epoch: 199, Loss: 0.07167396942774455\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss += loss_dist.item()\n",
    "\n",
    "        loss_dist.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, Loss: {running_loss / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss_dist: 0.691668967405955  Loss_CF: 1.8383880058924358\n",
      "Epoch: 2  Loss_dist: 0.6552395820617676  Loss_CF: 1.652824838956197\n",
      "Epoch: 3  Loss_dist: 0.6067018906275431  Loss_CF: 1.686516245206197\n",
      "Epoch: 4  Loss_dist: 0.5793521602948507  Loss_CF: 1.6632184584935505\n",
      "Epoch: 5  Loss_dist: 0.4610901474952698  Loss_CF: 1.6824331680933635\n",
      "Epoch: 6  Loss_dist: 0.416742483774821  Loss_CF: 1.4854202270507812\n",
      "Epoch: 7  Loss_dist: 0.3584097425142924  Loss_CF: 1.390825907389323\n",
      "Epoch: 8  Loss_dist: 0.34042949477831524  Loss_CF: 1.1268500884373982\n",
      "Epoch: 9  Loss_dist: 0.3319297830263774  Loss_CF: 0.8661627372105917\n",
      "Epoch: 10  Loss_dist: 0.3777369161446889  Loss_CF: 0.6504358053207397\n",
      "Epoch: 11  Loss_dist: 0.5413017968336741  Loss_CF: 0.5196541746457418\n",
      "Epoch: 12  Loss_dist: 0.38932392994562787  Loss_CF: 0.5397603611151377\n",
      "Epoch: 13  Loss_dist: 0.3767375449339549  Loss_CF: 0.5104561944802603\n",
      "Epoch: 14  Loss_dist: 0.2589414417743683  Loss_CF: 0.7227440079053243\n",
      "Epoch: 15  Loss_dist: 0.24907933672269186  Loss_CF: 0.6605047980944315\n",
      "Epoch: 16  Loss_dist: 0.2281227856874466  Loss_CF: 0.5741018255551656\n",
      "Epoch: 17  Loss_dist: 0.27674293021361035  Loss_CF: 0.5679811537265778\n",
      "Epoch: 18  Loss_dist: 0.23957612117131552  Loss_CF: 0.548842986424764\n",
      "Epoch: 19  Loss_dist: 0.20848442117373148  Loss_CF: 0.8247237900892893\n",
      "Epoch: 20  Loss_dist: 0.22709998488426208  Loss_CF: 0.5361159245173136\n",
      "Epoch: 21  Loss_dist: 0.25379499793052673  Loss_CF: 0.5083169639110565\n",
      "Epoch: 22  Loss_dist: 0.2820804665486018  Loss_CF: 0.5302612781524658\n",
      "Epoch: 23  Loss_dist: 0.24040713409582773  Loss_CF: 0.5259271462758383\n",
      "Epoch: 24  Loss_dist: 0.2382979542016983  Loss_CF: 0.5521541833877563\n",
      "Epoch: 25  Loss_dist: 0.21922776599725088  Loss_CF: 0.5528707305590311\n",
      "Epoch: 26  Loss_dist: 0.2625109056631724  Loss_CF: 0.5125043988227844\n",
      "Epoch: 27  Loss_dist: 0.19453570743401846  Loss_CF: 0.5502278010050455\n",
      "Epoch: 28  Loss_dist: 0.2040537397066752  Loss_CF: 0.5361809730529785\n",
      "Epoch: 29  Loss_dist: 0.2397615760564804  Loss_CF: 0.5389369527498881\n",
      "Epoch: 30  Loss_dist: 0.20183061063289642  Loss_CF: 0.522060364484787\n",
      "Epoch: 31  Loss_dist: 0.18868567049503326  Loss_CF: 0.5761381983757019\n",
      "Epoch: 32  Loss_dist: 0.19072121381759644  Loss_CF: 0.4897607962290446\n",
      "Epoch: 33  Loss_dist: 0.19915720323721567  Loss_CF: 0.5849321385224661\n",
      "Epoch: 34  Loss_dist: 0.21791845063368478  Loss_CF: 0.48341986536979675\n",
      "Epoch: 35  Loss_dist: 0.19112378358840942  Loss_CF: 0.53138534228007\n",
      "Epoch: 36  Loss_dist: 0.20440050959587097  Loss_CF: 0.5457590421040853\n",
      "Epoch: 37  Loss_dist: 0.21029389401276907  Loss_CF: 0.4872518479824066\n",
      "Epoch: 38  Loss_dist: 0.19607356687386832  Loss_CF: 0.4811143676439921\n",
      "Epoch: 39  Loss_dist: 0.20275802413622537  Loss_CF: 0.5295993189016978\n",
      "Epoch: 40  Loss_dist: 0.21378253400325775  Loss_CF: 0.48284605145454407\n",
      "Epoch: 41  Loss_dist: 0.20699367920557657  Loss_CF: 0.5432889262835184\n",
      "Epoch: 42  Loss_dist: 0.19559251268704733  Loss_CF: 0.558705190817515\n",
      "Epoch: 43  Loss_dist: 0.17053442200024924  Loss_CF: 0.5648359954357147\n",
      "Epoch: 44  Loss_dist: 0.18351677060127258  Loss_CF: 0.4909101525942485\n",
      "Epoch: 45  Loss_dist: 0.18242978056271872  Loss_CF: 0.5408855775992075\n",
      "Epoch: 46  Loss_dist: 0.19922994077205658  Loss_CF: 0.48224061727523804\n",
      "Epoch: 47  Loss_dist: 0.1877880096435547  Loss_CF: 0.5058782994747162\n",
      "Epoch: 48  Loss_dist: 0.18237319588661194  Loss_CF: 0.5122256875038147\n",
      "Epoch: 49  Loss_dist: 0.1853535771369934  Loss_CF: 0.5341958900292715\n",
      "Epoch: 50  Loss_dist: 0.1845457802216212  Loss_CF: 0.5319637954235077\n",
      "Epoch: 51  Loss_dist: 0.19749566912651062  Loss_CF: 0.5514031052589417\n",
      "Epoch: 52  Loss_dist: 0.18975932896137238  Loss_CF: 0.4947266678015391\n",
      "Epoch: 53  Loss_dist: 0.20556539793809256  Loss_CF: 0.492871214946111\n",
      "Epoch: 54  Loss_dist: 0.16606630384922028  Loss_CF: 0.5223288436730703\n",
      "Epoch: 55  Loss_dist: 0.16183464229106903  Loss_CF: 0.5037263433138529\n",
      "Epoch: 56  Loss_dist: 0.16682655115922293  Loss_CF: 0.552561084429423\n",
      "Epoch: 57  Loss_dist: 0.17392480373382568  Loss_CF: 0.5383949279785156\n",
      "Epoch: 58  Loss_dist: 0.192397008339564  Loss_CF: 0.5070809721946716\n",
      "Epoch: 59  Loss_dist: 0.20453924437363943  Loss_CF: 0.4994174838066101\n",
      "Epoch: 60  Loss_dist: 0.16728233794371286  Loss_CF: 0.5758764743804932\n",
      "Epoch: 61  Loss_dist: 0.20695177714029947  Loss_CF: 0.4822065830230713\n",
      "Epoch: 62  Loss_dist: 0.20356959104537964  Loss_CF: 0.5169638892014822\n",
      "Epoch: 63  Loss_dist: 0.19723337392012277  Loss_CF: 0.49174275000890094\n",
      "Epoch: 64  Loss_dist: 0.18387348453203836  Loss_CF: 0.45801307757695514\n",
      "Epoch: 65  Loss_dist: 0.16719326873620352  Loss_CF: 0.4870904783407847\n",
      "Epoch: 66  Loss_dist: 0.16354912519454956  Loss_CF: 0.561781624952952\n",
      "Epoch: 67  Loss_dist: 0.17513424654801688  Loss_CF: 0.4802402853965759\n",
      "Epoch: 68  Loss_dist: 0.1882601579030355  Loss_CF: 0.4623684187730153\n",
      "Epoch: 69  Loss_dist: 0.1768075774113337  Loss_CF: 0.5327671070893606\n",
      "Epoch: 70  Loss_dist: 0.18574799597263336  Loss_CF: 0.5108742515246073\n",
      "Epoch: 71  Loss_dist: 0.21773918469746908  Loss_CF: 0.5198428730169932\n",
      "Epoch: 72  Loss_dist: 0.1932016760110855  Loss_CF: 0.4683784345785777\n",
      "Epoch: 73  Loss_dist: 0.15876679619153342  Loss_CF: 0.555874248345693\n",
      "Epoch: 74  Loss_dist: 0.16622982919216156  Loss_CF: 0.5809158484141032\n",
      "Epoch: 75  Loss_dist: 0.19291339814662933  Loss_CF: 0.49750638008117676\n",
      "Epoch: 76  Loss_dist: 0.1810662398735682  Loss_CF: 0.5179875195026398\n",
      "Epoch: 77  Loss_dist: 0.19097655018170676  Loss_CF: 0.4670921266078949\n",
      "Epoch: 78  Loss_dist: 0.16251356899738312  Loss_CF: 0.5408083498477936\n",
      "Epoch: 79  Loss_dist: 0.1768133987983068  Loss_CF: 0.5606258710225424\n",
      "Epoch: 80  Loss_dist: 0.17837482690811157  Loss_CF: 0.47279538710912067\n",
      "Epoch: 81  Loss_dist: 0.16309025386969248  Loss_CF: 0.6118084192276001\n",
      "Epoch: 82  Loss_dist: 0.206696887811025  Loss_CF: 0.4495243529478709\n",
      "Epoch: 83  Loss_dist: 0.22177429497241974  Loss_CF: 0.49073752760887146\n",
      "Epoch: 84  Loss_dist: 0.24232268333435059  Loss_CF: 0.5077143410841624\n",
      "Epoch: 85  Loss_dist: 0.1895927588144938  Loss_CF: 0.5202362239360809\n",
      "Epoch: 86  Loss_dist: 0.16537832220395407  Loss_CF: 0.5068913002808889\n",
      "Epoch: 87  Loss_dist: 0.16553199787934622  Loss_CF: 0.5089820921421051\n",
      "Epoch: 88  Loss_dist: 0.17074044048786163  Loss_CF: 0.4651555120944977\n",
      "Epoch: 89  Loss_dist: 0.19722465177377066  Loss_CF: 0.467930128177007\n",
      "Epoch: 90  Loss_dist: 0.17487234870592752  Loss_CF: 0.49242691198984784\n",
      "Epoch: 91  Loss_dist: 0.16375082731246948  Loss_CF: 0.5094761153062185\n",
      "Epoch: 92  Loss_dist: 0.1714832286039988  Loss_CF: 0.48465938369433087\n",
      "Epoch: 93  Loss_dist: 0.1911215086778005  Loss_CF: 0.5058495899041494\n",
      "Epoch: 94  Loss_dist: 0.17804628610610962  Loss_CF: 0.5478412906328837\n",
      "Epoch: 95  Loss_dist: 0.17093145350615183  Loss_CF: 0.5119277536869049\n",
      "Epoch: 96  Loss_dist: 0.15738237897555032  Loss_CF: 0.5774512390295664\n",
      "Epoch: 97  Loss_dist: 0.20388257503509521  Loss_CF: 0.5082236627737681\n",
      "Epoch: 98  Loss_dist: 0.18486262361208597  Loss_CF: 0.49971776207288104\n",
      "Epoch: 99  Loss_dist: 0.18884631991386414  Loss_CF: 0.4892342984676361\n",
      "Epoch: 100  Loss_dist: 0.17079566915829977  Loss_CF: 0.4930294652779897\n",
      "Epoch: 101  Loss_dist: 0.14517983297506967  Loss_CF: 0.6009345054626465\n",
      "Epoch: 102  Loss_dist: 0.1813317040602366  Loss_CF: 0.4608854651451111\n",
      "Epoch: 103  Loss_dist: 0.15657155215740204  Loss_CF: 0.5435713330904642\n",
      "Epoch: 104  Loss_dist: 0.18224301934242249  Loss_CF: 0.49505162239074707\n",
      "Epoch: 105  Loss_dist: 0.19479882717132568  Loss_CF: 0.5143077075481415\n",
      "Epoch: 106  Loss_dist: 0.16971992452939352  Loss_CF: 0.5029106338818868\n",
      "Epoch: 107  Loss_dist: 0.17157205442587534  Loss_CF: 0.4601193169752757\n",
      "Epoch: 108  Loss_dist: 0.1773345172405243  Loss_CF: 0.4577884574731191\n",
      "Epoch: 109  Loss_dist: 0.17284641166528067  Loss_CF: 0.4977050721645355\n",
      "Epoch: 110  Loss_dist: 0.16559849182764688  Loss_CF: 0.47178229689598083\n",
      "Epoch: 111  Loss_dist: 0.1563375691572825  Loss_CF: 0.5185156365235647\n",
      "Epoch: 112  Loss_dist: 0.17652375996112823  Loss_CF: 0.4734844962755839\n",
      "Epoch: 113  Loss_dist: 0.17794731259346008  Loss_CF: 0.47492830952008563\n",
      "Epoch: 114  Loss_dist: 0.1856639434893926  Loss_CF: 0.47246318062146503\n",
      "Epoch: 115  Loss_dist: 0.18941419323285422  Loss_CF: 0.5068859755992889\n",
      "Epoch: 116  Loss_dist: 0.16209685802459717  Loss_CF: 0.4659259021282196\n",
      "Epoch: 117  Loss_dist: 0.14824650684992471  Loss_CF: 0.5199509263038635\n",
      "Epoch: 118  Loss_dist: 0.15722863872845969  Loss_CF: 0.5726848940054575\n",
      "Epoch: 119  Loss_dist: 0.16174963613351187  Loss_CF: 0.49931039412816364\n",
      "Epoch: 120  Loss_dist: 0.21084006627400717  Loss_CF: 0.47331446409225464\n",
      "Epoch: 121  Loss_dist: 0.2025789866844813  Loss_CF: 0.45453829566637677\n",
      "Epoch: 122  Loss_dist: 0.1711424191792806  Loss_CF: 0.5180043876171112\n",
      "Epoch: 123  Loss_dist: 0.16966078182061514  Loss_CF: 0.4758006731669108\n",
      "Epoch: 124  Loss_dist: 0.15503362814585367  Loss_CF: 0.5385744373003641\n",
      "Epoch: 125  Loss_dist: 0.1535137544075648  Loss_CF: 0.48811206221580505\n",
      "Epoch: 126  Loss_dist: 0.1786932349205017  Loss_CF: 0.47451986869176227\n",
      "Epoch: 127  Loss_dist: 0.17980802059173584  Loss_CF: 0.4869299829006195\n",
      "Epoch: 128  Loss_dist: 0.16491040587425232  Loss_CF: 0.4658164183298747\n",
      "Epoch: 129  Loss_dist: 0.16998209555943808  Loss_CF: 0.43592868248621625\n",
      "Epoch: 130  Loss_dist: 0.17139176030953726  Loss_CF: 0.48461901148160297\n",
      "Epoch: 131  Loss_dist: 0.18190395335356394  Loss_CF: 0.43470930059750873\n",
      "Epoch: 132  Loss_dist: 0.1685196061929067  Loss_CF: 0.5033774177233378\n",
      "Epoch: 133  Loss_dist: 0.1611359715461731  Loss_CF: 0.48055004080136615\n",
      "Epoch: 134  Loss_dist: 0.17107683420181274  Loss_CF: 0.4765344262123108\n",
      "Epoch: 135  Loss_dist: 0.1617571860551834  Loss_CF: 0.5429151952266693\n",
      "Epoch: 136  Loss_dist: 0.1787626494963964  Loss_CF: 0.48403002818425495\n",
      "Epoch: 137  Loss_dist: 0.17229118446509042  Loss_CF: 0.4824862579504649\n",
      "Epoch: 138  Loss_dist: 0.17455178995927176  Loss_CF: 0.4311662515004476\n",
      "Epoch: 139  Loss_dist: 0.19285487135251364  Loss_CF: 0.4626053273677826\n",
      "Epoch: 140  Loss_dist: 0.18694406747817993  Loss_CF: 0.5107270876566569\n",
      "Epoch: 141  Loss_dist: 0.16817839940388998  Loss_CF: 0.4875151614348094\n",
      "Epoch: 142  Loss_dist: 0.15828152497609457  Loss_CF: 0.49562130371729535\n",
      "Epoch: 143  Loss_dist: 0.17092429598172507  Loss_CF: 0.4464135666688283\n",
      "Epoch: 144  Loss_dist: 0.1678033322095871  Loss_CF: 0.469304492076238\n",
      "Epoch: 145  Loss_dist: 0.22056850294272104  Loss_CF: 0.4192812442779541\n",
      "Epoch: 146  Loss_dist: 0.20441237588723501  Loss_CF: 0.4142057200272878\n",
      "Epoch: 147  Loss_dist: 0.16670587162176767  Loss_CF: 0.5028462509314219\n",
      "Epoch: 148  Loss_dist: 0.1708055188258489  Loss_CF: 0.4222143789132436\n",
      "Epoch: 149  Loss_dist: 0.16436653335889181  Loss_CF: 0.4193791151046753\n",
      "Epoch: 150  Loss_dist: 0.1652407298485438  Loss_CF: 0.4614757200082143\n",
      "Epoch: 151  Loss_dist: 0.1857659618059794  Loss_CF: 0.38987574974695843\n",
      "Epoch: 152  Loss_dist: 0.22363962729771933  Loss_CF: 0.5063500006993612\n",
      "Epoch: 153  Loss_dist: 0.16896756490071616  Loss_CF: 0.43431155880292255\n",
      "Epoch: 154  Loss_dist: 0.1687144637107849  Loss_CF: 0.4288733998934428\n",
      "Epoch: 155  Loss_dist: 0.15823906163374582  Loss_CF: 0.45102205872535706\n",
      "Epoch: 156  Loss_dist: 0.18582730988661447  Loss_CF: 0.480586975812912\n",
      "Epoch: 157  Loss_dist: 0.15291105210781097  Loss_CF: 0.5490051805973053\n",
      "Epoch: 158  Loss_dist: 0.16055565575758615  Loss_CF: 0.4605344434579213\n",
      "Epoch: 159  Loss_dist: 0.16634615262349448  Loss_CF: 0.43562079469362897\n",
      "Epoch: 160  Loss_dist: 0.18922351797421774  Loss_CF: 0.44547038276990253\n",
      "Epoch: 161  Loss_dist: 0.17834577957789102  Loss_CF: 0.46986935536066693\n",
      "Epoch: 162  Loss_dist: 0.17854409913221994  Loss_CF: 0.46017420291900635\n",
      "Epoch: 163  Loss_dist: 0.17333796123663583  Loss_CF: 0.44681235154469806\n",
      "Epoch: 164  Loss_dist: 0.17141132056713104  Loss_CF: 0.4889165163040161\n",
      "Epoch: 165  Loss_dist: 0.17299122114976248  Loss_CF: 0.4717187285423279\n",
      "Epoch: 166  Loss_dist: 0.16899440189202627  Loss_CF: 0.4421250919500987\n",
      "Epoch: 167  Loss_dist: 0.15295864144961038  Loss_CF: 0.47492032249768573\n",
      "Epoch: 168  Loss_dist: 0.17734342316786447  Loss_CF: 0.4637063841025035\n",
      "Epoch: 169  Loss_dist: 0.21772982676823935  Loss_CF: 0.4000734289487203\n",
      "Epoch: 170  Loss_dist: 0.18705100317796072  Loss_CF: 0.4667113920052846\n",
      "Epoch: 171  Loss_dist: 0.19832874834537506  Loss_CF: 0.421885480483373\n",
      "Epoch: 172  Loss_dist: 0.1660596082607905  Loss_CF: 0.47923046350479126\n",
      "Epoch: 173  Loss_dist: 0.1581701636314392  Loss_CF: 0.4381665885448456\n",
      "Epoch: 174  Loss_dist: 0.18813000619411469  Loss_CF: 0.5071229338645935\n",
      "Epoch: 175  Loss_dist: 0.17465324203173319  Loss_CF: 0.47939422726631165\n",
      "Epoch: 176  Loss_dist: 0.17643955846627554  Loss_CF: 0.5167658825715383\n",
      "Epoch: 177  Loss_dist: 0.1777067333459854  Loss_CF: 0.440777321656545\n",
      "Epoch: 178  Loss_dist: 0.1664509375890096  Loss_CF: 0.4070467750231425\n",
      "Epoch: 179  Loss_dist: 0.1634084234635035  Loss_CF: 0.4400030275185903\n",
      "Epoch: 180  Loss_dist: 0.1891850382089615  Loss_CF: 0.4178008238474528\n",
      "Epoch: 181  Loss_dist: 0.15914198259512582  Loss_CF: 0.4750644465287526\n",
      "Epoch: 182  Loss_dist: 0.20256934563318887  Loss_CF: 0.471286416053772\n",
      "Epoch: 183  Loss_dist: 0.16297297676404318  Loss_CF: 0.4691830774148305\n",
      "Epoch: 184  Loss_dist: 0.2047125349442164  Loss_CF: 0.4336521824200948\n",
      "Epoch: 185  Loss_dist: 0.1716389258702596  Loss_CF: 0.4523311456044515\n",
      "Epoch: 186  Loss_dist: 0.17489096025625864  Loss_CF: 0.43176430463790894\n",
      "Epoch: 187  Loss_dist: 0.16834203402201334  Loss_CF: 0.45085086425145465\n",
      "Epoch: 188  Loss_dist: 0.16399971147378287  Loss_CF: 0.4224192500114441\n",
      "Epoch: 189  Loss_dist: 0.16430174311002096  Loss_CF: 0.474685659011205\n",
      "Epoch: 190  Loss_dist: 0.18395311137040457  Loss_CF: 0.39871328075726825\n",
      "Epoch: 191  Loss_dist: 0.20841574172178903  Loss_CF: 0.40100301305452984\n",
      "Epoch: 192  Loss_dist: 0.18409874538580576  Loss_CF: 0.41437824567159015\n",
      "Epoch: 193  Loss_dist: 0.16675863166650137  Loss_CF: 0.3966139554977417\n",
      "Epoch: 194  Loss_dist: 0.17046369115511575  Loss_CF: 0.4182459811369578\n",
      "Epoch: 195  Loss_dist: 0.1748447467883428  Loss_CF: 0.4642406205336253\n",
      "Epoch: 196  Loss_dist: 0.17439958453178406  Loss_CF: 0.4429502288500468\n",
      "Epoch: 197  Loss_dist: 0.18617754677931467  Loss_CF: 0.43262021740277606\n",
      "Epoch: 198  Loss_dist: 0.18143564959367117  Loss_CF: 0.4326208432515462\n",
      "Epoch: 199  Loss_dist: 0.1759785215059916  Loss_CF: 0.43555675943692523\n"
     ]
    }
   ],
   "source": [
    "# Training Loop.\n",
    "model_denoise_CF = denoising_model(T, time_embed_dim=time_embed_dim, \n",
    "                                num_node_feats=num_node_feats) \n",
    "\n",
    "optimizer = torch.optim.Adam(model_denoise_CF.parameters(), lr=0.01)\n",
    "CF_weight = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs): \n",
    "    running_loss_dist = 0.0\n",
    "    running_loss_CF = 0.0\n",
    "    for graphs in train_loader:\n",
    "        with torch.no_grad():\n",
    "            t = torch.randint(low=1, high=T, size=(1,))\n",
    "            noised_graphs = forward_diffusion_sample(graphs, t)\n",
    "\n",
    "        pred_weight, pred_graph = model_denoise_CF(noised_graphs, t)\n",
    "\n",
    "        loss_dist = F.binary_cross_entropy(\n",
    "            pred_weight, graphs.edge_attr.squeeze(1)\n",
    "        ) \n",
    "        running_loss_dist += loss_dist.item()\n",
    "\n",
    "        explainee.eval()\n",
    "        explainee_pred = F.softmax(explainee(pred_graph), dim=-1)\n",
    "        class_prob = explainee_pred[torch.arange(explainee_pred.shape[0]), \n",
    "                                    graphs.y]\n",
    "        loss_CF = (-1 * torch.log(1 - class_prob)).mean()\n",
    "        running_loss_CF += loss_CF.item()\n",
    "\n",
    "        loss = 1.0 * loss_dist + CF_weight * loss_CF\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch: {epoch}\", \n",
    "        f\" Loss_dist: {running_loss_dist / len(train_loader)}\", \n",
    "        f\" Loss_CF: {running_loss_CF / len(train_loader)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.) tensor(28.6338, grad_fn=<SumBackward0>)\n",
      "tensor([[0.1354, 0.8646]], grad_fn=<SoftmaxBackward0>) tensor([[0.8495, 0.1505]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Inference \n",
    "non_mut_example = pyg.data.Batch.from_data_list([test_data_list_1[0]])\n",
    "non_mut_noised = forward_diffusion_sample(non_mut_example, torch.tensor([T-1])) \n",
    "pred_weight, non_mut_denoised = model_denoise_CF(non_mut_noised, torch.tensor([T-1]))\n",
    "print(test_data_list_1[0].edge_attr.sum(), non_mut_denoised.edge_attr.sum())\n",
    "print(torch.softmax(explainee(test_data_list_1[0]), dim=-1), \n",
    "      torch.softmax(explainee(non_mut_denoised), dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
