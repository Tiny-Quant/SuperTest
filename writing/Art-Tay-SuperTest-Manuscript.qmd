---
title: "A Review of Methods for Explainable and Interpretable Graph Neural Networks"
author: "Art Tay"
format:
  pdf:
     documentclass: article
     papersize: letter
     geometry:
         margin=1in
     fontsize: 11pt
     keep-tex: true
     include-in-header: header.tex
     link-citations: true
     number-sections: true
bibliography: references.bib
---

# Introduction 

\quad Graphs provide an incredibly flexible structure for modeling complex data. Data can naturally appear as graphs, like molecules. We can reduce data to a graph, such as the key points of a image. We can even use graphs to add structure, such as grammatical relationships. Graph Neural Networks (GNNs) have become a popular choice for prediction and inference on graph data. At their core, GNNs work by iteratively updating node embeddings based on information from neighboring nodes. The idea is to use the graph's structure to engineer better features. This message passing scheme allows GNNs to capture complex dependencies and patterns present within the graph structure. GNN architectures typically consist of multiple layers, each performing message passing and aggregation operations to refine the embeddings. These layers are often followed by pooling and dense prediction layers to produce the final output. Some important applications of graph classification include predicting chemical toxicity [@bai2019unsupervised], classifying proteins [@gallicchio2019fast], and even detecting cancer from pathology slides [@Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023]. While GNNs achieve remarkable predictive power, their complexity prevents the exaction of the scientific rationale. Like many deep learning models, the black box nature of GNNs prevents wide adoption. Without strong methods for understanding their predictions, GNNs are more susceptible to adversarial attacks and undetected discrimination. Inferential methods also serve to direct modeling efforts by highlighting common structures or features that may be predictive in other applications. These reasons underscore the critical importance of methods for explaining and interpreting GNN models.

\quad Although explainability and interpretability are sometimes used interchangeably in the literature, we will adopt the distinction expressed in @Yuan_Yu_Gui_Ji_2022. In this article the authors say that a model is explainable if the models predictions can be reasoned post-hoc [@Yuan_Yu_Gui_Ji_2022]. For example, the effect of a specific input variable can be estimated by either dropping or randomly permuting the variable and accessing the effect on the output [@Breiman_2001]. This paradigm could be directly applied to GNNs; however, researchers are often not interested in the statistical significance of tabular node and edge-level features. Most scientific questions focus on the importance of specific graph-level substructures. The challenge with GNN explanations is that, in addition to computing the importance of features, the high-level features themselves must first be identified. On the other hand, a model is interpretable if the model's decision process can be readily understood by humans [@Yuan_Yu_Gui_Ji_2022]. For example, a linear regression model is interpretable because each coefficient clearly defines the relationship between any input and output. An interpretable model is typically superior to an explainable one because similar to a partial F-test, explanation methods might identify that certain variables enhance the model, but they cannot explain how these variables contribute. On the flip side, interpretable models may not reach the performance levels of black-box models. GNN models are generally not interpretable because the relation expressed by the parameters tends to exceed human understanding. A direct translation from traditional statistics would be a circuit type analysis, which has work for image processing convolution neural networks [@olah2020zoom]. For graphs, this would involve using coefficients on subgraphs to produce predictions. Similar to the difficulties of explanation methods, these subgraphs would have to be identified, which can be computational complex and expensive given the combinatorial nature of graphs. Overcoming these challenges has substantial scientific impacts. In applications where GNNs demonstrate strong predictive power, it enables the formulation of testable scientific hypotheses about the nature of the classification. Conversely, in cases where GNNs exhibit weak predictive power, it helps identify and understand potential misunderstandings within the model.

## General Notation 

- Any graph $G$ can be describe by $X, A, E$. The node feature, matrix, edge feature matrix, and adjacency matrix respectively.  

- Let $X = [X_c, \ X_d]$, where $X_c$ is the subset of continuous node features and $X_d$ is the subset of one-hot discrete node features.  

- Let $E = [E_c, \ E_d]$, denoted in the same manner. 

- Let $n$ represent the number of nodes in the graph and $v$ represent the number of edges.

- For any graph, let $\nu$ denote the set of node and $\mathcal{E}$ denote the set of edges. 

- Let $\text{feat}_{(.)}$ denote the number of features or columns in the the corresponding feature matrix.  

- $A$ is a binary $n \times n$ matrix where $A[i, \ j] = 1$ indicates that an edge exists between nodes labeled $i$ and $j$.  

- Let $\text{explainee}(G; \ \Omega) = h^{(1)}_G, \dots h^{(L)}_G, \rho_G$ be an $L$ layer GNN model with parameters $\Omega$ that we would like to explain. 

- Let $\hat Y_G$ be the predicted class label for graph $G$ predicted from explainee().

- Unless otherwise noted, assume any GNN model discussed is a graph-level classification model. 

# Analysis of Core Papers 

## GNNInterpreter [@Wang_Shen_2024]

\quad GNNInterpreter [@Wang_Shen_2024] is a method for generating model-level explanations of GNN graph classification models. In general, explanation methods serve to elucidate which features within the data influence disparate predictions. These methods typically fall into two categories: instance-level and model-level. Instance-level explanations aim to unveil the model’s rationale behind a particular prediction. In domains such as image and text analysis, a prevalent approach involves masking or perturbing the instance and assessing the impact on the model’s prediction. On the other hand, model-level explanations seek to understand how a model generally distinguishes between classes. In image and text analysis, for instance, one common technique involves treating the input as a trainable parameter and optimizing the model’s prediction towards a specific class. Consequently, the resulting optimized input comprises a set of features strongly associated with the targeted class. GNNInterpreter provides model level explanations for GNN in this manner. Formally, GNNInterpreter tries to learn the graph generating distribution for each class. GNNInterpreter works by optimizing the parameters of a generic graph generating distribution to produce samples that closely match the explainee's understanding of the targeted class.

\quad Graph generating distributions are hard to specify because there can be discrete and continuous elements of $X$, $E$ and $A$. Furthermore, the interactions between these matrices can be complex. The authors tackle these issues by making two simplify assumptions. First, they assume that every graph is a *Gilbert random graph* [@Gilbert_1959], where every possible edge as an independent fixed probability of occurring. Second, the author assume that the features of every node and edge are independently distributed. The justification of the first assumption is that the other common types of random graphs are not suitable for this application. Erdo-Renyi random graphs [@erdds1959random] have a fixed number of edges,  which limits the diversity of explanations, Rado random graphs [@Rado1964UniversalGA] are infinite in size, and the random dot-product graph model is just a generalization of Gilbert's model. The second assumption is justified by the fact that the parameters of the independent distributions will be updated jointly using the *explainee* model. Therefore, the *explainee's* understanding of the latent correlation structure should be contained in the final estimates.   

\quad Although the graphs discuss in this paper only contain discrete features, $X_c$ and $E_c$ can be sampled from any continuous distribution that can be expressed as a location-scale family. Separating the stochastic and systematic components is necessary for gradient based optimization. It is commonly known as the *reparametrization trick*. The discrete feature matrices, ($X_d$, $E_d$, $A$), need to be sampled from a continuous distribution for gradient based optimization, but the distribution has to have sampling properties close to a discrete distribution. The author assume that the true underlying distribution for every discrete node and edge feature is *categorical*. The categorical distribution is also know as the multi-bernoulli, where every sample has a fixed probability of being in one of the discrete categories. Let $\theta_\text{cat}$ represent the associated vector of un-normalized or relative probabilities, where each entries is $>0$. Then, 

\begin{equation} \label{gumbel-max-eq}
    \text{argmax} \ \log \theta_\text{cat} + \text{Gumbel}(0, 1)
        \sim \text{Cat}(\theta_\text{cat})
\end{equation}

The intuition is that the Gumbel distribution, which is the density of the maximum order statistic of i.i.d. standard normals, makes it a good candidate for modeling the winning or maximum probability category. Adding Gumbel noise to the logits should maintain the true relative proportions, but enough skewness such that every category has some probability of having the maximum noised logit. A proof of equation \ref{gumbel-max-eq} is provide in section @sec-proof. Approximating the argmax function with the Softmax function allows for approximate categorical sampling that is differentiable w.r.t. $\theta_\text{cat}$. The associate inverse uniform CDF sampling formula below is referred to as the *Gumbel-Softmax trick* or the *concrete distribution* [@Maddison_Mnih_Teh_2017]. 
\begin{equation}
    \text{Softmax}
    \left(
        \dfrac{\log \theta_{\text{Cat}} - log(-log \ \epsilon)}{\tau}
    \right), \quad \epsilon \sim U[0, 1].
\end{equation}
$\tau$ is a hyperparameter that controls the degree of relaxation. Smaller value of $\tau$ approximate the discrete sampling better, but can result in numerical issues. The adjacency matrix can be sampled in a similar manner since the Bernoulli is just a special case of the categorical.   
    \begin{equation} \label{binary-concrete}
        \text{sigmoid}
            \left(\dfrac{\log(\theta_A / (1 - \theta_A)) + \log \ \epsilon - \log(1 - \log \ \epsilon)}{\tau} \right), \quad \epsilon \sim U[0, 1], 
    \end{equation}
where $\theta_A$ is an $n \times n$ matrix with each $[i, j]$ entry representing the relative probability that the $ij$ edge exists. Equation \ref{binary-concrete} samples from the *binary concrete distribution* [@Maddison_Mnih_Teh_2017]. Taken together, let  
$$
    G_{\text{gen}} \sim \text{gen}(\Theta)  
$$

notate the combined graph generating distribution, where $\Theta$ is the set of all parameters from the independently sampled distributions. 

\quad An obvious objective is to maximize the likelihood that the *explainee* model predicts a sampled graph to be a member of the target class. Let $\tilde{\rho}$ denote the desired predicted probability vector. Then the above objective can be expressed as: 
    \begin{equation}
        \mathcal{L}_\text{pred} (\Theta \ | \ G_\text{gen}) = \mathams{E}_{G_\text{gen}} \ \text{CrsEnt} (\text{explainee}(G_\text{gen}), \ \tilde{\rho})
    \end{equation}
While the above objective enforces a desirable property, it fails to be restrictive enough to generate realistic graphs. This is because the final prediction, $\rho_{G_{\text{gen}}}$ is compute using only final embeddings, $h^{(L)}_{G_{\text{gen}}}$. Normally $h^{(L)}_{G_{\text{gen}}}$ contains all the needed information from the graphs structure; however, the generation scheme allows the feature distribution to be optimized directly. This means that an explanation can ignore the graph structure and optimize towards the desired final embeddings. It is dangerously common for nonsensical features and structures at the graph level to end up on the desired side of the decision boundary. Empirically, the authors found that even completely random graphs can produce confidence and consistent predictions.   

![An abridged copy of Table 5 from @Wang_Shen_2024.](figures/random_baseline.png){fig-align="center" #fig-random-baseline width="90%"}

For example, @fig-random-baseline shows that random graphs had an average predicted probability of being non-mutagenic in the MUTAG dataset [@Debnath_1991] if 93.2%. In order to mitigate this issue, the authors proposed additional minimizing the cosine distance between the average embedding of all the observed graph from the targeted class, $\bar h^{(L)}_{G_c}$, and the embedding of the generated explanation: 
    \begin{equation}
       \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) = 
            \mathams{E}_{G_\text{gen}}
            \text{CosDist}\left( \bar h^{(L)}_{G_c}, \ h^{(L)}_{G_\text{gen}} \right). 
    \end{equation}

Additionally, the author wanted to encourage sparsity for ease of interpretation. This was done by employing an $L_1$, $L_2$, and a budget penalty on the edge probabilities:  
    \begin{equation}
        \mathcal{L}_{\text{spars}}(\theta_A) = ||\theta_A||_1 + ||\theta_A||_2 + \text{softplus}(\text{sigmoid}||\theta_A||_1 - B)^2, 
    \end{equation}
where $B$ is the expected maximum number of edge for generated explanation graphs.Connectivity is another desirable property as it ensures a cohesive explanation. To encourage connectivity the author minimize the *KL-Divergence* between edge probabilities that share a common node.
    \begin{equation}
        \mathcal{L}_{\text{conn}}(\theta_A) = \sum_{i \in \nu} \sum_{j, k \in \mathcal{E}(i)} D_{KL}(\text{sigmoid}(\theta_A[i, \ j]) \ || \ \text{sigmoid}(\theta_A[i, \ k])), 
    \end{equation}
where $\mathcal{E}(i)$ is the set of edges that connect to node $i$.

\quad The final generator model is trained by sampling $G_\text{gen} \sim \text{gen}(\Theta)$ and then iteratively updating $\Theta$ via gradient descent on the full loss:
    \begin{equation}
        \begin{split}
            \mathcal{L}_{\text{GNNInterpreter}}(\Theta \ | \ G_\text{gen}) = 
            &\lambda_1 \mathcal{L}_{\text{pred}}(\Theta \ | \ G_\text{gen}) + 
            \lambda_2 \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) +  \\
            &\lambda_3 \mathcal{L}_{\text{spars}}(\Theta \ | \ G_\text{gen}) +
            \lambda_4 \mathcal{L}_{\text{conn}}(\Theta \ | \ G_\text{gen}) 
        \end{split}
    \end{equation}
GNNInterpreter achieves remarkable accuracy on most target classes, with most intervals in @fig-GNNInt-pred-results being tight and very close to 1. This indicates that the generated examples are almost always classified as the targeted class. The explanations for the house motif and the lollipop shape perform worse in terms of predictions. Although the authors critique the use of predictions as the sole objective, they do not employ any other quantitative metrics to evaluate the validity of their explanations.

![A copy of the quantitative modeling results from Table 2 in @Wang_Shen_2024.](figures/GNNInt_prediction_results.png){fig-align="center" #fig-GNNInt-pred-results width="90%"}

![A copy of the qualitative modeling results from Figure 1 in @Wang_Shen_2024.](figures/GNNInt_drawn_results.png){fig-align="center" #fig-GNNInt-Drawn width="80%"}

Qualitatively, @fig-GNNInt-Drawn displays limitations in terms of realism. For instance, for the mutagen class, the explanation correctly identifies the importance of the N02 group; however, the generated graph is unrealistic and might not even be chemically feasible. Additionally, the non-mutagen example lacks clear patterns or identifiable structures. The explanations for the Cyclicity dataset, as well as the Wheel and Grid classes, do not seem to be from their respective underlying data distributions. In summary, GNNInterpreter offers a method for generating example graphs that would be classified as a targeted class by a GNN model, without requiring domain-specific rules. However, optimizing for predictions and even embeddings alone does not seem to be a sufficient objective for producing in-distribution graphs. While the authors correctly note that focusing on predictions can lead to unrealistic graphs, they also inadvertently show that optimizing embeddings alone is not necessarily sufficient either.

## D4Explainer [@Chen_Wu_Gupta_Ying_2023]

\quad D4Explainer [@Chen_Wu_Gupta_Ying_2023], or in-**D**istribution GNN Explanations via **D**iscrete **D**enoising **D**iffusion, directly addresses the realism of generated graphs in model-level explanations by using a diffusion based generator model. In the image domain, diffusion models have proven to produce more realistic images than other generative AI methods, such as Generative Adversarial Networks (GANs). These models work by iteratively adding noise to an observation until it becomes pure noise. A denoising model is then trained to predict the noise added at any step. New observations can be generated by reversing this process, passing pure noise through the denoising model, a technique known as reverse sampling. Additional objectives, such as a desired class label, can be added to the loss function to generate samples with certain properties.   

\quad The authors here are focused on *discrete structural diffusion*. D4Explainer generate example graph by noising and denoising the adjacency matrices of observed graphs. The sampled graphs have the same features, but different structures. The process of gradually adding noise to the input data is called *forward diffusion*. During forward diffusion, random noise is added iteratively until the data becomes pure noise at the final iteration. This ensures that the denoising model can start with pure noise. Forward diffusion is usually a Markov process. Let $t \in [0, T]$ denote the current iteration. Let $\beta_t$ be the common probability that any edge changes state at time step $t$. $(\beta_1, \dots, \beta_T)$ is known as the variance schedule and is a set hyperparameter. Let $A_t$ be a one-hot encoded version of the $t^{th}$ noised adjacency. Then the forward diffusion process can be expressed as:  
    \begin{equation}
         A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{t-1}[i, \ j]) 
            = \text{Cat}(A_{t-1}[i, \ j] \cdot Q_t)
    \end{equation}
    or 
    \begin{equation}
         A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{0}[i, \ j]) 
            = \text{Cat}\left(A_0[i, \ j] \prod_{i=1}^t  Q_i \right)
    \end{equation}
    where, 
    $$
    Q_t = 
    \left[
    \begin{matrix}
        1-\beta_t & \beta_t \\ 
        \beta_t & 1 - \beta_t
    \end{matrix}
    \right]
    $$
the $t^{th}$ element-wise transition matrix. Even though only the adjacency matrix is being noised, the authors still wanted to use all of the available information during the *backwards diffusion* process. Thus the denoising model was parameterized as: 
    \begin{equation}
         p(A_0 \ | \ A_t, t, X_0, E_0; \ \Omega)  
    \end{equation}
where $\Omega$ is the set of trainable parameters. The authors employed a *Provably Powerful Graph Network* (PPGN) [@Maron_Ben-Hamu_Serviansky_Lipman_2020] as their denoising architecture; however, they have added an additional neural network to learn the time or noise level effect. Like most diffusion models, the primary objective is to minimize the distance between the predicted denoised observation and the original. The authors have added an additional weight term to focus the model on noisier or more difficult training examples. The core loss function is expressed as: 
    \begin{equation} \label{L-dist}
        \mathcal{L}_{dist} (\Omega \ | \ A_0) =  \sum_{t=1}^T 
            \left(1 - 2 \bar \beta_t + \dfrac 1 T \right)
            \mathams{E}_{\hat A_0}
            \text{CrsEnt} \left(
                A_0, \hat A_0
            \right), 
    \end{equation}
where $\hat A_0 = p(A_0 \ | \ A_t, t, X_0, E_0; \ \hat \Omega)$, $A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{0}[i, \ j])$, and 
$$
    \bar \beta_t = \frac 1 2 - \frac 1 2 \prod^t_{i=1}(1-2\beta_i), 
$$ 

the cumulative transition probability. After the denoising model has been trained, model-level explanations are produced by iterative sampling candidate adjacency structures for an observed set of graph features and selecting the candidate with the highest probability of being from the targeted class. This is a multi-step procedures, where at each step $A_t$ is denoised to $k$ candidates $A_0, 1 \dots k$. The candidate with the best predicted probability is then noised to a level of $t-1$ and the process is repeated. The pseudocode is reproduced below.
    \begin{algorithm}
    \caption{D4Explainer Model-level Explanation Reverse Sampling Algorithm}\label{alg:cap}
    \begin{algorithmic}
        \Require $\hat \Omega$: trained denoising parameters; 
                $q(A_t \ | \ A_0)$: forward diffusion process.
        \renewcommand{\algorithmicrequire}{\textbf{Input:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \Require N: maximum number of nodes; T: maximum noise level; 
                K: number of candidates per iteration; 
                $\tilde{\rho}$ targeted prediction vector; 
                $(X, E)$: node and edge features.  
        \Ensure $\hat A$: adjacency matrix for model-level explanation.
        \State Sample $A_T[1:n, 1:n]$ \sim Bernoulli(0.5)
        \For{t in T to 1}
            \State Sample candidates 
                $\{\hat A_{0, k} \sim p(A_t,t, X, E; \ \hat \Omega) : k \in 1, \dots, K\}$
            \State Select the best candidate 
            $\underset{j \ \in \ i, \dots K}{\text{argmin}}$ 
            CrsEnt(explainee($G = (X, A_{0, j}, E)), \ \tilde{\rho}$)
            \State Sample $A_{t-1}[1:n, 1:n] \sim q(A_{t-1}, A_{0, j})$
        \EndFor
        \State \Return $A_0$
    \end{algorithmic}
    \end{algorithm}

\quad To assess how well a sampled explanation matches the observed graph distribution, the authors compute various maximum mean discrepancy (MMD) statistics. MMD is a general technique used to measure the distance between two data distributions based solely on observed data. The distributions are estimated using *kernel density estimation*, and then the distance between their means is compared. As long as the chosen kernel is *characteristic*, an MMD statistics defined as the l2 distance between *kernel mean embeddings* is representative of the distance between the true distributions [@Gretton_Borgwardt_Rasch_Schölkopf_Smola_2012]. The *Gaussian Earth Mover’s Distance kernel* was selected here. The author use MMD statistics to compare the degree, clustering, and spectrum distributions of the generated explanation against the observed data. The degree distribution of a graph indicates how frequently nodes have different numbers of connections, reflecting the overall connectivity pattern. The clustering coefficient of a node measures the proportion of the node's neighbors that are also connected to each other, representing local clustering. The spectrum distribution, which is the distribution of eigenvalues of the adjacency matrix or Laplacian matrix, provides insights into the graph’s structural characteristics and dynamic properties. Similar to GNNInterpreter, the author of D4Explainer also value the sparsity of their generate example. They measure the density of a graph as the number of present edges divided by the number of possible edges or 
    \begin{equation}
        \text{Density} = \mathcal{|E|} / |\nu|^2. 
    \end{equation}

\quad In additional to model-level explanations, D4Explainer can provide *counterfactual explanations* by adding a term to the loss function. The authors define a counterfactual explanation for a particular observed graph to be $G^c$ such that $\hat Y_{G^c} \neq \hat Y_G$, but the difference between $G^c$ and $G$ is minimal. $\mathcal{L}_{dist}$ (\ref{L-dist}) ensures that the $G^c$ generated does not stray too far from the observed graph distribution, and adding $\mathcal{L}_{CF}$ minimize the probability that the generated graph is classified the same as the input. 
    \begin{equation}
         \mathcal{L}_{CF}(\Omega \ | \ A_0) = 
            \mathams{E}_{\hat A_0} - \log(1 - \rho_{G^c}[\hat Y_{G}]), 
    \end{equation} 
where $G = (X, A, E)$ is an observed graph, $G^c = (X, \hat A_0, E)$, and $\rho_{G^c}[\hat Y_{G}]$ denote the probability that $G^c$ belongs to the same class as $G$. To quantify the quality of generated counterfactual explanations, the author report *Counterfactual Accuracy*, *Fidelity*, and *modification ratio*. CF-ACC measure the proportion of $G^c$s that have different predicted labels than there associated observed graph.  
    \begin{equation}
            \text{CF-ACC} = \mathams{E}_{\hat A_0} \ I(\hat Y_{G^c} \neq \hat Y_{G}) 
    \end{equation}

Fidelity measures the difference in the predicted probability of $G$ and $G^c$ with respected to the original class label.  
    \begin{equation}
        \text{Fidelity} = \mathams{E}_{\hat A_0} \ \rho_{G}[\hat Y_G] - \rho_{G^c}[\hat Y_G]
    \end{equation}

Finally, modification ratio measures difference in edges between $G^c$ and $G$ relative to the size of the original graph.  
    \begin{equation}
        \text{MR} = \mathams{E}_{\hat A_0} \ \dfrac{|\sum_{i, j} A[i, j] - \hat A_0(i, j)|}{\sum_{i, j} A[i, j]}
    \end{equation}

\quad Similar to GNNInterpreter, the author report the mean predicted probability for the targeted class. Using a maximum of 6 nodes, the author were able to achieve a mean of 0.832 on the MUTAG dataset. Since the counterfactual explanation are generated on a per observation basis, the author's compared their method to other popular instance-level explanation methods.     

![Comparison of the CF-ACC relative to MR of various methods and datasets. A copy of Figure 4 from @Chen_Wu_Gupta_Ying_2023.](figures/D4-CF-Plot.png){fig-align="center" width="90%" #fig-D4-CF-ACC}

@fig-D4-CF-ACC displays that D4Explainer can flip the prediction on roughly 80% of observed graphs at most modification levels. Missing from the plot is a variance measure. It is possible that the counterfactual accuracy varies more between different data splits or hyperparameter than the other methods. @fig-D4-MMD indicates that the D4Explainer generated graphs tend to have feature distributions that are closer to the original dataset; however, there might be a hidden class bias. The distributions might only be close for the majority class, skewing the averages.      

![MMD statistic for various methods and datasets copied from Table 2 in @Chen_Wu_Gupta_Ying_2023.](figures/D4-MMD-Table.png){fig-align="center" width="90%" #fig-D4-MMD}

## ProtGNN [@Zhang_Liu_Wang_Lu_Lee_2021]

\quad Up until now, we have been discussing post-hoc explanation methods; however, @Rudin_2019 raises two important concerns with this approach. First, parity in the predictions does not guarantee parity in the reasoning. Explanations may bear strong predictions, but for distinct and sometimes nonsensical reasons. Recall that @fig-random-baseline showed that completely random graphs can illicit confident predictions. Second, feature importance indicates that a feature is significant to the model, but it does not reveal how the model uses the feature. For instance, a substructure might lead to a prediction due to a strong relation with the target class, or it could be important because of a strong negative relation with other classes. ProtGNN [@Zhang_Liu_Wang_Lu_Lee_2021] uses the prototype modeling paradigm to create a GNN that is *interpretable*. Prototype based models make prediction by comparing new inputs to exemplar cases or *learned prototypes* of each class. This results in an interpretable model because, as long as the prediction function applied to the prototype similarity scores is simple, the reasoning for any given prediction will be clear. Intuitively, this can be thought of as using a black-box model to engineer features, which are then passed to a straightforward white-box model.

\quad ProtGNN initializes $m$ random prototype vectors for each of the $C$ classes. Let $p_k$ denote a prototype vector and $P_{Y_G}$ denote the set of prototypes vectors assigned to the ground truth label of $G$, $Y_G$. Now, let $f(G; \ \Omega_f) = \hat p_G$ be a graph encoder function that maps any graph into the prototype vector space, where $\Omega_f$ is the set of trainable parameters. $f$ is generally one of the standard GNN architectures. Since ProtGNN was primarily designed to be a graph classification model, the author optimized for predictive power using:      
    \begin{equation}
        \mathcal{L}_{\text{prot}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mC} \ | \ G)= \mathams{E}_G \ \text{CrsEnt} \left[\psi \left(sim\left(\hat p_G, p_1\right), \dots, sim\left(\hat p_G, p_{mC}\right); \ \Omega_\psi\right), \ Y_G \right]
    \end{equation}
where $\psi$ is any simple prediction model with trainable parameters $\Omega_\psi$ and $sim$ is any vector similarity function. ProtGNN uses a full connected layer with a softmax output activation for $\psi$, which is similar to a multinomial regression model. To address similar out-of-distribution concerns discussed earlier, the authors include three additional loss terms as constraints. The cluster loss ensure that each embedding is close to at least one prototype assigned to its ground truth class:  
    \begin{equation}
        \mathcal{L}_{\text{clst}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mC} \ | \ G) = \mathams{E}_G \ \underset{p_k \in P_{Y_G}}{\text{min}} || \hat p_G - p_k ||^2_2. 
    \end{equation}
A separation loss ensures that embeddings are far from the prototypes of other classes: 
    \begin{equation}
        \mathcal{L}_{\text{sep}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mC} \ | \ G) = - \mathams{E}_G \ \underset{p_k \notin P_{Y_G}}{\text{min}} || \hat p_G - p_k ||^2_2. 
    \end{equation}
Finally, the diversity loss encourages each prototype within a class to learn different information:  
    \begin{equation}
            \mathcal{L}_{\text{div}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mC} \ | \ G) = \sum_{k=1}^C \sum_{p_i \neq p_j \in P_k} \ \max(0, cos(p_i, p_j) - s_\text{max})
    \end{equation}
where $s_\text{max}$ is a set similarity threshold. 

\quad The learned prototypes are latent embedding vectors that are not directly interpretable. To address this issue, the authors use a Monte Carlo Tree Search (MCTS) algorithm [@Coulom2006EfficientSA] to identify the subgraph within the observed graphs of the prototype's class that has an embedding closest to the given prototype. Specifically, the author use the MCTS to select an optimal sequence of pruning actions. Let $\mathcal{G}_{0, \ i, \dots, j}$ represent the subgraph resulting from the sequence of pruning actions $\text{act}_i,\dots, \text{act}_j$, where $\mathcal{G}_0$, the root of the search tree, is an observed graph. The MCTS algorithm proceeds in 4 core steps. First, an existing node in the search tree $\mathcal{G}_{0, \ i, \dots, j}$ is *selected*. Second, the chosen node is *expanded* by adding a child node $\mathcal{G}_{0, \ i, \dots, j, k}$ resulting from action $\text{act}_k$. To limit the search space, the authors restrict $\text{act}_k$ to be the removal of a peripheral node with minimum degree. Third, the reward for the sequence of actions $i, \dots, j, k$ is estimated via a Monte Carlo *simulation* where additional random actions are taken until a subgraph of the desired size is produced. The sampled embeddings are then compared to the prototype vector and the similarities are averaged. Finally, the child node's estimated score is then *backpropagated* to update the estimated scores of all its parent nodes. After a fixed number of iterations through the observed graphs from the prototype's class, the decision path with the highest resulting score is selected to produce the ultimate prototype projection. In addition to finding the closest subgraph in the observed class, the authors also suggest a method for finding similar subgraphs within each input. This is done by training a neural network to predict an edge mask conditional on a prototype vector:  
    \begin{equation}
        \hat A[i, \ j] = \text{nn}\left(G, p_k; \ \Omega_{\text{nn}}\right).  
    \end{equation}

The resulting induced subgraphs' embeddings are then optimized to be as close as possible to the given prototype: 
    \begin{equation}
        \mathcal{L}(\Omega_{\text{nn}}, \ \Omega_f, \ p_k \ | \ G) = -\mathams{E}_G \ sim(\hat p_G, p_k), 
    \end{equation}

thus producing a similar observed subgraph per learned prototype. The authors denote the version of the model that utilizes this *conditional subgraph sampling module*, in addition to the above losses, as **ProtGNN+**.

\quad ProtGNN achieves similar or better graph classification accuracy than state of the art models across 5 standard benchmark datasets (see @fig-prot-acc-table). 

![A copy of the quantitative modeling results report in Table 1 of @Zhang_Liu_Wang_Lu_Lee_2021.](figures/prot_acc_table.png){fig-align="center" width="90%" #fig-prot-acc-table}

@fig-prot-acc-table also indicates that ProtGNN is robust to the choice of embedding model as there seems to be benefits across various GNN architectures. The authors do note that the computational complexity of ProtGNN can be significantly higher than a standard GNN model, taking roughly 5 times longer to train. Most of this complexity is attributable to the use of the MCTS. On the other hand, ProtGNN allows for easier and clearer inference. @fig-prot-diagram demonstrates that the output of ProtGNN is similar to a simple regression model. The prototypes can be thought of as features, the similar subgraphs and similarity scores as the observed feature values, and the class connections as the marginal effects on the prediction.         

![An example ProtGNN+ output for the MUTAG dataset taken from Figure 3 in @Zhang_Liu_Wang_Lu_Lee_2021.](figures/prot_inference_ex.png){fig-align="center" width="90%" #fig-prot-diagram}

Since prototypes from every class are used for the classification, ProtGNN can also be used for counterfactual type analysis. For any given predictions, researcher can determine why a certain classification was made and why the other classes were not selected. @fig-prot-diagram display both a strong similarity to the mutagenic prototypes as well as a weaker relation to the non-mutagenic prototypes. 

# Synthesis of Core Papers

# Technical Details 

## Methodology

## Results

## Proofs {#sec-proof}

**Proof of Equation \ref{gumbel-max-eq}:** WLOG assume we want to sample for a categorical distribution $\text{cat}(\theta_\text{cat})$ with a set of $W$ categories each with a probability 
$$
\pi_\omega = \dfrac{\theta_\text{cat}[\omega]}{\sum_{i \in W}\theta_\text{cat}[i]}.
$$ 

Define random variable 
$$
D = \text{argmax} \ \log \theta_\text{cat} + \text{Gumbel}(0, 1) = \underset{i \in W}{\text{argmax}} \ \log \theta_\text{cat}[i] + \text{Gumbel}(0, 1)[i], 
$$

here $\text{Gumbel}(0, 1)[i]$ denotes the $i^{th}$ i.i.d. sample from the associated Gumbel. In order for $D$ to be a true categorical random variable, $Pr[D = \omega]$ need to be $\pi_\omega$. $D = \omega$ if and only if $\log \theta_\text{cat}[\omega]  + \text{Gumbel}(0,1)[\omega] > \log \theta_\text{cat}[i] + \text{Gumbel}(0, 1)[i] \ \forall i \in W \setminus \omega$. Now, let $M_i$ denote a random variable that follows a $\text{Gumbel}(\log \theta_\text{cat}[i], 1)$ distribution. Then, 
    \begin{align*}
        Pr[D = \omega] 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in W \setminus \omega} Pr(M_i < m_\omega) 
                \text{ i.i.d location shifted Gumbel distributions.} \\ 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in W \setminus \omega} \exp 
                    \left(-e^{\log \theta_\text{cat}[i] - m_\omega} \right)
                \text{ Gumbel CDF.} \\ 
            &= \mathams{E}_{M_\omega} 
                \exp \left(-\sum_{i \in W \setminus \omega}
                e^{\log \theta_\text{cat}[i] - m_\omega} \right) \\ 
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\text{cat}[\omega] - m_\omega \right) 
                \exp \left(-e^{\log \theta_\text{cat}[\omega] - m_\omega} \right) \cdot 
                \exp \left (-\sum_{i \in W \setminus \omega}
                e^{\log \theta_\text{cat}[i] - m_\omega} \right) \ dm \\
                &\text{ Gumbel PDF.} \\
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\text{cat}[\omega] - m_\omega \right) 
                \exp \left (-\sum_{i \in W}
                e^{\log \theta_\text{cat}[i] - m_\omega} \right) \ dm \\
            &= \int_{-\infty}^{\infty}
                \theta_\text{cat}[\omega] 
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in W}
                    \theta_\text{cat}[i]  \right) \ dm \\
            &= \pi_\omega \sum_{i \in W} \theta_\text{cat}[i]
            \int_{-\infty}^{\infty}
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in W}
                    \theta_\text{cat}[i]  \right) \ dm 
                    \text{ From the above definition of } \pi_\omega \\
            &= \pi_\omega \sum_{i \in W} \theta_\text{cat}[i]
                \dfrac{\exp\left( -e^{-m_\omega} \sum_{i \in W} \theta_\text{cat}[i] \right)
                    }{\sum_{i \in W} \theta_\text{cat}[i]} \bigg|_{-\infty}^\infty \\ 
            &= \pi_\omega \sum_{i \in W} \theta_\text{cat}[i]
                \dfrac{1}{\sum_{i \in W} \theta_\text{cat}[i]} = \pi_\omega. 
    \end{align*}
    Reference: @Huijben_Kool_Paulus_van_Sloun_2022 

# Future Directions

\pagebreak 

# References

::: {#refs}
:::

# Appendix 