% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
  letterpaper,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{amsfonts}
\DeclareMathAlphabet{\mathams}{U}{msb}{m}{n}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Qualifying Exam},
  pdfauthor={Art Tay},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Qualifying Exam}
\author{Art Tay}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, boxrule=0pt, frame hidden, breakable, borderline west={3pt}{0pt}{shadecolor}, sharp corners, enhanced]}{\end{tcolorbox}}\fi

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{itemize}
\item
  Overview of the problem area.

  \begin{itemize}
  \tightlist
  \item
    Graphs are an important data structure.

    \begin{itemize}
    \tightlist
    \item
      GRAPHS provide an incredibly flexible structure for modeling
      complex data. Data can naturally appear as graphs, like molecules.
      We can reduce data to a graph, such as the key points of a image.
      We can even use graphs to add structure, such as grammatical
      relationships.
    \end{itemize}
  \item
    GNN models are good at prediction and inference on graph data.

    \begin{itemize}
    \tightlist
    \item
      Graph Neural Networks (GNNs) have become a popular choice for
      prediction and inference on graph data. At their core, GNNs work
      by iteratively updating node embeddings based on information from
      neighboring nodes. The idea is to use the graph's structure to
      engineer better features. This message passing scheme allows GNNs
      to capture complex dependencies and patterns present within the
      graph structure. GNN architectures typically consist of multiple
      layers, each performing message passing and aggregation operations
      to refine the embeddings. These layers are often followed by
      pooling and dense prediction layers to produce the final output.
    \end{itemize}
  \item
    There are many important applications for graph classification
    models.

    \begin{itemize}
    \tightlist
    \item
      Some important applications of graph classification include
      predicting chemical toxicity (Bai et al. 2019), classifying
      proteins (Gallicchio and Micheli 2019), and even detecting cancer
      from pathology slides (Xiao et al. 2023).
    \end{itemize}
  \item
    \textbf{Problem:} While GNNs achieve remarkable predictive power,
    their complexity prevents the exaction of the scientific rationale.
  \end{itemize}
\item
  Why is the problem important?

  \begin{itemize}
  \tightlist
  \item
    Explaining or interpreting GNN predictions would

    \begin{itemize}
    \tightlist
    \item
      help with the adoption of such models for critical applications,
    \item
      prevent adversarial attacks,
    \item
      detect potential implicit discrimination,
    \item
      guide scientific as well as machine learning research.
    \end{itemize}
  \end{itemize}
\item
  How does the problem relate to the fundamentals areas of Statistics?

  \begin{itemize}
  \item
    Explain-ability vs Interpretability

    \begin{itemize}
    \tightlist
    \item
      Yuan et al. (2022)
    \item
      A model is interpretable if the models decision process can be
      readily understood by humans. For example, a linear regression
      model is interpretable because the coefficient clearly define how
      any prediction get made.
    \item
      A model is explainable if the models prediction can be reasoned
      post-hoc. Permuting each variable and measuring the variation in
      the predictions can be used to estimate each variables marginal
      effect {[}cite{]}.
    \end{itemize}
  \item
    One goal would be to create a GNN type model whose decision process
    is human interpretable. A straight translation from statistics would
    be a circuit type analysis {[}cite{]}. For graphs, this would mean
    some form of coefficients on subgraphs producing the prediction.
  \item
    Another goal might be to develope a method that determines if a
    feature is statistical significant to the GNN model. The challenge
    is that the graph features that matter to researchers aren't
    necessarily tabular.
  \end{itemize}
\item
  What is the impact of solving this problem?

  \begin{itemize}
  \item
    In the application where GNNs have shown strong predictive power, we
    can exact a testable scientific hypothesis for the nature of the
    classification.
  \item
    In the application where GNNs have weak predictive power, highlight
    the potential misunderstandings the model is having.
  \end{itemize}
\end{itemize}

\hypertarget{notation}{%
\section{Notation}\label{notation}}

\begin{itemize}
\item
  Let \(G\) denote a graph.
\item
  Any graph \(G\) can be describe by \(X, A, E\). The node feature
  matrix, edge feature matrix, and adjacency matrix respectively
\item
  Let \(X = [X_c, \ X_d]\), where \(X_c\) is the subset of continuous
  node features and \(X_d\) is the subset of one-hot discrete node
  features.
\item
  Let \(E = [E_c, \ E_d]\), denoted in the same manner.
\item
  Let \(n\) represent the number of nodes in the graph and \(v\)
  represent the number of edges.
\item
  Let \(\text{feat}_{(.)}\) denote the number of features or columns in
  the the corresponding feature matrix.
\item
  \(A\) is a binary \(n \times n\) matrix where \(A[i, \ j] = 1\)
  indicates that an edge exists between nodes labeled \(i\) and \(j\).
\item
  Let
  \(\text{explainee}(G; \ \Omega) = h^{(1)}_G, \dots h^{(L)}_G, \rho_G\)
  be an \(L\) layer GNN model with parameters \(\Omega\) that we would
  like to explain.
\end{itemize}

\hypertarget{analysis-of-core-papers}{%
\section{Analysis of Core Papers}\label{analysis-of-core-papers}}

\hypertarget{gnninterpreter}{%
\subsection{GNNInterpreter}\label{gnninterpreter}}

(Wang and Shen 2024)

\begin{itemize}
\item
  Overview

  \begin{itemize}
  \item
    Instance v. Model Level

    \begin{itemize}
    \tightlist
    \item
      In general, explanation methods serve to elucidate which features
      within the data influence disparate predictions. These methods
      typically fall into two categories: instance-level and
      model-level. Instance-level explanations aim to unveil the model's
      rationale behind a particular prediction. In domains such as image
      and text analysis, a prevalent approach involves masking or
      perturbing the instance and assessing the impact on the model's
      prediction. On the other hand, model-level explanations seek to
      understand how a model generally distinguishes between classes. In
      image and text analysis, for instance, one common technique
      involves treating the input as a trainable parameter and
      optimizing the model's prediction towards a specific class.
      Consequently, the resulting optimized input comprises a set of
      features strongly associated with the targeted class.
    \end{itemize}
  \item
    GNNInterpreter provides model level explanations for GNN in this
    manner.
  \item
    Formally, GNNInterpreter tries to learn the graph generating
    distribution for each class.
  \item
    GNNInterpreter works by optimizing the parameters of a generic graph
    generating distribution to produce samples that closely match the
    explainee's understanding of the targeted class.
  \end{itemize}
\item
  Explanation of the graph generating distribution.

  \begin{itemize}
  \item
    Graph generating distributions are hard to specify because there can
    be discrete and continuous elements of \(X\), \(E\) and \(A\).
    Furthermore, the interactions between these matrices can be complex.
  \item
    The authors tackle these issues by making two simplify assumptions.

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \item
      Assume that \(G\) is a \emph{Gilbert} random graph, every possible
      edge as an independent fixed probability of occurring. \[
           \forall (i, \ j) \neq (k, l) \
               Pr(A[i, \ j] = 1) \perp Pr(A[k, \ l] = 1)
       \]
    \item
      The features of every node and edge are independently distributed.
    \end{enumerate}
  \item
    The author justify these assumptions by:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      The other graph distributions aren't suitable.

      \begin{enumerate}
      \def\labelenumii{\alph{enumii}.}
      \tightlist
      \item
        Erdo-Renyi graphs have a fixed number of edges and nodes.\\
      \item
        Rado graphs are infinite in size.
      \item
        The random dot-product graph model is just a generalization of
        Gilbert random graphs.
      \end{enumerate}
    \item
      Because the parameters of the independent distributions will be
      updated jointly using the \emph{explainee} model, the
      \emph{explainee's} understanding of the latent correlation
      structure should be contained in the final estimates.
    \end{enumerate}
  \item
    \(X_c\) and \(E_c\) can be sampled from any continuous distribution
    that can be expressed as a location-scale family. Separating the
    stochastic and systematic components is necessary for gradient based
    optimization. It is commonly known as the ``re-parametrization
    trick''.
  \item
    \(X_d\), \(E_d\) as well as \(A\) need to be sampled from a
    continuous distribution for gradient based optimization, but the
    distribution has to have sampling properties close to a discrete
    distribution.
  \item
    The author assume that the true underlying distribution for every
    discrete node and edge feature is \emph{categorical}. The
    categorical distribution is also know as the multi-bernoulli, where
    every sample has a fixed probability of being in one of the discrete
    categories.
  \item
    Suppose there are \(D\) categories with probabilities
    \(\pi_\omega = \dfrac{\theta_\omega}{\sum_{i \in D}\theta_i}\). Then
    \[
      I = \underset{i \in D}{\text{argmax}} \ \log \theta_i + G^{(i)} 
          \sim \text{Cat}(\pi)
      \] where \(G^{(i)} \overset{i.i.d.}{\sim} \text{Gumbel}(0, 1)\).
  \item
    The intuition is that the Gumbel or extreme value distribution is
    the density of the maximum order statistic of i.i.d. standard
    normals which makes it a good candidate for model the winning or
    maximum probability category. Adding Gumbel noise to the logits
    should maintain the true relative proportions, but enough skewness
    such that every category has some probability of having the maximum
    noised logit.
  \item
    \textbf{Proof 1:} In order for \(I\) to be a true categorical
    distribution, \(Pr[I = \omega] = \pi_\omega\). \(I = \omega\) if and
    only if
    \(\log \theta_\omega + G^{(\omega)} > \log \theta_i + G^{i} \   \forall i \in D \setminus \omega\).
    Let \(M_i\) denote a random variable that follows a
    \(\text{Gumbel}(\log \theta_i, 1)\) distribution. \begin{align*}
      Pr[I = \omega] 
          &= \mathams{E}_{M_\omega} 
              \prod_{i \in D \setminus \omega} Pr(M_i < m_\omega) 
              \text{ i.i.d location shifted Gumbel distributions.} \\ 
          &= \mathams{E}_{M_\omega} 
              \prod_{i \in D \setminus \omega} \exp 
                  \left(-e^{\log \theta_i - m_\omega} \right)
              \text{ Gumbel CDF.} \\ 
          &= \mathams{E}_{M_\omega} 
              \exp \left(-\sum_{i \in D \setminus \omega}
              e^{\log \theta_i - m_\omega} \right) \\ 
          &= \int_{-\infty}^{\infty}
              \exp \left( \log \theta_\omega - m_\omega \right) 
              \exp \left(-e^{\log \theta_\omega - m_\omega} \right) \cdot 
              \exp \left (-\sum_{i \in D \setminus \omega}
              e^{\log \theta_i - m_\omega} \right) \ dm \\
              &\text{ Gumbel PDF.} \\
          &= \int_{-\infty}^{\infty}
              \exp \left( \log \theta_\omega - m_\omega \right) 
              \exp \left (-\sum_{i \in D}
              e^{\log \theta_i - m_\omega} \right) \ dm \\
          &= \int_{-\infty}^{\infty}
              \theta_\omega 
              \exp \left( -m_\omega \right) 
              \exp \left( -e^{-m_\omega} \sum_{i \in D}
                  \theta_i  \right) \ dm \\
          &= \pi_\omega \sum_{i \in D} \theta_i
          \int_{-\infty}^{\infty}
              \exp \left( -m_\omega \right) 
              \exp \left( -e^{-m_\omega} \sum_{i \in D}
                  \theta_i  \right) \ dm 
                  \text{ From the above definition of } \pi_\omega \\
          &= \pi_\omega \sum_{i \in D} \theta_i
              \dfrac{\exp\left( -e^{-m_\omega} \sum_{i \in D} \theta_i \right)
                  }{\sum_{i \in D} \theta_i} \bigg|_{-\infty}^\infty \\ 
          &= \pi_\omega \sum_{i \in D} \theta_i
              \dfrac{1}{\sum_{i \in D} \theta_i} = \pi_\omega
      \end{align*} Reference: Huijben et al. (2022)
  \item
    Using inverse CDF sampling and and relaxing the argmax to a Softmax,
    we can sample one-hot categorical vectors based on two parameters
    \(\theta_{\text{Cat}}\), a trainable parameter vector of length
    equal to the number of categories, and \(\tau\), a hyperparameter
    that controls the degree of relaxation (smaller value approximate
    the discrete sampling better, but can result in numerical issues).
    \[
      \text{Softmax}
      \left(
          \dfrac{\theta_{\text{Cat}} - log(-log \ \epsilon)}{\tau}
      \right), \quad \epsilon \sim U[0, 1]
      \] This method, known as the concrete distribution (Maddison,
    Mnih, and Teh 2017), yields a reasonable smooth gradient w.r.t. to
    the probability parameters.
  \item
    The adjacency matrix can be sampled in a similar manner since the
    Bernoulli is just a special case of the categorical.\\
    \[
      \text{sigmoid}
          \left(\dfrac{\theta_A + \log \ \epsilon - \log(1 - \log \ \epsilon)}{\tau} \right)
      \] This is known as the binary concrete distribution (Maddison,
    Mnih, and Teh 2017).
  \item
    Notate the combined graph generating distribution as: \[
      G_{\text{gen}} \sim \text{gen}(\Theta)  
      \] where \(\Theta\) is the set of all parameters from the
    independently sampled distributions.
  \end{itemize}
\item
  Prediction objective.

  \begin{itemize}
  \item
    An obvious objective is to maximize the likelihood that the
    \emph{explainee} model predicts a sampled graph to be a member of
    the target class.
  \item
    Let \(\tilde{\rho}\) denote the desired predicted probability
    vector. Then the above objective can be expressed as: \[
      \underset{\Theta}{\text{argmin}} \ 
          \mathcal{L}_{\text{pred}}(\Theta) = 
          \mathams{E}_{G_{\text{gen}}} \ 
              \text{CrtEnt}(\text{explainee}(G_{\text{gen}}), \tilde{\rho})
      \]
  \end{itemize}
\item
  Embedding objective.

  \begin{itemize}
  \tightlist
  \item
    The authors note that \ldots{}
  \end{itemize}
\item
  Regularization terms.
\item
  Summary of Results + Figures

  \begin{itemize}
  \tightlist
  \item
    Metrics
  \end{itemize}
\end{itemize}

\hypertarget{d4explainer}{%
\subsection{D4Explainer}\label{d4explainer}}

(Chen et al. 2023)

\begin{itemize}
\item
  Note on counter-factual explanations.
\item
  Graph diffusion.
\end{itemize}

\hypertarget{protgnn}{%
\subsection{ProtGNN}\label{protgnn}}

(Zhang et al. 2021)

\hypertarget{synthesis-of-core-papers}{%
\section{Synthesis of Core Papers}\label{synthesis-of-core-papers}}

\begin{itemize}
\tightlist
\item
  Comparison of generation methods.

  \begin{itemize}
  \tightlist
  \item
    GNNInterpreter uses continuously relaxed discrete distributions.
  \item
    D4Explainer uses diffusion.
  \item
    Diffusion is slower, but can be more realistic. Probably because
    diffusion is less subject to the \textbf{out-of-distribution (OOD)
    problem}.
  \item
    Prototype projection are like generative methods. Restricted to in
    distribution, but realism is all but guaranteed.
  \end{itemize}
\end{itemize}

\hypertarget{technical-details}{%
\section{Technical Details}\label{technical-details}}

\begin{itemize}
\tightlist
\item
  Minimal reproduction of each method on MUTAG.
\end{itemize}

\hypertarget{future-directions}{%
\section{Future Directions}\label{future-directions}}

\pagebreak

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bai2019unsupervised}{}}%
Bai, Yunsheng, Hao Ding, Yang Qiao, Agustin Marinovic, Ken Gu, Ting
Chen, Yizhou Sun, and Wei Wang. 2019. {``Unsupervised Inductive
Graph-Level Representation Learning via Graph-Graph Proximity.''}
\url{https://arxiv.org/abs/1904.01098}.

\leavevmode\vadjust pre{\hypertarget{ref-Chen_Wu_Gupta_Ying_2023}{}}%
Chen, Jialin, Shirley Wu, Abhijit Gupta, and Rex Ying. 2023.
{``D4Explainer: In-Distribution GNN Explanations via Discrete Denoising
Diffusion,''} no. arXiv:2310.19321 (October).
\url{https://doi.org/10.48550/arXiv.2310.19321}.

\leavevmode\vadjust pre{\hypertarget{ref-gallicchio2019fast}{}}%
Gallicchio, Claudio, and Alessio Micheli. 2019. {``Fast and Deep Graph
Neural Networks.''} \url{https://arxiv.org/abs/1911.08941}.

\leavevmode\vadjust pre{\hypertarget{ref-Huijben_Kool_Paulus_van_Sloun_2022}{}}%
Huijben, Iris A. M., Wouter Kool, Max B. Paulus, and Ruud J. G. van
Sloun. 2022. {``A Review of the Gumbel-Max Trick and Its Extensions for
Discrete Stochasticity in Machine Learning,''} no. arXiv:2110.01515
(March). \url{https://doi.org/10.48550/arXiv.2110.01515}.

\leavevmode\vadjust pre{\hypertarget{ref-Maddison_Mnih_Teh_2017}{}}%
Maddison, Chris J., Andriy Mnih, and Yee Whye Teh. 2017. {``The Concrete
Distribution: A Continuous Relaxation of Discrete Random Variables,''}
no. arXiv:1611.00712 (March).
\url{https://doi.org/10.48550/arXiv.1611.00712}.

\leavevmode\vadjust pre{\hypertarget{ref-Wang_Shen_2024}{}}%
Wang, Xiaoqi, and Han-Wei Shen. 2024. {``GNNInterpreter: A Probabilistic
Generative Model-Level Explanation for Graph Neural Networks,''} no.
arXiv:2209.07924 (February).
\url{https://doi.org/10.48550/arXiv.2209.07924}.

\leavevmode\vadjust pre{\hypertarget{ref-Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023}{}}%
Xiao, Guanghua, Shidan Wang, Ruichen Rong, Donghan Yang, Xinyi Zhang,
Xiaowei Zhan, Justin Bishop, et al. 2023. \emph{Deep Learning of Cell
Spatial Organizations Identifies Clinically Relevant Insights in Tissue
Images}. Preprint. In Review.
\url{https://doi.org/10.21203/rs.3.rs-2928838/v1}.

\leavevmode\vadjust pre{\hypertarget{ref-Yuan_Yu_Gui_Ji_2022}{}}%
Yuan, Hao, Haiyang Yu, Shurui Gui, and Shuiwang Ji. 2022.
{``Explainability in Graph Neural Networks: A Taxonomic Survey,''} no.
arXiv:2012.15445 (July).
\url{https://doi.org/10.48550/arXiv.2012.15445}.

\leavevmode\vadjust pre{\hypertarget{ref-Zhang_Liu_Wang_Lu_Lee_2021}{}}%
Zhang, Zaixi, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. 2021.
{``ProtGNN: Towards Self-Explaining Graph Neural Networks,''} no.
arXiv:2112.00911 (December).
\url{https://doi.org/10.48550/arXiv.2112.00911}.

\end{CSLReferences}



\end{document}
