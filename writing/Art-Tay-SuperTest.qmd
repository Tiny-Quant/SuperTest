---
title: "Qualifying Exam"
author: "Art Tay"
format:
  pdf:
     documentclass: article
     papersize: letter
     geometry:
         margin=1in
     fontsize: 11pt
     keep-tex: true
     include-in-header: header.tex
bibliography: references.bib
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
library(magrittr)
library(tidymodels)
library(kableExtra)


tidymodels_prefer()
```

# Introduction 

- Overview of the problem area.

    - Graphs are an important data structure. 
        - GRAPHS provide an incredibly flexible structure for modeling complex data. Data can naturally appear as graphs, like molecules. We can reduce data to a graph, such as the key points of a image. We can even use graphs to add structure, such as grammatical relationships.
    
    - GNN models are good at prediction and inference on graph data. 
        - Graph Neural Networks (GNNs) have become a popular choice for prediction and inference on graph data. At their core, GNNs work by iteratively updating node embeddings based on information from neighboring nodes. The idea is to use the graph's structure to engineer better features. This message passing scheme allows GNNs to capture complex dependencies and patterns present within the graph structure. GNN architectures typically consist of multiple layers, each performing message passing and aggregation operations to refine the embeddings. These layers are often followed by pooling and dense prediction layers to produce the final output.

    - There are many important applications for graph classification models. 
        - Some important applications of graph classification include predicting chemical toxicity [@bai2019unsupervised], classifying proteins [@gallicchio2019fast], and even detecting cancer from pathology slides [@Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023]. 
    
    - **Problem:** While GNNs achieve remarkable predictive power, their complexity prevents the exaction of the scientific rationale.

- Why is the problem important? 

    - Explaining or interpreting GNN predictions would 
        - help with the adoption of such models for critical applications, 
        - prevent adversarial attacks, 
        - detect potential implicit discrimination, 
        - guide scientific as well as machine learning research. 


- How does the problem relate to the fundamentals areas of Statistics? 

    - Explain-ability vs Interpretability  
        - @Yuan_Yu_Gui_Ji_2022 
        - A model is interpretable if the models decision process can be readily understood by humans. For example, a linear regression model is interpretable because the coefficient clearly define how any prediction get made. 
        - A model is explainable if the models prediction can be reasoned post-hoc. Permuting each variable and measuring the variation in the predictions can be used to estimate each variables marginal effect [cite]. 

    - One goal would be to create a GNN type model whose decision process is human interpretable. A straight translation from statistics would be a circuit type analysis [cite]. For graphs, this would mean some form of coefficients on subgraphs producing the prediction. 

    - Another goal might be to develope a method that determines if a feature is statistical significant to the GNN model. The challenge is that the graph features that matter to researchers aren't necessarily tabular.  

- What is the impact of solving this problem? 
    - In the application where GNNs have shown strong predictive power, we can exact a testable scientific hypothesis for the nature of the classification.  

    - In the application where GNNs have weak predictive power, highlight the potential misunderstandings the model is having. 

# Notation 

- Let $G$ denote a graph. 

- Any graph $G$ can be describe by $X, A, E$. The node feature
matrix, edge feature matrix, and adjacency matrix respectively 

- Let $X = [X_c, \ X_d]$, where $X_c$ is the subset of continuous node features and $X_d$ is the subset of one-hot discrete node features.  

- Let $E = [E_c, \ E_d]$, denoted in the same manner. 

- Let $n$ represent the number of nodes in the graph and $v$ represent the 
number of edges.  

- Let $\text{feat}_{(.)}$ denote the number of features or columns in the the corresponding feature matrix.  

- $A$ is a binary $n \times n$ matrix where $A[i, \ j] = 1$ indicates that an edge exists between nodes labeled $i$ and $j$.  

- Let $\text{explainee}(G; \ \Omega) = h^{(1)}_G, \dots h^{(L)}_G, \rho_G$ be an $L$ layer GNN model with parameters $\Omega$ that we would like to explain. 

- Let $\hat Y_G$ be the predicted class label for graph $G$ predicted from explainee(). 

- For any graph Let $\nu$ denote the set of node and $\mathcal{E}$ denote the set of edges. 

# Analysis of Core Papers

## GNNInterpreter 
[@Wang_Shen_2024]

- Overview
    - Instance v. Model Level 
        - In general, explanation methods serve to elucidate which
        features within the data influence disparate predictions. These
        methods typically fall into two categories: instance-level and
        model-level. Instance-level explanations aim to unveil the
        model’s rationale behind a particular prediction. In domains
        such as image and text analysis, a prevalent approach involves
        masking or perturbing the instance and assessing the
        impact on the model’s prediction. On the other hand, model-level
        explanations seek to understand how a model generally
        distinguishes between classes. In image and text analysis, for
        instance, one common technique involves treating the input as
        a trainable parameter and optimizing the model’s prediction
        towards a specific class. Consequently, the resulting optimized
        input comprises a set of features strongly associated with
        the targeted class.

    - GNNInterpreter provides model level explanations for GNN in this manner. 

    - Formally, GNNInterpreter tries to learn the graph generating distribution for each class. 

    - GNNInterpreter works by optimizing the parameters of a generic graph generating distribution to produce samples that closely match the explainee's understanding of the targeted class.

- Explanation of the graph generating distribution.  
    - Graph generating distributions are hard to specify because there can be discrete and continuous elements of $X$, $E$ and $A$. Furthermore, the interactions between these matrices can be complex.    

    - The authors tackle these issues by making two simplify assumptions.   
        1. Assume that $G$ is a *Gilbert* random graph, every possible edge as an independent fixed probability of occurring. 
            \begin{equation}
                \forall (i, \ j) \neq (k, l) \ Pr(A[i, \ j] = 1) \perp Pr(A[k, \ l] = 1)
            \end{equation}
        
        2. The features of every node and edge are independently distributed.   

    - The author justify these assumptions by: 
        1. The other graph distributions aren't suitable. 
            a. Erdo-Renyi graphs have a fixed number of edges (and nodes, but nodes are also fixed for Gilbert). 
            b. Rado graphs are infinite in size. 
            c. The random dot-product graph model is just a generalization of Gilbert random graphs.   

        2. Because the parameters of the independent distributions will be updated jointly using the *explainee* model, the *explainee's* understanding of the latent correlation structure should be contained in the final estimates.  

    - $X_c$ and $E_c$ can be sampled from any continuous distribution that can be expressed as a location-scale family. Separating the stochastic and systematic components is necessary for gradient based optimization. It is commonly known as the "re-parametrization trick". 

    - $X_d$, $E_d$ as well as $A$ need to be sampled from a continuous distribution for gradient based optimization, but the distribution has to have sampling properties close to a discrete distribution.  

    - The author assume that the true underlying distribution for every discrete node and edge feature is *categorical*. The categorical distribution is also know as the multi-bernoulli, where every sample has a fixed probability of being in one of the discrete categories. 

    - Suppose there are $D$ categories with probabilities 
    $\pi_\omega = \dfrac{\theta_\omega}{\sum_{i \in D}\theta_i}$.
    Then 
        \begin{equation}
            I = \underset{i \in D}{\text{argmax}} \ \log \theta_i + G^{(i)} 
                \sim \text{Cat}(\pi)
        \end{equation}
    where $G^{(i)} \overset{i.i.d.}{\sim} \text{Gumbel}(0, 1)$. 

    - The intuition is that the Gumbel or extreme value distribution is the density of the maximum order statistic of i.i.d. standard normals which makes it a good candidate for model the winning or maximum probability category. Adding Gumbel noise to the logits should maintain the true relative proportions, but enough skewness such that every category has some probability of having the maximum noised logit.  

    - **Proof 1:**
    In order for $I$ to be a true categorical distribution, 
    $Pr[I = \omega] = \pi_\omega$. $I = \omega$ if and only if
    $\log \theta_\omega  + G^{(\omega)} > \log \theta_i + G^{i} \ 
        \forall i \in D \setminus \omega$. Let $M_i$ denote a random variable 
        that follows a $\text{Gumbel}(\log \theta_i, 1)$ distribution. 
    \begin{align*}
        Pr[I = \omega] 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in D \setminus \omega} Pr(M_i < m_\omega) 
                \text{ i.i.d location shifted Gumbel distributions.} \\ 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in D \setminus \omega} \exp 
                    \left(-e^{\log \theta_i - m_\omega} \right)
                \text{ Gumbel CDF.} \\ 
            &= \mathams{E}_{M_\omega} 
                \exp \left(-\sum_{i \in D \setminus \omega}
                e^{\log \theta_i - m_\omega} \right) \\ 
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\omega - m_\omega \right) 
                \exp \left(-e^{\log \theta_\omega - m_\omega} \right) \cdot 
                \exp \left (-\sum_{i \in D \setminus \omega}
                e^{\log \theta_i - m_\omega} \right) \ dm \\
                &\text{ Gumbel PDF.} \\
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\omega - m_\omega \right) 
                \exp \left (-\sum_{i \in D}
                e^{\log \theta_i - m_\omega} \right) \ dm \\
            &= \int_{-\infty}^{\infty}
                \theta_\omega 
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in D}
                    \theta_i  \right) \ dm \\
            &= \pi_\omega \sum_{i \in D} \theta_i
            \int_{-\infty}^{\infty}
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in D}
                    \theta_i  \right) \ dm 
                    \text{ From the above definition of } \pi_\omega \\
            &= \pi_\omega \sum_{i \in D} \theta_i
                \dfrac{\exp\left( -e^{-m_\omega} \sum_{i \in D} \theta_i \right)
                    }{\sum_{i \in D} \theta_i} \bigg|_{-\infty}^\infty \\ 
            &= \pi_\omega \sum_{i \in D} \theta_i
                \dfrac{1}{\sum_{i \in D} \theta_i} = \pi_\omega
        \end{align*}
        Reference: @Huijben_Kool_Paulus_van_Sloun_2022 


    - Using inverse CDF sampling and and relaxing the argmax to a Softmax, we 
    can sample one-hot categorical vectors based on two parameters 
    $\theta_{\text{Cat}}$, a trainable parameter vector of length equal to 
    the number of categories, and $\tau$, a hyperparameter that controls 
    the degree of relaxation (smaller value approximate the discrete 
    sampling better, but can result in numerical issues). 
        \begin{equation}
            \text{Softmax}
            \left(
                \dfrac{\theta_{\text{Cat}} - log(-log \ \epsilon)}{\tau}
            \right), \quad \epsilon \sim U[0, 1]
        \end{equation}
    This method, known as the concrete distribution [@Maddison_Mnih_Teh_2017], yields a reasonable smooth gradient w.r.t. to the probability parameters. 

    - The adjacency matrix can be sampled in a similar manner since the Bernoulli is just a special case of the categorical.   
        \begin{equation}
            \text{sigmoid}
                \left(\dfrac{\theta_A + \log \ \epsilon - \log(1 - \log \ \epsilon)}{\tau} \right)
        \end{equation}
    This is known as the binary concrete distribution [@Maddison_Mnih_Teh_2017]. 

    - Notate the combined graph generating distribution as: 

        $$
        G_{\text{gen}} \sim \text{gen}(\Theta)  
        $$

        where $\Theta$ is the set of all parameters from the independently sampled distributions. 

- Prediction objective. 
    - An obvious objective is to maximize the likelihood that the *explainee* model predicts a sampled graph to be a member of the target class.

    - Let $\tilde{\rho}$ denote the desired predicted probability vector. Then the above objective can be expressed as: 

    \begin{equation}
        \mathcal{L}_\text{pred} (\Theta \ | \ G_\text{gen}) = \mathams{E}_{G_\text{gen}} \ \text{CrtEnt} (\text{explainee}(G_\text{gen}), \ \tilde{\rho})
    \end{equation}

- Embedding objective. 
    - While the above objective enforces a desirable property, it isn't restrictive enough to make the generated graph realistic. This is because the final prediction, $\rho_{G_{\text{gen}}}$ is compute using only final embeddings, $h^{(L)}_{G_{\text{gen}}}$. Normally $h^{(L)}_{G_{\text{gen}}}$ contains all the needed information from the graphs structure; however, the generation scheme allows the feature distribution to be optimized directly. This means that explanation can ignore the graph structure and optimize towards the desired final embeddings.

    - Another way of understanding the problems with the above objective is to consider the *out-of-distribution* (ood) issue. Since the above generation scheme is not restricted by the observed data distribution, the initial generated graphs may be very ood, but clearly on one side of the decision boundary.   

    - The author find that empirically GNN model exhibit a class preference.   
    ![](figures/random_baseline.png){fig-align="center"}
    [@Wang_Shen_2024]

    For example, random graphs have an average predicted probability of being Non-mutagenic in the MUTAG dataset if 93.2%. This demonstrates why the above objective is insufficient to generate realistic or *in-distribution* (id) graph. 

    - In order to mitigate this issue, the author proposed additional minimizing the cosine distance between the average embedding of all the observed graph from the targeted class, $\bar h^{(L)}_{G_c}$, and the embedding of the generated explanation. 

    \begin{equation}
       \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) = 
            \mathams{E}_{G_\text{gen}}
            \text{CosDist}\left( \bar h^{(L)}_{G_c}, \ h^{(L)}_{G_\text{gen}} \right)
    \end{equation}

- Regularization terms.   
    - Sparse graphs are easy for humans to interpret. To encourage sparsity the authors employed an $L_1$, $L_2$, and a budget penalty on the edge probabilities.

    \begin{equation}
        \mathcal{L}_{\text{Sparsity}}(\theta_A) = ||\theta_A||_1 + ||\theta_A||_2 + \text{softplus}(\text{sigmoid}||\theta_A||_1 - B)^2
    \end{equation}
    where $B$ is the expected maximum number of edge for generated explanation graphs.

    - Connectivity is another desirable property as it ensures a cohesive explanation.
    To encourage connectivity the author minimize the *KL-Divergence* between edge probabilities that share a common node.

    \begin{equation}
        \mathcal{L}_{\text{Connect}}(\theta_A) = \sum_{i \in \nu} \sum_{j, k \in \mathcal{E}(i)} D_{KL}(\text{sigmoid}(\theta_A[i, \ j]) \ || \ \text{sigmoid}(\theta_A[i, \ k]))
    \end{equation}

    where $\mathcal{E}(i)$ is the set of edges that connect to node $i$.

- Summary of Results + Figures
    - The final generator model is trained by sampling $G_\text{gen} \sim \text{gen}(\Theta)$ and then iterative updated $\Theta$ via gradient descent on the full loss:
    \begin{equation}
        \begin{split}
            \mathcal{L}_{\text{GNNInterpreter}}(\Theta \ | \ G_\text{gen}) = 
            &\mathcal{L}_{\text{pred}}(\Theta \ | \ G_\text{gen}) + 
            \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) +  \\
            &\mathcal{L}_{\text{sparsity}}(\Theta \ | \ G_\text{gen}) +
            \mathcal{L}_{\text{connect}}(\Theta \ | \ G_\text{gen}) 
        \end{split}
    \end{equation}

    ![](figures/GNNInt_prediction_results.png){fig-align="center"}

    - GNNInterpreter achieve remarkable accuracy on most target classes. Many of the interval are tight and very close to 1, which implies that the examples generated are almost always classified as the targeted class. The explanations for the house motif and the lollipop shape are worse in terms of predictions. Although the author critique the use of predictions as the sole objective, they do not use any other quantitative metric to evaluate the validity of their explanations. 

    ![](figures/GNNInt_drawn_results.png){fig-align="center"}
    - Qualitatively we can see some limitation in terms of realism. 
    
    - For example, for the mutagen class, the explanation correctly identifies the importance of the N02 group; however, the generated graph isn't realistic and might not even be chemically possible. Furthermore, the non-mutagen example doesn't display any clear patterns or identifiable structures.   

    - The explanations for the Cyclicity dataset as well as the Wheel and Grid classes do not appear to be members of their respective underlying data distributions. 

    - GNNInterpreter provides a way of generating example graph that would be classified as a target class by a GNN model, without needing to specify domain specific rules. On the other hand, optimizing predictions and even embeddings does not appear to be a sufficient objective for producing in-distribution graphs. The author correctly point out that optimizing predictions can lead to unrealistic graphs, but they have also inadvertently demonstrated that optimizing embeddings is not necessarily sufficient either.       

## D4Explainer
[@Chen_Wu_Gupta_Ying_2023]

- Overview

    - D4Explainer or in-Distribution GNN explanations via Discrete Denoising Diffusion attempt to directly address the realism of generated graphs in model-level explanation by using the observed data to train a Diffusion model. 
    - In the image domain, diffusion model have been shown to produce the most realistic images when compared to other generative AI methods such as Generative Adversarial Networks (GANs). 
    - Diffusion model work by iteratively noising an observation until it is pure noise. Then a denoising model is trained to predict the noise added at any given time step. Then new observations can be generated by passing pure noise through the diffusion model in a process known as reverse sampling. 
    - Additional label information can be passed to generate observations with similar a label. 

- Forward Diffusion  
    
    - The authors here are focused on discrete structural diffusion. D4Explainer generate example graph by noising and de-nosing the adjacency matrices of observed graphs. The sampled graphs have the same features, but different structures. 

    - The process of gradually adding noise to the input data is called forward diffusion. During forward diffusion, random noise is added iteratively until the data becomes pure noise in the final iteration. This ensures that the denoising model can start with pure noise. Forward diffusion is usually a Markov process.

    - If we assume that the observed graphs are Gilbert random graphs, like GNNInterpreter, pure noise would mean that $\forall \ (i, \ j) \ A[i, j] \sim \text{Bernoulli(0.5)}$. 

    - Let $t \in [0, T]$ denote the current iteration. Let $\beta_t$ be the common probability that any edge changes state at time step $t$. 
    $(\beta_1, \dots, \beta_T)$ is known as the variance schedule and is a set hyperparameter. Let $A_t$ be a one-hot encoded version of the $t^{th}$ noised adjacency, $[1, 0]$ is the edge exists and $[0, 1]$ otherwise. Then the forward diffusion process can be expressed as:  
    \begin{equation}
         A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{t-1}[i, \ j]) 
            = \text{Cat}(A_{t-1}[i, \ j] \cdot Q_t)
    \end{equation}
    or 
    \begin{equation}
         A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{0}[i, \ j]) 
            = \text{Cat}\left(A_0[i, \ j] \prod_{i=1}^t  Q_i \right)
    \end{equation}
    where, 
    $$
    Q_t = 
    \left[
    \begin{matrix}
        1-\beta_t & \beta_t \\ 
        \beta_t & 1 - \beta_t
    \end{matrix}
    \right]
    $$
    the $t^{th}$ element-wise transition matrix. 

    - **Proof 2:** 
    $\lim_{t \to \infty} q(A_t[i, \ j] \ | \ A_{t-1}[i, \ j]) 
        \overset{D}{\to}$ Bernoulli(0.5) (converges in distribution). 
    \begin{align*}
         \lim_{t \to \infty} q(A_t[i, \ j] \ | \ A_{t-1}[i, \ j]) 
            &= \lim_{t \to \infty} \text{Cat}\left(A_0[i, \ j] \prod_{i=1}^t Q_i \right) \\
            &= \lim_{t \to \infty} \text{Cat}
            \left(A_0[i, \ j] 
                \prod_{i=1}^t  
                \left[
                \begin{matrix}
                    1  & -1 \\
                    1 & 1
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    1  &  0 \\
                    0 & 1-2\beta_i
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    0.5  &  0.5 \\
                    -0.5 & 0.5
                \end{matrix}
                \right]
            \right) \\ 
            &\text{Eigen decomposition.} \\ 
            &= \lim_{t \to \infty} \text{Cat}
            \left(A_0[i, \ j] 
                \left[
                \begin{matrix}
                    1  & -1 \\
                    1 & 1
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    1  &  0 \\
                    0 & \prod_{i=1}^t 1-2\beta_i
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    0.5  &  0.5 \\
                    -0.5 & 0.5
                \end{matrix}
                \right]
            \right) \\ 
            &= \text{Cat}
            \left(A_0[i, \ j] 
                \left[
                \begin{matrix}
                    1  & -1 \\
                    1 & 1
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    1  &  0 \\
                    0 & 0
                \end{matrix}
                \right]
                \left[
                \begin{matrix}
                    0.5  &  0.5 \\
                    -0.5 & 0.5
                \end{matrix}
                \right]
            \right) \\ 
            &\text{Set }\beta < 0.5. \\
            &= \text{Cat}([0.5, 0.5]) = \text{Bernoulli}(0.5)
    \end{align*}

- Backward Diffusion (Denoising Model)

    - A denoising model either predicts the error that was added or the value of the original observation. Even though only the adjacency matrix is being noised, we still want to include all available data. Thus the denoising model is parameterized as: 
    \begin{equation}
         p(A_0 \ | \ A_t, t, X_0, E_0; \ \Omega)  
    \end{equation}
    where $\Omega$ is the set of trainable parameters.

    - The author employed a Provably Power Graph Network (PPGN) [cite] as their denoising model; however, they have added an additional neural network to learn the time or noise level effect.      

- Loss Dist

    - Like most diffusion models, the primary objective is to minimize the distance between the predicted de-noised observation and the original. The author have added an additional weight term to focus the model on noisier or more difficult training examples. 
    \begin{equation}
        \mathcal{L}_{dist} (\Omega \ | \ A_0) =  \sum_{t=1}^T 
            \left(1 - 2 \bar \beta_t + \dfrac 1 T \right)
            \mathams{E}_{\hat A_0}
            \text{CrtEnt} \left(
                A_0, \hat A_0
            \right)
    \end{equation}
    where $\hat A_0 = p(A_0 \ | \ A_t, t, X_0, E_0; \ \hat \Omega)$, 
    $A_t[i, j] \sim q(A_t[i, \ j] \ | \ A_{0}[i, \ j])$, and 
    $$
        \bar \beta_t = \frac 1 2 - \frac 1 2 \prod^t_{i=1}(1-2\beta_i)
    $$ 
    the cumulative transition probability.

- Model-level sampling algorithm 

    - Use a set of observed graph features, D4Explainer generates model level explanations by sequentially denoising pure noise to generate an adjacency structure that has a high probability of being a member of the targeted class. At each step $A_t$ is denoised to $k$ candidates $A_0, k$. The candidate with the best predicted probability is then noised to a level of $t-1$ and the process is repeated. The pseudocode is reproduced below.  

    \begin{algorithm}
    \caption{D4Explainer Model-level Explanation Reverse Sampling Algorithm}\label{alg:cap}
    \begin{algorithmic}
        \Require $\hat \Omega$: trained denoising parameters; 
                $q(A_t \ | \ A_0)$: forward diffusion process.
        \renewcommand{\algorithmicrequire}{\textbf{Input:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \Require N: maximum number of nodes; T: maximum noise level; 
                K: number of candidates per iteration; 
                $\tilde{\rho}$ targeted prediction vector; 
                $(X, E)$: node and edge features.  
        \Ensure $\hat A$: adjacency matrix for model-level explanation.
        \State Sample $A_T[1:n, 1:n]$ \sim Bernoulli(0.5)
        \For{t in T to 1}
            \State Sample candidates 
                $\{\hat A_{0, k} \sim p(A_t,t, X, E; \ \hat \Omega) : k \in 1, \dots, K\}$
            \State Select the best candidate 
            $\underset{j \ \in \ i, \dots K}{\text{argmin}}$ 
            CrtEnt(explainee($G = (X, A_{0, j}, E)), \ \tilde{\rho}$)
            \State Sample $A_{t-1}[1:n, 1:n] \sim q(A_{t-1}, A_{0, j})$
        \EndFor
        \State \Return $A_0$
    \end{algorithmic}
    \end{algorithm}

- In-Distribution metrics. 

    - To access how well a sampled explanation matches the observed graph distribution, the author compute various maximum mean discrepancy (mmd) statistics. 

    - MMD is a general technique to compute the distance between two distributions of data using only observed data. The distributions are estimated using a kernel an then the distance between there means are compared. 

    - A common way to approximate a density is to use the average of Gaussian distributions centered at each observation: 
    \begin{align*}
        f(x)_X 
            &\approx \dfrac 1 n \dfrac{1}{\sqrt{2\pi}\sigma} 
            \sum_{i = 1}^n e^{\dfrac{-(x - x_i)^2}{2\sigma^2}} \\
            &\approx \dfrac 1 n \dfrac{1}{\sqrt{2\pi}\sigma} 
            \sum_{i = 1}^n \phi(x)^T \phi(x_i) 
    \end{align*}
    where, $k(x_i, x_j) = \phi^T(x_i)^T \phi(x_j)$ is the Gaussian kernel function. As long as the chosen kernel is *characteristic* the kernel mean embedding
    $$
        \dfrac 1 n \sum_{i = 1}^n \phi(x_i)^T = 
            \dfrac{1}{n} \phi(\vec{x}) \mathbb{1}
    $$
    is representative of the distribution. Here $\vec x$ is the vector of observations and $\mathbb{1}$ is the appropriate dimensional vector of 1s. 

    - MMD statistics are then simple the l2 distance between the kernel mean embeddings:
    \begin{align*}
         MMD(\vec x, \vec y) 
            &= 
            || 
            \dfrac{1}{n} \phi(\vec{x}) \mathbb{1} - 
            \dfrac{1}{m} \phi(\vec{y}) \mathbb{1}
            ||^2_2 \\
            &= 
            \left(
                \dfrac{1}{n} \phi(\vec{x}) \mathbb{1} -
                \dfrac{1}{m} \phi(\vec{y}) \mathbb{1}
            \right)^T
            \left(
                (\dfrac{1}{n} \phi(\vec{x}) \mathbb{1} -
                \dfrac{1}{m} \phi(\vec{y}) \mathbb{1}
            \right)^T \\
            &= 
            \dfrac{1}{n^2} \phi(\vec{x})^T \phi(\vec{x}) + 
            \dfrac{1}{m^2} \phi(\vec{y})^T \phi(\vec{y}) - 
            \dfrac{2}{nm} \phi(\vec{x})^T \phi(\vec{y})
    \end{align*}

    - Intuitively $\phi(\vec{x})^T \phi(\vec{x})$ and 
    $\phi(\vec{y})^T \phi(\vec{y})$ can be thought of as the distance within the observations of $x$ and $y$ and $\phi(\vec{x})^T \phi(\vec{y})$ as the distance between. Therefore an MMD statistic close to zero means that the distribution are approximately the same.  

    - The author use MMD statistics to compare the degree, clustering, and spectrum distributions of the generated explanation against the observed data. The degree distribution of a graph indicates how frequently nodes have different numbers of connections, reflecting the overall connectivity pattern. The clustering coefficient of a node measures the proportion of the node's neighbors that are also connected to each other, representing local clustering. The spectrum distribution, which is the distribution of eigenvalues of the adjacency matrix or Laplacian matrix, provides insights into the graph’s structural characteristics and dynamic properties.

    - Like GNNInterpreter, the author of D4Explainer also value the sparsity of their generate example. They measure the density of a graph as the number of present edges divided by the number of possible edges or 
    \begin{equation}
        \text{Density} = \mathcal{|E|} / |\nu|^2
    \end{equation}

- Loss CF

    - In additional to model-level explanations, D4Explainer can provide *counterfactual explanation* be include an additional term in the loss function. The authors define a counterfactual explanation for a particular observed graph to be $G^c$ such that $\hat Y_{G^c} \neq \hat Y_G$, but the difference between $G^c$ and $G$ is minimal. 

    - $\mathcal{L}_{dist}$ ensures that the $G^c$ generated won't deviate too far from the observed graph distribution.   

    - Adding $\mathcal{L}_{CF}$ minimize the probability that the generated graph is classified the same as the input graph. 
    \begin{equation}
         \mathcal{L}_{CF}(\Omega \ | \ A_0) = 
            \mathams{E}_{\hat A_0} - \log(1 - \rho_{G^c}[\hat Y_{G}])
    \end{equation} 
    where $G = (X, A, E)$ is an observed graph and $G^c = (X, \hat A_0, E)$.

    - To quantify the quality of generated counterfactual explanations, the author report *Counterfactual Accuracy*, *Fidelity*, and modification ratio. 

        - CF-ACC measure the proportion of $G^c$s that have different predicted labels than there associated observed graph.  
        \begin{equation}
             \text{CF-ACC} = \mathams{E}_{A_t} I(\hat Y_{G^c} \neq \hat Y_{G}) 
        \end{equation}

        - Fidelity measures the difference in the predicted probability of $G$ and $G^c$ with respected to the original class label.  
        \begin{equation}
            \text{Fidelity} = \mathams{E}_{\hat A_0} \rho_{G}[\hat Y_G] - \rho_{G^c}[\hat Y_G]
        \end{equation}

        - Modification ratio measures difference in edges between $G^c$ and $G$ relative to the size of the original graph.  
        \begin{equation}
            \text{MR} = \dfrac{|\sum_{i, j} A[i, j] - \hat A_0(i, j)|}{\sum_{i, j} A[i, j]}
        \end{equation}

- Summary of Results + Figures

    - Similar to GNNInterpreter, the author report the mean predicted probability for the targeted class. Using a maximum of 6 nodes, the author were able to achieve a mean of 0.832 on the MUTAG dataset. 

    - Since the counterfactual explanation are generated on a per observation basis, the author's compared their method to other popular instance-level explanation methods.     

    ![](figures/D4-CF-Plot.png)
    The above plot displays that D4Explainer can flip the prediction on ~80% of observed graphs at most modification levels. Missing from the plot is a variance measure. It is possible that the counterfactual accuracy varies more between different data splits or hyperparameter than the other methods.

    ![](figures/D4-MMD-Table.png)
    The above table indicates that the generated graph tend to have feature distribution that are close to the original dataset; however, there might be a hidden class bias. The distributions might only be close for the majority class, skewing the averages.      

## ProtGNN
[@Zhang_Liu_Wang_Lu_Lee_2021]

- Overview: 

    - Up until this point we have been describing post-hoc explanation methods. 

    - @Rudin_2019 raises several important concerns with this paradigm. 

        1. Parity in the outcomes does not imply parity in the reasoning. Explanation may bear strong prediction, but for distinct and sometimes nonsensical reasons. Recall that that completely random graphs can illicit confident predictions. 

        2. Feature importance does not indicate how the model is using the feature, just that it is important. For example, a substructure might lead to a prediction because of a strong correlation with the class, but the substructure could also be important because of a strong negative correlation with the other classes.       

    - ProtGNN use the prototype learning paradigm to create a GNN model that is *interpretable*.  

- Prototype Learning:

    - Prototype learning models make predictions by comparing new inputs to exemplar cases learned during training. 

    - This results in an interpretable model because as long as the prediction function applied to the prototype scores in simple, the reasoning for any given prediction will be clear.   

    - Intuitively, this can be thought of as using a black-box model to engineer features, which are then passed to a simple white-box model.    

- ProtGNN Architecture:  

    - ProtGNN initialize $m$ prototype vectors for each of the $c$ classes. Let $p_k$ denote a prototype vector and $P_{Y_G}$ denote the set of prototype vectors assigned to prediction class $Y_G$. 

    - Let $f(G; \ \Omega_f) = \hat p_G \in \mathbb{R}^p$ be a graph encoder function that maps any graph into the prototype vector space, where $\Omega_f$ is the set of trainable parameters.  

    - Prediction Loss: 
        \begin{equation}
            \mathcal{L}_{\text{prot}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mc} \ | \ G)= \mathams{E}_G \ \text{CrsEnt} \left[\psi \left(sim\left(\hat p_G, p_1\right), \dots, sim\left(\hat p_G, p_{mc}\right); \ \Omega_\psi\right), \ Y_G \right]
        \end{equation}
        where $\psi$ is any simple prediction model with trainable parameters $\Omega_\psi$ and $sim$ is any vector similarity function. ProtGNN uses a full connected layer with a softmax output activation for $\psi$, which is similar to a multinomial regression model. 

    - To address similar out-of-distribution concerns discussed above, the author include 3 additional loss terms to constraints. 

    - Cluster Loss: Each embedding should be close to at least one prototype assigned to its ground truth class.  
        \begin{equation}
            \mathcal{L}_{\text{clst}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mc} \ | \ G) = \mathams{E}_G \ \underset{p_k \in P_{Y_G}}{\text{min}} || \hat p_G - p_k ||^2_2
        \end{equation}

    - Separation Loss: Embeddings should be far away from the prototypes of other classes. 
        \begin{equation}
            \mathcal{L}_{\text{sep}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mc} \ | \ G) = - \mathams{E}_G \ \underset{p_k \notin P_{Y_G}}{\text{min}} || \hat p_G - p_k ||^2_2
        \end{equation}

    - Diversity Loss: The prototypes within a class should learn different information.  
        \begin{equation}
             \mathcal{L}_{\text{div}}(\Omega_f, \ \Omega_\psi, \ p_1, \dots, p_{mc} \ | \ G) = \sum_{k=1}^C \sum_{p_i \neq p_j \in P_k} \ \max(0, cos(p_i, p_j) - s_\text{max})
        \end{equation}
        where $s_\text{max}$ is a similarity threshold. 

- Prototype Projection Using MCTS:

    - The learned prototypes are latent embedding vectors which are not directly interpretable. To address this issue, the authors employ a Monte Carlo Tree Search (MCTS) algorithm to find the subgraph within the observed graphs of the prototype's class that has an embedding closest to a given prototype. 

    - Specifically the author using MCTS to select an optimal sequence of pruning actions. 

    - Let $\mathcal{G}_{0, \ i, \dots, j}$ represent the subgraph resulting from the sequence of pruning actions $\text{act}_i,\dots, \text{act}_j$, where $\mathcal{G}_0$, the root of the search tree, is the observed graph. 

    - The MCTS algorithm works in 4 steps:  

        1. An existing node in the search tree $\mathcal{G}_{0, \ i, \dots, j}$ is *selected*. 

        2. $\mathcal{G}_{0, \ i, \dots, j}$ is *expanded* adding a child node $\mathcal{G}_{0, \ i, \dots, j, k}$ resulting from action $\text{act}_k$. To limit the search space, the author restrict $\text{act}_k$ to be the removal a peripheral node with minimum degree. 

        3. Then the reward for the sequence of actions $i, \dots, j, k$ is estimated via a Monte Carlo *simulation* where random actions are taken until a subgraph of the desired size is produced. The subgraph embeddings are then compared to the prototype vector and the similarities are averaged.     

        4. The child nodes estimated score is then *backpropagated* to update the estimated scores of all its parent nodes. 

    - After a fixed set of iterations, the decision path the highest resulting score is selected to produce the prototype projection. This process is repeated across all observed graphs in the prototype's class.   

- Conditional Subgraph Sampling:

    - In addition to finding the closest subgraph in the observed class, the authors also suggest a method for efficient finding similar subgraph in each input.  

    - This is done by training a neural network to predict an edge mask, conditional on a prototype vector. The resulting induced subgraph's embeddings are optimized to be also close as possible to the given prototype, thus producing a similar observed subgraph per learned prototype. 
    \begin{equation}
        \hat A[i, \ j] = \text{nn}\left(G, p_k; \ \Omega_{\text{nn}}\right)
    \end{equation}

    \begin{equation}
        \mathcal{L}(\Omega_{\text{nn}}, \ \Omega_f, \ p_k \ | \ G) = -\mathams{E}_G \ sim(\hat p_G, p_k) 
    \end{equation}

    - The authors denote the version of the model that utilizes this module, in addition to the above losses, as *ProtGNN+*.

- Summary of Results + Figures 

    - ProtGNN achieves similar or better graph classification accuracy than state of the art models across 5 standard benchmark datasets.   
    ![](figures/prot_acc_table.png)

    - ProtGNN is also robust to the choice of embedding model as the author have shown benefits across various GNN architectures. 

    - The author note that the computational complexity of ProtGNN can be significantly higher than a standard GNN model taking roughly 5 times longer to train. Most of this complexity is attributable to the use of the MCTS.   

    - On the other hand ProtGNN produces much clearer results. Analogous to a regression model, the prototypes can be thought of as features, the similar subgraphs and similarity scores as the observed feature values, and the class connections as the marginal effects on the prediction.         
    ![](figures/prot_inference_ex.png)

    - Since prototypes from every class are used for the classification, ProtGNN can also be used for counterfactual type analysis. For any given predictions, researcher can determine why a certain classification was made and why the other classes were not selected. 

# Synthesis of Core Papers

- GNNInterpreter and D4Explainer are both *post-hoc generative explanation method*. 

- Each defines a model-level explanation for a given class $c$ as an estimated graph generating distribution $G_\text{Gen, c} \sim f(G \ | \ \Theta)$ such that $\underset{\Theta}{\text{argmin}}\ \mathams{E}_{G_\text{Gen}} \ \text{CrsEnt}(\text{explainee}(G_\text{Gen}, \ \tilde{\rho_c})$. In other words, the distribution that maximize the probability that a sampled graph is classified by the explainee as the desired class.  

- The key difference between the methods is the form of $f(G \ | \ \Theta)$. This is akin to the parametric (distributional) assumption. 

- GNNInterpreter assumes that each graph matrix has a separate distribution with possibly dependent parameters. On the other hand, D4Explainer assumes that the only random variation between graphs is in the adjacency structure.   

- Both models assume that every entry of the adjacency matrix follows a Bernoulli distribution, but diverge in terms of estimating the probability. D4Explainer makes use of the observed graphs directly, whereas GNNInterpreter only injects latent embeddings. This makes GNNInterpreter much more sensitive to the underlying explainee model, which could be both a good and bad property.       

- Both sets of authors highlight the potential *out-of-distribution* problem that plagues generative explanations. GNN models are not robust to out-of-distribution or unrealistic graph, which is to say that the model can still return a confident classification for a complete random graph. 

- D4Explainer partial resolves this issue by restricting generated node and edge features to the observed data. This ensure that the feature values of any generated graph will not be out-of-distribution; however, this limits inference.   

- GNNInterpreter attempts to resolve this issue by imposing similarity to the average class embedding; however, embeddings can also suffer from out-of-distribution problems. The same way an unrealistic or random graph can have similarity predictions, it could also have similar embeddings because of the information loss in the lower dimensional space.  

- While GNNInterpreter and D4Explainer place differing restrictions on their respective likelihoods, they should be interchangeable. For example, there is no reason why $\mathcal{L}_\text{spars}$ couldn't be included in the loss for D4Explainer. 

- The MMD statistic methods could be applied to graph generated by GNNInterpreter.  

- ProtGNN in contrast to the first two methods, is a *self interpretable GNN model*. ProtGNN isn't attempting to decipher a predictive model, it is a predictive model.  

- The prototype similarity is closely related to GNNInterpreter's $\mathcal{L}_\text{embed}$ as they both measure the similarity between graph in a latent vector space. Unlike GNNInterpreter, the embedding function in ProtGNN is trainable. Furthermore, ProtGNN also uses dissimilarity to the non-target classes, but this could be easily added to GNNInterpreter's loss.  

- On the other hand, similar to D4Explainer, the graph created from the prototype is restricted to the observed data.    

- The similar subgraphs are instance level; however, the class connection weights and prototype projections are model-level.     

- D4Explainer produces counterfactual examples be directly compute possible input changes that flip the prediction, while ProtGNN demonstrates how the prediction could have changed by display the similarity to structures indicative of other classes. While not the traditional directly counterfactual analysis, ProtGNN provides model-level insight.   

- Results + Big Picture in the Literature

# Technical Details 

## Methodology
- Since GNNInterpreter only makes qualitative assessments of generated structures, we applied the metrics describe in D4Explainer to quantify the properties of the generating distribution. 

- We reimplemented GNNInterpreter using an identical generation scheme and loss function. 

- We trained the reimplemented model on the MUTAG dataset which consists of nitroaromatic compounds, with the goal being to predict their mutagenicity on Salmonella typhimurium [cite]. Each observation is a graphs representing a chemical structure, where vertices denote atoms and edges represent bonds [cite]. MUTAG is a standard benchmark graph classification dataset. 

- For our explainee model, we also implemented a 3-layer GCN model. There are slight variations between the architectures and training hyperparameter, but the accuracies are comparable, 86% (ours) and 92% (theirs). 

- We additionally reimplement D4Explainer's counterfactual generator. Although not discussed in the original paper, this model can also be used to produce model level explanations. We simply randomly sample a graph from the opposite class and take the generated counter factual graph as our explanation.   

## Results
```{r}
#| echo: false

table_1 <- data.frame(
    # GNNInterpreter Reported
    class_0 = c("1.0 +/- 0.0", NA, NA, NA, NA), 
    class_1 = c("1.0 +/- 0.0", NA , NA, NA, NA), 
    # GNNInterpreter Reimplemented 
    class_0 = c("0.93 +/- 0.01", "0.38 +/- 0.002", "1.77", "1.53", "0.07"),
    class_1 = c( "0.98 +/- 0.01", "0.32 +/- 0.003", "1.41", "1.43", "0.04"), 
    D4Explainer = c("0.92", "0.315", "0.12", "0.00", "0.02"), 
    # D4Explainer Reimplemented
    class_0 = c("0.63 +/- 0.23", "0.13 +/- 0.03", "0.27", "0.57", "0.05"), 
    class_1 = c("0.66 +/- 0.20", "0.13 +/- 0.03", "0.08", "0.25", "0.04"), 
    check.names = F
) |> t()

colnames(table_1) <- c("Predictions", "Density", "Deg.", "Clus.", "Spec.")
rownames(table_1) <- c("Class 0", "Class 1", "Class 0", "Class 1", 
                       "Aggregated", "Class 0", "Class 1")

table_1 |> kbl(format = "latex", booktabs = T,
     longtable = T, linesep = "", caption = "") |>
     pack_rows(index = c("GNNInterpreter Original" = 2, 
                         "GNNInterpreter Reimplemented" = 2, 
                         "D4Explainer Original" = 1, 
                         "D4Explainer Reimplemented" = 2
                         )) |>
    footnote(number = c("+/- 1 standard deviation; 1000 graphs."), 
             symbol = c("Empty"))
```

![(a) Observed graph from class 0, Pr = 0.92; (b) Generated counterfactual explanation of (a) from D4Explainer reimplemented, Pr = 0.67; (c) Observed graph from class 1, Pr = 0.96; (d) Generated counterfactual explanation of (b) from D4Explainer reimplemented, Pr = 0.96; (e) Generated explanation for class 0 from GNNInterpreter reimplemented, Pr = 0.94; (f) Generated explanation for class 1 from GNNInterpreter reimplemented, Pr = 1.0; (g) Generated explanation for class 0 from GNNInterpreter (Wang and Shen 2024, fig. 1, pg.7), Pr = 1.0; (h) Generated explanation for class 1 from GNNInterpreter (Wang and Shen 2024, fig. 1, pg.7), Pr = 1.0.](figures/mutag_plot_together.png){fig-align="center" height="80%" width="80%"}

- We were unable to achieve 0 variance predictive accuracy, but again the values are comparable (see table ?) given the lack of extensive hyperparameter tuning. The prediction are similarity superior to that of D4Explainer (see table ?). 

- As noted by the authors, the explainee model has a bias towards the mutagenic class (class 0), which can explain the greater deviation between the class 0 models. 

- Another limiting factor is a lower set maximum number of nodes. We set it to 8 to make the results more comparable to the model-level explanation reported by D4Explainer.      

- Surprisingly D4Explainer produces sparser graphs, in terms of density, than GNNInterpreter, despite no explicit penalty in the loss function.   

- Based on the MMD statistics, the D4Explainer produces graph with better in-distribution properties. This might be the result of the explicit regularization in GNNInterpreter. Penalize the edge probabilities should result in a different degree distribution. The clustering coefficient might also be effect since fewer edges result in fewer triangle clusters.  

- The spectrum distributions are close for both model, since both model enforce similarity in the explainee embedding space which is similar to the nature of the spectrum distribution. 

- On the other hand, an argument could be made that a similar degree distribution and clustering coefficient isn't actually desirable. For larger or more densely connected graphs, lower MMD statistics might results in explanations that are too complex to be useful. 

- Similar to the original examples display in GNNInterpreter, the generated non-mutagenic graphs do not have clear or realistic structures. Graph (f) highlights that nitrogen in key. We believe that this result is consistent with the original findings about the N02 groups, but because of the more restrictive node limit, our reproduction focuses on nitrogen since multiple oxygens can be found in non-mutagenic chemicals. 

- The reimplement D4Explainer has a CF-ACC in line with the original paper, 
76.32% at 0.41 log MR. 

- The reimplement D4Explainer produces the least dense graphs, but has the least discriminative predictions. The latter makes sense, since the goal of the counterfactual loss is to flip the prediction, not necessarily maximize it. 

- One the other hand, this model demonstrates that it is possible to generate sparse predictions and maintain a similar degree distribution and clustering.   

- Visually the counterfactual explanation make sense. Graph (b) eliminated the red or chlorine atom which is typically associated with non-mutagenic compounds [XGNN]. Graph (d) 
corrupted 3 of the 4 N02 groups, which is in line with GNNInterpreter's findings that multiple N02 groups is a strong indicator of mutagenicity. 

- This results lead to two main conclusions. 
    - First, the generator model should make use of the graph level observations. GNNInterpreter only use information about the graph structure through the explainee, never on the raw scale. 

        - This lack of graph level information result can result in unrealistic distributional properties. 

        - This could be done by using the observed graphs to predict the distributional parameters. 

        - Alternatively, the generator could be replaced with a diffusion model without any need to modify the loss.    

    - Second, the generator model should use class or prediction information. 

        - D4Explainer's model-level sampling algorithm is less efficient and less accuracy than GNNInterpreter's direct optimization during training. 

        - The reimplemented sampling algorithm demonstrates that direct denoising can still maintain low MMD statistics. 

        - A possible modification would be to add the $\mathcal{L}_\text{pred}$ and $\mathcal{L}_\text{embed}$ from GNNInterpreter to the loss. 

        - Alternatively, one could embedding the class label, similar to the way time is embedded, in order to establish a conditional diffusion model, which has worked extremely well in other applications. 

        - Additionally, it would be easy to explore diffusing the node and edge features as well as like *DiGress* [cite]. 

# Future Directions 

## Graph Edit Distance

- GNNInterpreter generates graphs with predictions very close to the target class, but lack good in-distributions properties. 

- D4Explainer generates explanations with more similar features distributions to the observed data; however, the predicted probabilities show more variance. Furthermore, while the model parameters of GNNInterpreter's generator can be interpreted, D4Explainer introduces another black-box model.     

- We propose using a differentiable approximation to the graph edit distance as a way of generically enforcing structural consistency. 

- GED measures the similarity between two graphs by counting the minimum number of edits required to make the graphs isomorphic (or subgraph isomorphic). Similar to the modification ratio for counterfactual explanations. 

- This has the key advantage of being human interpretable. For example, the percentage of atoms and bonds that need to be changed for two molecules to match is more relatable than the cosine distance between their embeddings. 

- Finding the exact optimal GED is a known NP-complete problem, and most solutions are not differentiable [@Bougleux_Brun_Carletti_Foggia_Gaüzère_Vento_2015]. 

- Fortunately, approximating GED as a quadratic assignment
problem (QAP) solves both the time complexity and derivative issue [@wang2024pygm]. 

Let $G_1$ and $G_2$ be graphs we wish to compute the GED between. QAP approximates this be solving the following optimization problem: 
\begin{equation}
    \mathcal{L}_{\text{GED}} = \underset{M}{\max} \ \text{vec}(M) \ K(G_1, G_2) \ \text{vec}(M)^T
\end{equation} 
here $M$ is a $(n_1 \times n_2)$ binary node matching matrix and $K$ is a $(n_1 n_2 \times n_1 n_2)$ quadratic affinity matrix that encode both node and edge similarity information. The efficiency of QAP comes from the fact that when any two nodes are matched together, the above objective will account for the possible edges that could connect the nodes. In order to compute GED, each entry of $K$ should range from [-1, 0], where -1 signals the need for a complete edit and 0 means no edits are required [@wang2024pygm]. We use the following similarity metric for every pair of nodes and edges: 
\begin{equation}
    \begin{split}
         &-0.25 \left[ 1 - \phi(\text{Continuous Features}) \right] \ + \\
         &-0.5 \left[1 - \phi(\text{Discrete Features}) \right]
    \end{split}
\end{equation} 
where $\phi(.)$ is the normalized cosine distance. $1 - \phi(.)$ will return 2 is the vectors are in opposite directions, 1 if the vectors are orthogonal, and 0 if they are pointing in the same direction; however, one-hot encoded vectors will never be in opposite directions, so the different types of feature must be weighted correctly. Computing affinity this way allows for $K$ to be differentiable w.r.t. generated graph features. Furthermore we use the reweighted random walks algorithm [@Cho_Lee_Lee_2010] to solve the above objective, which generates an M that is differentiable w.r.t. to K.

- We conducted a simulation study to compare graph level and latent similarity, or 
$\mathcal{L}_{\text{GED}}$ and $\mathcal{L}_{\text{embed}}$. 

- We generated 4 clusters of graph data. Every graph had one continuous edge feature sampled from a standard normal, one discrete node and edge feature sampled from a Bernoulli, and an adjacency matrix also Bernoulli. The only variation between the clusters was the maximum number of nodes as well as the mean of the continuous node feature distribution. Cluster 0 had 75-100 nodes, cluster 1 had 50-75 nodes, and cluster 2 as well as 3 had 10-35 nodes. This was designed to mimic a classic binary graph classification dataset where cluster 0 and 1 are the observed classes and clusters 2 and 3 are generated subgraph explanations. The embedding model was a 3 layer graph convolution network with a neural network type convolution that utilized every feature. 

- During each of the 50 trials, the GCN was trained using 50 samples (each) from only clusters 0 and 1. We then sampled 50 test graphs from each of the 4 cluster and computed 95\% t intervals for GED and embedding cosine distance as percentages of their respective upper bounds. Since $(\mu_0 = -1, \mu_1 = 1, \mu_2 = 0, \mu_3 = -5)$ we would expect an unbiased distance metric to maintain the following relationship, 
\begin{equation}
    \text{dist}(1,\ 2) \approx \text{dist}(0, 2) < \text{dist}(0, 1) < \text{dist}(0, 3) < \text{dist}(1, 3)
\end{equation} 
based on the distance between cluster means.

```{r}
#| echo: false
table_1 <- data.frame(
    #within = c("(0.0008, 0.0011)", "(0.0006, 0.0009)"), 
    #between = c("(0.7666, 0.795)", "(0.7677, 0.7968)"), 
    #out1 = c("(0.3803, 0.4672)", "(0.2187, 0.3019)"),
    #out2 = c("(0.0136, 0.0181)", "(0.809, 0.8387)"),
    within = c("(0.0008, 0.0015)", "(0.0005, 0.0009)"), 
    between = c("(0.5662, 0.6264)", "(0.5689, 0.6289)"), 
    out1 = c("(0.2894, 0.3589)", "(0.1282, 0.209)"),
    out2 = c("(0.018, 0.0253)", "(0.6521, 0.7097)"),
    within = c("(0.4147, 0.419)", "(0.462, 0.4674)"), 
    between = c("(0.6589, 0.6639)", "(0.6586, 0.6646)"), 
    out1 = c("(0.7843, 0.7947)", "(0.7571, 0.7715)"), 
    out2 = c("(0.6164, 0.6286)", "(0.8256, 0.8352)"),
    check.names = F
)

rownames(table_1) <- c("Cluster 0", "Cluster 1")
colnames(table_1) <-c("Within", "Between", "Cluster 2", "Cluster 3", 
                      "Within", "Between", "Cluster 2", "Cluster 3")

t(table_1) |> 
    kbl(format = "latex", booktabs = T, longtable = T, 
        linesep = "", align = "c", 
        caption = "Simulation Results") |> 
    #pack_rows("Multiple Embeddings", 1, 4) |>
    pack_rows("Embeddings", 1, 4) |>
    pack_rows("GED", 5, 8) |> 
    footnote(
        number = c("Node Means: $\\\\mu_0 = -1$, $\\\\mu_1 = 1$, $\\\\mu_{out1} = 0$, 
                    $\\\\mu_{out2}$ = -5.", 
                   "95\\\\% t intervals on \\\\% difference."), 
        escape = F
    )
```
\begin{align*}
    \text{Expected: } &\text{dist}(1,\ 2) \approx \text{dist}(0, 2) < \text{dist}(0, 1) < \\ 
    &\text{dist}(0, 3) < \text{dist}(1, 3) \\ 
    \text{GED: }&(0.75, 0.77) \textcolor{green}{\approx} (0.78, 0.79) 
                \textcolor{red}{>} (0.65, 0.66) \textcolor{red}{>} \\
                &(0.61, 0.62) \textcolor{green}{<} (0.82, 0.83) \\
    \text{Embedding: }&(0.12, 0.20) \textcolor{red}{<} (0.28, 0.35) 
                \textcolor{green}{<} (0.56, 0.62) \textcolor{red}{>>} \\
                &(0.018, 0.0253) \textcolor{green}{<} (0.65, 0.7)
\end{align*}
The most worrisome bias arises between the distances from cluster 0 to 1 and from cluster 0 to 3. Despite cluster 0's mean being twice as close to cluster 1, the embedding distance indicates that cluster 0 is (53, 60) percentage points farther away from cluster 1. Although GED also displayed this bias, it was considerably less pronounced, registering at (1, 5) percentage points. This implies that a generator initialized with a mean of -5 would produce samples with embeddings confidently assigning them to cluster 0, despite having a feature distribution far from the truth. Relying solely on embedding distance for training also increases the likelihood of the generator becoming trapped in a misleading local optimum due to the substantial bias. Although GED exhibits a statistically significant bias, its magnitude is considerably smaller, making it less likely to disrupt training. Another troubling bias observed is that the embedding distance between clusters 1 and 2 is approximately 10 percentage points less than the embedding distance between clusters 0 and 2, even though the mean of cluster 2 is equidistant from the means of clusters 1 and 0. This could stem from cluster 1 having fewer nodes making samples more similar in terms of size to cluster 2. However, since researcher typically cap the size of examples for interpretability, an objective with a degree bias could be problematic. On the contrary, GED introduces a bias absent in the embedding space. The edit distance between clusters 0 and 2 is actually greater than the distance between clusters 0 and 1. While biased concerning mean locations, this direction is aligned with the maximum node size. Despite this, the differing directions of the biases of GED and embedding distance imply that they capture unique structural nuances, potentially complementing each other.

- $\mathcal{L}_{\text{GED}}$ could also be added to the loss function for D4Explainer as well. 

## Graph Prototype Masking Network 

- ProtGNN has two main limiations:  

    1. Prototype projections are computationally expensive.   

    2. Prototype projections cannot produce out-of-sample insights.   

- These two issue can be resolve by using graph protypes as opposed to vectors. Define each prototype to be a graph,   
\begin{equation}
    p_k = (X_{p_k}, A_{p_k}, \ E_{p_k})
\end{equation}   

- $f(G; \ \Omega_f) = \hat p_G$ would need to be replace with a model similar to the denoising model in D4Explainer, that can output graph matrices. 

- The model can be trained using graph level similarity via GED and/or the original loss terms of ProtGNN applied to the prototypes embeddings. 

- This elimates the need for the upward prediction back to the graph space, which addresses issue 1.     

- Since $X_{p_k}, A_{p_k}, \ E_{p_k}$ are free trainable parameters, they could contain structure that weren't entirerly observed in the original dataset.  

- We expect sample size to be a limiting factor. Although ProtGNN uses fairly high dimensional vectors for their prototypes, graph prototypes might still introduce exponentially more parameters. The size and sparity of the graph prototypes could be enforce, but it would introduces additional hyperparmeters. If the dataset is extreme large, then the additional parameters should be okay.  

\pagebreak

# References