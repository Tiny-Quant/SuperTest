---
title: "Qualifying Exam"
author: "Art Tay"
format:
  pdf:
     documentclass: article
     papersize: letter
     geometry:
         margin=1in
     fontsize: 11pt
     keep-tex: true
     include-in-header: header.tex
bibliography: references.bib
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
library(magrittr)
library(tidymodels)
library(kableExtra)


tidymodels_prefer()
```

# Introduction 

- Overview of the problem area.

    - Graphs are an important data structure. 
        - GRAPHS provide an incredibly flexible structure for modeling complex data. Data can naturally appear as graphs, like molecules. We can reduce data to a graph, such as the key points of a image. We can even use graphs to add structure, such as grammatical relationships.
    
    - GNN models are good at prediction and inference on graph data. 
        - Graph Neural Networks (GNNs) have become a popular choice for prediction and inference on graph data. At their core, GNNs work by iteratively updating node embeddings based on information from neighboring nodes. The idea is to use the graph's structure to engineer better features. This message passing scheme allows GNNs to capture complex dependencies and patterns present within the graph structure. GNN architectures typically consist of multiple layers, each performing message passing and aggregation operations to refine the embeddings. These layers are often followed by pooling and dense prediction layers to produce the final output.

    - There are many important applications for graph classification models. 
        - Some important applications of graph classification include predicting chemical toxicity [@bai2019unsupervised], classifying proteins [@gallicchio2019fast], and even detecting cancer from pathology slides [@Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023]. 
    
    - **Problem:** While GNNs achieve remarkable predictive power, their complexity prevents the exaction of the scientific rationale.

- Why is the problem important? 

    - Explaining or interpreting GNN predictions would 
        - help with the adoption of such models for critical applications, 
        - prevent adversarial attacks, 
        - detect potential implicit discrimination, 
        - guide scientific as well as machine learning research. 


- How does the problem relate to the fundamentals areas of Statistics? 

    - Explain-ability vs Interpretability  
        - @Yuan_Yu_Gui_Ji_2022 
        - A model is interpretable if the models decision process can be readily understood by humans. For example, a linear regression model is interpretable because the coefficient clearly define how any prediction get made. 
        - A model is explainable if the models prediction can be reasoned post-hoc. Permuting each variable and measuring the variation in the predictions can be used to estimate each variables marginal effect [cite]. 

    - One goal would be to create a GNN type model whose decision process is human interpretable. A straight translation from statistics would be a circuit type analysis [cite]. For graphs, this would mean some form of coefficients on subgraphs producing the prediction. 

    - Another goal might be to develope a method that determines if a feature is statistical significant to the GNN model. The challenge is that the graph features that matter to researchers aren't necessarily tabular.  

- What is the impact of solving this problem? 
    - In the application where GNNs have shown strong predictive power, we can exact a testable scientific hypothesis for the nature of the classification.  

    - In the application where GNNs have weak predictive power, highlight the potential misunderstandings the model is having. 

# Notation 

- Let $G$ denote a graph. 

- Any graph $G$ can be describe by $X, A, E$. The node feature
matrix, edge feature matrix, and adjacency matrix respectively 

- Let $X = [X_c, \ X_d]$, where $X_c$ is the subset of continuous node features and $X_d$ is the subset of one-hot discrete node features.  

- Let $E = [E_c, \ E_d]$, denoted in the same manner. 

- Let $n$ represent the number of nodes in the graph and $v$ represent the 
number of edges.  

- Let $\text{feat}_{(.)}$ denote the number of features or columns in the the corresponding feature matrix.  

- $A$ is a binary $n \times n$ matrix where $A[i, \ j] = 1$ indicates that an edge exists between nodes labeled $i$ and $j$.  

- Let $\text{explainee}(G; \ \Omega) = h^{(1)}_G, \dots h^{(L)}_G, \rho_G$ be an $L$ layer GNN model with parameters $\Omega$ that we would like to explain. 

- For any graph Let $\nu$ denote the set of node and $\mathcal{E}$ denote the set of edges. 

# Analysis of Core Papers

## GNNInterpreter 
[@Wang_Shen_2024]

- Overview
    - Instance v. Model Level 
        - In general, explanation methods serve to elucidate which
        features within the data influence disparate predictions. These
        methods typically fall into two categories: instance-level and
        model-level. Instance-level explanations aim to unveil the
        model’s rationale behind a particular prediction. In domains
        such as image and text analysis, a prevalent approach involves
        masking or perturbing the instance and assessing the
        impact on the model’s prediction. On the other hand, model-level
        explanations seek to understand how a model generally
        distinguishes between classes. In image and text analysis, for
        instance, one common technique involves treating the input as
        a trainable parameter and optimizing the model’s prediction
        towards a specific class. Consequently, the resulting optimized
        input comprises a set of features strongly associated with
        the targeted class.

    - GNNInterpreter provides model level explanations for GNN in this manner. 

    - Formally, GNNInterpreter tries to learn the graph generating distribution for each class. 

    - GNNInterpreter works by optimizing the parameters of a generic graph generating distribution to produce samples that closely match the explainee's understanding of the targeted class.

- Explanation of the graph generating distribution.  
    - Graph generating distributions are hard to specify because there can be discrete and continuous elements of $X$, $E$ and $A$. Furthermore, the interactions between these matrices can be complex.    

    - The authors tackle these issues by making two simplify assumptions.   
        1. Assume that $G$ is a *Gilbert* random graph, every possible edge as an independent fixed probability of occurring. 
            \begin{equation}
                \forall (i, \ j) \neq (k, l) \ Pr(A[i, \ j] = 1) \perp Pr(A[k, \ l] = 1)
            \end{equation}
        
        2. The features of every node and edge are independently distributed.   

    - The author justify these assumptions by: 
        1. The other graph distributions aren't suitable. 
            a. Erdo-Renyi graphs have a fixed number of edges and nodes.   
            b. Rado graphs are infinite in size. 
            c. The random dot-product graph model is just a generalization of Gilbert random graphs.   

        2. Because the parameters of the independent distributions will be updated jointly using the *explainee* model, the *explainee's* understanding of the latent correlation structure should be contained in the final estimates.  

    - $X_c$ and $E_c$ can be sampled from any continuous distribution that can be expressed as a location-scale family. Separating the stochastic and systematic components is necessary for gradient based optimization. It is commonly known as the "re-parametrization trick". 

    - $X_d$, $E_d$ as well as $A$ need to be sampled from a continuous distribution for gradient based optimization, but the distribution has to have sampling properties close to a discrete distribution.  

    - The author assume that the true underlying distribution for every discrete node and edge feature is *categorical*. The categorical distribution is also know as the multi-bernoulli, where every sample has a fixed probability of being in one of the discrete categories. 

    - Suppose there are $D$ categories with probabilities 
    $\pi_\omega = \dfrac{\theta_\omega}{\sum_{i \in D}\theta_i}$.
    Then 
        \begin{equation}
            I = \underset{i \in D}{\text{argmax}} \ \log \theta_i + G^{(i)} 
                \sim \text{Cat}(\pi)
        \end{equation}
    where $G^{(i)} \overset{i.i.d.}{\sim} \text{Gumbel}(0, 1)$. 

    - The intuition is that the Gumbel or extreme value distribution is the density of the maximum order statistic of i.i.d. standard normals which makes it a good candidate for model the winning or maximum probability category. Adding Gumbel noise to the logits should maintain the true relative proportions, but enough skewness such that every category has some probability of having the maximum noised logit.  

    - **Proof 1:**
    In order for $I$ to be a true categorical distribution, 
    $Pr[I = \omega] = \pi_\omega$. $I = \omega$ if and only if
    $\log \theta_\omega  + G^{(\omega)} > \log \theta_i + G^{i} \ 
        \forall i \in D \setminus \omega$. Let $M_i$ denote a random variable 
        that follows a $\text{Gumbel}(\log \theta_i, 1)$ distribution. 
    \begin{align*}
        Pr[I = \omega] 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in D \setminus \omega} Pr(M_i < m_\omega) 
                \text{ i.i.d location shifted Gumbel distributions.} \\ 
            &= \mathams{E}_{M_\omega} 
                \prod_{i \in D \setminus \omega} \exp 
                    \left(-e^{\log \theta_i - m_\omega} \right)
                \text{ Gumbel CDF.} \\ 
            &= \mathams{E}_{M_\omega} 
                \exp \left(-\sum_{i \in D \setminus \omega}
                e^{\log \theta_i - m_\omega} \right) \\ 
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\omega - m_\omega \right) 
                \exp \left(-e^{\log \theta_\omega - m_\omega} \right) \cdot 
                \exp \left (-\sum_{i \in D \setminus \omega}
                e^{\log \theta_i - m_\omega} \right) \ dm \\
                &\text{ Gumbel PDF.} \\
            &= \int_{-\infty}^{\infty}
                \exp \left( \log \theta_\omega - m_\omega \right) 
                \exp \left (-\sum_{i \in D}
                e^{\log \theta_i - m_\omega} \right) \ dm \\
            &= \int_{-\infty}^{\infty}
                \theta_\omega 
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in D}
                    \theta_i  \right) \ dm \\
            &= \pi_\omega \sum_{i \in D} \theta_i
            \int_{-\infty}^{\infty}
                \exp \left( -m_\omega \right) 
                \exp \left( -e^{-m_\omega} \sum_{i \in D}
                    \theta_i  \right) \ dm 
                    \text{ From the above definition of } \pi_\omega \\
            &= \pi_\omega \sum_{i \in D} \theta_i
                \dfrac{\exp\left( -e^{-m_\omega} \sum_{i \in D} \theta_i \right)
                    }{\sum_{i \in D} \theta_i} \bigg|_{-\infty}^\infty \\ 
            &= \pi_\omega \sum_{i \in D} \theta_i
                \dfrac{1}{\sum_{i \in D} \theta_i} = \pi_\omega
        \end{align*}
        Reference: @Huijben_Kool_Paulus_van_Sloun_2022 


    - Using inverse CDF sampling and and relaxing the argmax to a Softmax, we 
    can sample one-hot categorical vectors based on two parameters 
    $\theta_{\text{Cat}}$, a trainable parameter vector of length equal to 
    the number of categories, and $\tau$, a hyperparameter that controls 
    the degree of relaxation (smaller value approximate the discrete 
    sampling better, but can result in numerical issues). 
        \begin{equation}
            \text{Softmax}
            \left(
                \dfrac{\theta_{\text{Cat}} - log(-log \ \epsilon)}{\tau}
            \right), \quad \epsilon \sim U[0, 1]
        \end{equation}
    This method, known as the concrete distribution [@Maddison_Mnih_Teh_2017], yields a reasonable smooth gradient w.r.t. to the probability parameters. 

    - The adjacency matrix can be sampled in a similar manner since the Bernoulli is just a special case of the categorical.   
        \begin{equation}
            \text{sigmoid}
                \left(\dfrac{\theta_A + \log \ \epsilon - \log(1 - \log \ \epsilon)}{\tau} \right)
        \end{equation}
    This is known as the binary concrete distribution [@Maddison_Mnih_Teh_2017]. 

    - Notate the combined graph generating distribution as: 

        $$
        G_{\text{gen}} \sim \text{gen}(\Theta)  
        $$

        where $\Theta$ is the set of all parameters from the independently sampled distributions. 

- Prediction objective. 
    - An obvious objective is to maximize the likelihood that the *explainee* model predicts a sampled graph to be a member of the target class.

    - Let $\tilde{\rho}$ denote the desired predicted probability vector. Then the above objective can be expressed as: 

    \begin{equation}
        \mathcal{L}_\text{pred} (\Theta \ | \ G_\text{gen}) = \mathams{E}_{G_\text{gen}} \ \text{CrtEnt} (\text{explainee}(G_\text{gen}), \ \tilde{\rho})
    \end{equation}

- Embedding objective. 
    - While the above objective enforces a desirable property, it isn't restrictive enough to make the generated graph realistic. This is because the final prediction, $\rho_{G_{\text{gen}}}$ is compute using only final embeddings, $h^{(L)}_{G_{\text{gen}}}$. Normally $h^{(L)}_{G_{\text{gen}}}$ contains all the needed information from the graphs structure; however, the generation scheme allows the feature distribution to be optimized directly. This means that explanation can ignore the graph structure and optimize towards the desired final embeddings.

    - Another way of understanding the problems with the above objective is to consider the *out-of-distribution* (ood) issue. Since the above generation scheme is not restricted by the observed data distribution, the initial generated graphs may be very ood, but clearly on one side of the decision boundary.   

    - The author find that empirically GNN model exhibit a class preference.   
    ![](figures/random_baseline.png){fig-align="center"}
    [@Wang_Shen_2024]

    For example, random graphs have an average predicted probability of being Non-mutagenic in the MUTAG dataset if 93.2%. This demonstrates why the above objective is insufficient to generate realistic or *in-distribution* (id) graph. 

    - In order to mitigate this issue, the author proposed additional minimizing the cosine distance between the average embedding of all the observed graph from the targeted class, $\bar h^{(L)}_{G_c}$, and the embedding of the generated explanation. 

    \begin{equation}
       \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) = \text{CosDist}\left( \bar h^{(L)}_{G_c}, \ h^{(L)}_{G_\text{gen}} \right)
    \end{equation}

- Regularization terms.   
    - Sparse graphs are easy for humans to interpret. To encourage sparsity the authors employed an $L_1$, $L_2$, and a budget penalty on the edge probabilities.

    \begin{equation}
        \mathcal{L}_{\text{Sparsity}}(\theta_A) = ||\theta_A||_1 + ||\theta_A||_2 + \text{softplus}(\text{sigmoid}||\theta_A||_1 - B)^2
    \end{equation}
    where $B$ is the expected maximum number of edge for generated explanation graphs.

    - Connectivity is another desirable property as it ensures a cohesive explanation.
    To encourage connectivity the author minimize the *KL-Divergence* between edge probabilities that share a common node.

    \begin{equation}
        \mathcal{L}_{\text{Connect}}(\theta_A) = \sum_{i \in \nu} \sum_{j, k \in \mathcal{E}(i)} D_{KL}(\text{sigmoid}(\theta_A[i, \ j]) \ || \ \text{sigmoid}(\theta_A[i, \ k]))
    \end{equation}

    where $\mathcal{E}(i)$ is the set of edges that connect to node $i$.

- Summary of Results + Figures
    - The final generator model is trained by sampling $G_\text{gen} \sim \text{gen}(\Theta)$ and then iterative updated $\Theta$ via gradient descent on the full loss:
    \begin{equation}
        \begin{split}
            \mathcal{L}_{\text{GNNInterpreter}}(\Theta \ | \ G_\text{gen}) = 
            &\mathcal{L}_{\text{pred}}(\Theta \ | \ G_\text{gen}) + 
            \mathcal{L}_{\text{embed}}(\Theta \ | \ G_\text{gen}) +  \\
            &\mathcal{L}_{\text{sparsity}}(\Theta \ | \ G_\text{gen}) +
            \mathcal{L}_{\text{connect}}(\Theta \ | \ G_\text{gen}) 
        \end{split}
    \end{equation}

    ![](figures/GNNInt_prediction_results.png){fig-align="center"}

    - GNNInterpreter achieve remarkable accuracy on most target classes. Many of the interval are tight and very close to 1, which implies that the examples generated are almost always classified as the targeted class. The explanations for the house motif and the lollipop shape are worse in terms of predictions. Although the author critique the use of predictions as the sole objective, they do not use any other quantitative metric to evaluate the validity of their explanations. 

    ![](figures/GNNInt_drawn_results.png){fig-align="center"}
    - Qualitatively we can see some limitation in terms of realism. 
    
    - For example, for the mutagen class, the explanation correctly identifies the importance of the N02 group; however, the generated graph isn't realistic and might not even be chemically possible. Furthermore, the non-mutagen example doesn't display any clear patterns or identifiable structures.   

    - The explanations for the Cyclicity dataset as well as the Wheel class do not appear to be members of the underlying data distribution. 

    - GNNInterpreter provides a way of generating example graph that would be classified as a target class by a GNN model, without needing to specify domain specific rules. On the other hand, optimizing predictions and even embeddings does not appear to be a sufficient objective for producing in-distribution graphs. The author correctly point out that optimizing predictions can lead to unrealistic graphs, but they have also inadvertently demonstrated that optimizing embeddings is not necessarily sufficient either.       

## D4Explainer
[@Chen_Wu_Gupta_Ying_2023]
- Overview
    - D4Explainer or in-Distribution GNN explanations via Discrete Denoising Diffusion attempt to directly address the realism of generated graphs in model-level explanation by using the observed data to train a Diffusion model. In the image domain, diffusion model have been shown to produce the most realistic images when compared to other generative AI methods such as Generative Adversarial Networks (GANs). Diffusion model work by iteratively noising an observation until it is pure noise. Then a denoising model is trained to predict the noise added at any given time step. Then new observations can be generated by passing pure noise through the diffusion model in a process known as reverse sampling. Additional label information can be passed to generate observations with similar a label. 

- Graph diffusion. 

- Note on counter-factual explanations. 

## ProtGNN
[@Zhang_Liu_Wang_Lu_Lee_2021]

# Synthesis of Core Papers

- Comparison of generation methods. 
    - GNNInterpreter uses continuously relaxed discrete distributions. 
    - D4Explainer uses diffusion. 
    - Diffusion is slower, but can be more realistic. Probably because diffusion is less subject to the **out-of-distribution (OOD) problem**. 
    - Prototype projection are like generative methods. Restricted to in distribution, but realism is all but guaranteed. 

# Technical Details 

- Minimal reproduction of each method on MUTAG. 

# Future Directions 

\pagebreak

# References