@article{Chen_Wu_Gupta_Ying_2023, title={D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion}, url={http://arxiv.org/abs/2310.19321}, DOI={10.48550/arXiv.2310.19321}, abstractNote={The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.}, note={arXiv:2310.19321 [cs]}, number={arXiv:2310.19321}, publisher={arXiv}, author={Chen, Jialin and Wu, Shirley and Gupta, Abhijit and Ying, Rex}, year={2023}, month=oct }
@article{Wang_Shen_2024, title={GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks}, url={http://arxiv.org/abs/2209.07924}, DOI={10.48550/arXiv.2209.07924}, abstractNote={Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation. The official implementation can be found at https://github.com/yolandalalala/GNNInterpreter.}, note={arXiv:2209.07924 [cs]}, number={arXiv:2209.07924}, publisher={arXiv}, author={Wang, Xiaoqi and Shen, Han-Wei}, year={2024}, month=feb }
@article{Zhang_Liu_Wang_Lu_Lee_2021, title={ProtGNN: Towards Self-Explaining Graph Neural Networks}, url={http://arxiv.org/abs/2112.00911}, DOI={10.48550/arXiv.2112.00911}, abstractNote={Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.}, note={arXiv:2112.00911 [cs]}, number={arXiv:2112.00911}, publisher={arXiv}, author={Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Cheekong}, year={2021}, month=dec }
@book{Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023, type={preprint}, title={Deep Learning of Cell Spatial Organizations Identifies Clinically Relevant Insights in Tissue Images}, url={https://www.researchsquare.com/article/rs-2928838/v1}, DOI={10.21203/rs.3.rs-2928838/v1}, abstractNote={Abstract Recent advancements in tissue imaging techniques have facilitated the visualization and identification of various cell types within physiological and pathological contexts. Despite the emergence of cell-cell interaction studies, there is a lack of methods for evaluating individual spatial interactions. In this study, we introduce Ceograph, a novel cell spatial organization-based graph convolutional network designed to analyze cell spatial organization (i.e. the cell spatial distribution, morphology, proximity, and interactions) derived from pathology images. Ceograph identifies key cell spatial organization features by accurately predicting their influence on patient clinical outcomes. In patients with oral potentially malignant disorders, our model highlights reduced structural concordance and increased closeness in epithelial substrata as driving features for an elevated risk of malignant transformation. In lung cancer patients, Ceograph detects elongated tumor nuclei and diminished stroma-stroma closeness as biomarkers for insensitivity to EGFR tyrosine kinase inhibitors. With its potential to predict various clinical outcomes, Ceograph offers a deeper understanding of biological processes and supports the development of personalized therapeutic strategies.}, institution={In Review}, author={Xiao, Guanghua and Wang, Shidan and Rong, Ruichen and Yang, Donghan and Zhang, Xinyi and Zhan, Xiaowei and Bishop, Justin and Wilhelm, Clare and Zhang, Siyuan and Pickering, Curtis and Kris, Mark and Minna, John and Xie, Yang}, year={2023}, month={Jul}, language={en} }
@misc{bai2019unsupervised,
      title={Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity}, 
      author={Yunsheng Bai and Hao Ding and Yang Qiao and Agustin Marinovic and Ken Gu and Ting Chen and Yizhou Sun and Wei Wang},
      year={2019},
      eprint={1904.01098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gallicchio2019fast,
      title={Fast and Deep Graph Neural Networks}, 
      author={Claudio Gallicchio and Alessio Micheli},
      year={2019},
      eprint={1911.08941},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Yuan_Yu_Gui_Ji_2022, title={Explainability in Graph Neural Networks: A Taxonomic Survey}, url={http://arxiv.org/abs/2012.15445}, DOI={10.48550/arXiv.2012.15445}, abstractNote={Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.}, note={arXiv:2012.15445 [cs]}, number={arXiv:2012.15445}, publisher={arXiv}, author={Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang}, year={2022}, month=jul }
@article{Huijben_Kool_Paulus_van_Sloun_2022, title={A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning}, url={http://arxiv.org/abs/2110.01515}, DOI={10.48550/arXiv.2110.01515}, abstractNote={The Gumbel-max trick is a method to draw a sample from a categorical distribution, given by its unnormalized (log-)probabilities. Over the past years, the machine learning community has proposed several extensions of this trick to facilitate, e.g., drawing multiple samples, sampling from structured domains, or gradient estimation for error backpropagation in neural network optimization. The goal of this survey article is to present background about the Gumbel-max trick, and to provide a structured overview of its extensions to ease algorithm selection. Moreover, it presents a comprehensive outline of (machine learning) literature in which Gumbel-based algorithms have been leveraged, reviews commonly-made design choices, and sketches a future perspective.}, note={arXiv:2110.01515 [cs, stat]}, number={arXiv:2110.01515}, publisher={arXiv}, author={Huijben, Iris A. M. and Kool, Wouter and Paulus, Max B. and van Sloun, Ruud J. G.}, year={2022}, month=mar }
@article{Maddison_Mnih_Teh_2017, title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}, url={http://arxiv.org/abs/1611.00712}, DOI={10.48550/arXiv.1611.00712}, abstractNote={The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.}, note={arXiv:1611.00712 [cs, stat]}, number={arXiv:1611.00712}, publisher={arXiv}, author={Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye}, year={2017}, month=mar }


