@article{Chen_Wu_Gupta_Ying_2023, title={D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion}, url={http://arxiv.org/abs/2310.19321}, DOI={10.48550/arXiv.2310.19321}, abstractNote={The widespread deployment of Graph Neural Networks (GNNs) sparks significant interest in their explainability, which plays a vital role in model auditing and ensuring trustworthy graph learning. The objective of GNN explainability is to discern the underlying graph structures that have the most significant impact on model predictions. Ensuring that explanations generated are reliable necessitates consideration of the in-distribution property, particularly due to the vulnerability of GNNs to out-of-distribution data. Unfortunately, prevailing explainability methods tend to constrain the generated explanations to the structure of the original graph, thereby downplaying the significance of the in-distribution property and resulting in explanations that lack reliability. To address these challenges, we propose D4Explainer, a novel approach that provides in-distribution GNN explanations for both counterfactual and model-level explanation scenarios. The proposed D4Explainer incorporates generative graph distribution learning into the optimization objective, which accomplishes two goals: 1) generate a collection of diverse counterfactual graphs that conform to the in-distribution property for a given instance, and 2) identify the most discriminative graph patterns that contribute to a specific class prediction, thus serving as model-level explanations. It is worth mentioning that D4Explainer is the first unified framework that combines both counterfactual and model-level explanations. Empirical evaluations conducted on synthetic and real-world datasets provide compelling evidence of the state-of-the-art performance achieved by D4Explainer in terms of explanation accuracy, faithfulness, diversity, and robustness.}, note={arXiv:2310.19321 [cs]}, number={arXiv:2310.19321}, publisher={arXiv}, author={Chen, Jialin and Wu, Shirley and Gupta, Abhijit and Ying, Rex}, year={2023}, month=oct }
@article{Wang_Shen_2024, title={GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks}, url={http://arxiv.org/abs/2209.07924}, DOI={10.48550/arXiv.2209.07924}, abstractNote={Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation. The official implementation can be found at https://github.com/yolandalalala/GNNInterpreter.}, note={arXiv:2209.07924 [cs]}, number={arXiv:2209.07924}, publisher={arXiv}, author={Wang, Xiaoqi and Shen, Han-Wei}, year={2024}, month=feb }
@article{Zhang_Liu_Wang_Lu_Lee_2021, title={ProtGNN: Towards Self-Explaining Graph Neural Networks}, url={http://arxiv.org/abs/2112.00911}, DOI={10.48550/arXiv.2112.00911}, abstractNote={Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.}, note={arXiv:2112.00911 [cs]}, number={arXiv:2112.00911}, publisher={arXiv}, author={Zhang, Zaixi and Liu, Qi and Wang, Hao and Lu, Chengqiang and Lee, Cheekong}, year={2021}, month=dec }
@book{Xiao_Wang_Rong_Yang_Zhang_Zhan_Bishop_Wilhelm_Zhang_Pickering_et_al._2023, type={preprint}, title={Deep Learning of Cell Spatial Organizations Identifies Clinically Relevant Insights in Tissue Images}, url={https://www.researchsquare.com/article/rs-2928838/v1}, DOI={10.21203/rs.3.rs-2928838/v1}, abstractNote={Abstract Recent advancements in tissue imaging techniques have facilitated the visualization and identification of various cell types within physiological and pathological contexts. Despite the emergence of cell-cell interaction studies, there is a lack of methods for evaluating individual spatial interactions. In this study, we introduce Ceograph, a novel cell spatial organization-based graph convolutional network designed to analyze cell spatial organization (i.e. the cell spatial distribution, morphology, proximity, and interactions) derived from pathology images. Ceograph identifies key cell spatial organization features by accurately predicting their influence on patient clinical outcomes. In patients with oral potentially malignant disorders, our model highlights reduced structural concordance and increased closeness in epithelial substrata as driving features for an elevated risk of malignant transformation. In lung cancer patients, Ceograph detects elongated tumor nuclei and diminished stroma-stroma closeness as biomarkers for insensitivity to EGFR tyrosine kinase inhibitors. With its potential to predict various clinical outcomes, Ceograph offers a deeper understanding of biological processes and supports the development of personalized therapeutic strategies.}, institution={In Review}, author={Xiao, Guanghua and Wang, Shidan and Rong, Ruichen and Yang, Donghan and Zhang, Xinyi and Zhan, Xiaowei and Bishop, Justin and Wilhelm, Clare and Zhang, Siyuan and Pickering, Curtis and Kris, Mark and Minna, John and Xie, Yang}, year={2023}, month={Jul}, language={en} }
@misc{bai2019unsupervised,
      title={Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity}, 
      author={Yunsheng Bai and Hao Ding and Yang Qiao and Agustin Marinovic and Ken Gu and Ting Chen and Yizhou Sun and Wei Wang},
      year={2019},
      eprint={1904.01098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gallicchio2019fast,
      title={Fast and Deep Graph Neural Networks}, 
      author={Claudio Gallicchio and Alessio Micheli},
      year={2019},
      eprint={1911.08941},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Yuan_Yu_Gui_Ji_2022, title={Explainability in Graph Neural Networks: A Taxonomic Survey}, url={http://arxiv.org/abs/2012.15445}, DOI={10.48550/arXiv.2012.15445}, abstractNote={Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets specifically for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.}, note={arXiv:2012.15445 [cs]}, number={arXiv:2012.15445}, publisher={arXiv}, author={Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang}, year={2022}, month=jul }
@article{Huijben_Kool_Paulus_van_Sloun_2022, title={A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning}, url={http://arxiv.org/abs/2110.01515}, DOI={10.48550/arXiv.2110.01515}, abstractNote={The Gumbel-max trick is a method to draw a sample from a categorical distribution, given by its unnormalized (log-)probabilities. Over the past years, the machine learning community has proposed several extensions of this trick to facilitate, e.g., drawing multiple samples, sampling from structured domains, or gradient estimation for error backpropagation in neural network optimization. The goal of this survey article is to present background about the Gumbel-max trick, and to provide a structured overview of its extensions to ease algorithm selection. Moreover, it presents a comprehensive outline of (machine learning) literature in which Gumbel-based algorithms have been leveraged, reviews commonly-made design choices, and sketches a future perspective.}, note={arXiv:2110.01515 [cs, stat]}, number={arXiv:2110.01515}, publisher={arXiv}, author={Huijben, Iris A. M. and Kool, Wouter and Paulus, Max B. and van Sloun, Ruud J. G.}, year={2022}, month=mar }
@article{Maddison_Mnih_Teh_2017, title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}, url={http://arxiv.org/abs/1611.00712}, DOI={10.48550/arXiv.1611.00712}, abstractNote={The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.}, note={arXiv:1611.00712 [cs, stat]}, number={arXiv:1611.00712}, publisher={arXiv}, author={Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye}, year={2017}, month=mar }
@article{Bougleux_Brun_Carletti_Foggia_Gaüzère_Vento_2015, title={A Quadratic Assignment Formulation of the Graph Edit Distance}, url={http://arxiv.org/abs/1512.07494}, abstractNote={Computing eﬃciently a robust measure of similarity or dissimilarity between graphs is a major challenge in Pattern Recognition. The Graph Edit Distance (GED) is a ﬂexible measure of dissimilarity between graphs which arises in error-tolerant graph matching. It is deﬁned from an optimal sequence of edit operations (edit path) transforming one graph into an other. Unfortunately, the exact computation of this measure is NP-hard. In the last decade, several approaches have been proposed to approximate the GED in polynomial time, mainly by solving linear programming problems. Among them, the bipartite GED has received much attention. It is deduced from a linear sum assignment of the nodes of the two graphs, which can be eﬃciently computed by Hungarian-type algorithms. However, edit operations on nodes and edges are not handled simultaneously, which limits the accuracy of the approximation. To overcome this limitation, we propose to extend the linear assignment model to a quadratic one, for directed or undirected graphs having labelized nodes and edges. This is realized through the deﬁnition of a family of edit paths induced by assignments between nodes. We formally show that the GED, restricted to the paths in this family, is equivalent to a quadratic assignment problem. Since this problem is NP-hard, we propose to compute an approximate solution by an adaptation of the Integer Projected Fixed Point method. Experiments show that the proposed approach is generally able to reach a more accurate approximation of the optimal GED than the bipartite GED, with a computational cost that is still aﬀordable for graphs of non trivial sizes.}, note={arXiv:1512.07494 [cs]}, number={arXiv:1512.07494}, publisher={arXiv}, author={Bougleux, Sébastien and Brun, Luc and Carletti, Vincenzo and Foggia, Pasquale and Gaüzère, Benoit and Vento, Mario}, year={2015}, month=dec, language={en} }
@article{wang2024pygm,
  author  = {Runzhong Wang and Ziao Guo and Wenzheng Pan and Jiale Ma and Yikai Zhang and Nan Yang and Qi Liu and Longxuan Wei and Hanxue Zhang and Chang Liu and Zetian Jiang and Xiaokang Yang and Junchi Yan},
  title   = {Pygmtools: A Python Graph Matching Toolkit},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {33},
  pages   = {1-7},
  url     = {https://jmlr.org/papers/v25/23-0572.html},
} 
@inproceedings{Cho_Lee_Lee_2010, address={Berlin, Heidelberg}, series={Lecture Notes in Computer Science}, title={Reweighted Random Walks for Graph Matching}, ISBN={978-3-642-15555-0}, DOI={10.1007/978-3-642-15555-0_36}, abstractNote={Graph matching is an essential problem in computer vision and machine learning. In this paper, we introduce a random walk view on the problem and propose a robust graph matching algorithm against outliers and deformation. Matching between two graphs is formulated as node selection on an association graph whose nodes represent candidate correspondences between the two graphs. The solution is obtained by simulating random walks with reweighting jumps enforcing the matching constraints on the association graph. Our algorithm achieves noise-robust graph matching by iteratively updating and exploiting the confidences of candidate correspondences. In a practical sense, our work is of particular importance since the real-world matching problem is made difficult by the presence of noise and outliers. Extensive and comparative experiments demonstrate that it outperforms the state-of-the-art graph matching algorithms especially in the presence of outliers and deformation.}, booktitle={Computer Vision – ECCV 2010}, publisher={Springer}, author={Cho, Minsu and Lee, Jungmin and Lee, Kyoung Mu}, editor={Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos}, year={2010}, pages={492–505}, collection={Lecture Notes in Computer Science}, language={en} }
@article{Rudin_2019, title={Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead}, url={http://arxiv.org/abs/1811.10154}, DOI={10.48550/arXiv.1811.10154}, abstractNote={Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to textit{explain} black box models, rather than creating models that are textit{interpretable} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.}, note={arXiv:1811.10154 [cs, stat]}, number={arXiv:1811.10154}, publisher={arXiv}, author={Rudin, Cynthia}, year={2019}, month=sep }

